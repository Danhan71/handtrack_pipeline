{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-extraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" generically summarize learning-related effects.\n",
    "Here is like a condensed version of the task-scoring plots previously coded (for lines5\n",
    "mainly).\n",
    "Goal here is to simplify, main plots, quicker code (working with datasets)\n",
    "And potentially flexibly combine with models later on.\n",
    "(This spurred by BRAIN INitiative poster, wanting to make simple histogram of num strokes\n",
    "in epoch 1 and 2)\n",
    "\n",
    "LOGGING PROGRESS FOR PORTING FROM analysis_TEMPLATE (Probedat)\n",
    "- behavior plots: DONE [main useful ones. not yet separate by block. see README.md]\n",
    "- scoring: DONE [except separating by block] \n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfecac80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imageio in /Users/kgg/opt/anaconda3/lib/python3.9/site-packages (2.9.0)\n",
      "Requirement already satisfied: numpy in /Users/kgg/opt/anaconda3/lib/python3.9/site-packages (from imageio) (1.20.3)\n",
      "Requirement already satisfied: pillow in /Users/kgg/opt/anaconda3/lib/python3.9/site-packages (from imageio) (8.4.0)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement pythonlib (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for pythonlib\u001b[0m\n",
      "Requirement already satisfied: pickle5 in /Users/kgg/opt/anaconda3/lib/python3.9/site-packages (0.0.11)\n"
     ]
    }
   ],
   "source": [
    "# install packages\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "sys.path.append('~/Desktop/rockefeller/pythonlib')\n",
    "import os\n",
    "# change to pythonlib for installation & import\n",
    "os.chdir('/Users/kgg/Desktop/rockefeller/pythonlib')\n",
    "!{sys.executable} -m pip install imageio\n",
    "!{sys.executable} -m pip install pythonlib\n",
    "!{sys.executable} -m pip install pickle5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "further-championship",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd ..\n",
    "# from tools.utils import * \n",
    "# from tools.plots import *\n",
    "# from tools.analy import *\n",
    "# from tools.calc import *\n",
    "# from tools.analyplot import *\n",
    "# from tools.preprocess import *\n",
    "# from tools.dayanalysis import *\n",
    "\n",
    "from pythonlib.drawmodel.analysis import *\n",
    "from pythonlib.tools.stroketools import *\n",
    "import pythonlib\n",
    "from pythonlib.dataset.dataset import Dataset\n",
    "import pickle\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "natural-integer",
   "metadata": {},
   "source": [
    "### Simple summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paperback-packaging",
   "metadata": {},
   "source": [
    "#### Plot behavior (raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28e445d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEFORE running any summary code...\n",
    "\n",
    "# drawmonkey/analysis/dataset.py\n",
    "# python -m analysis.dataset (change animal, expt list) --- makes .pkl dataset file, go into code to see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c3d2083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching using this string:\n",
      "/Users/kgg/data2/analyses/database/*Diego*gridbiasdir1*zero*/*dat*.pkl\n",
      "-- Splitting off dir from fname\n",
      "Found this many paths:\n",
      "0\n",
      "Searching using this string:\n",
      "/Users/kgg/data2/analyses/database/BEH/*Diego*gridbiasdir1*zero*/*dat*.pkl\n",
      "-- Splitting off dir from fname\n",
      "Found this many paths:\n",
      "0\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/x2/9pcv48pj7c33h8sc6rnsd2_r0000gn/T/ipykernel_4772/203398899.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mrulelist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_rulelist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manimal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_dataset_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manimal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mult\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrulelist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#TODO: change load_dataset_helper or params.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0mD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_tasks_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/rockefeller/pythonlib/pythonlib/dataset/dataset.py\u001b[0m in \u001b[0;36mload_dataset_helper\u001b[0;34m(self, animal, expt, ver, rule)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexpt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrule\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                         \u001b[0mpathlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m                         \u001b[0maer_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_main_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manimal_expt_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maer_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/rockefeller/pythonlib/pythonlib/dataset/dataset.py\u001b[0m in \u001b[0;36mfind_dataset\u001b[0;34m(self, animal, expt, assert_only_one, rule)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0massert_only_one\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpathlist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##### Related to \"Task scores\" (porting from probedat analyses, will replace it)\n",
    "### NOTE: currently is partly hard coded for biasdir expts (angle stuff). need to modify to \n",
    "# be more general.\n",
    "\n",
    "# Load dataset\n",
    "\n",
    "# INPUTS:\n",
    "# for animal in [\"Pancho\"]:\n",
    "#     for expt in [\"lines5\"]:\n",
    "\n",
    "# animal = \"Pancho\"\n",
    "# expt = \"lines5\"\n",
    "# rule = [\"straight\", \"bent\"]\n",
    "# for animal in [\"Diego\", \"Pancho\"]:\n",
    "# #     animal = \"Pancho\"\n",
    "#     expt = \"linecircle\"\n",
    "#     rule = [\"null\"]\n",
    "\n",
    "# --------------- INPUT PARAMS\n",
    "PLOT_OVERVIEW = False\n",
    "PLOT_TEST_TASK_IMAGES = False\n",
    "PLOT_EXAMPLE_DATEVTASK = False\n",
    "PLOT_ALL_EACHTRIAL = False\n",
    "PLOT_TRAIN_GRID = False\n",
    "PLOT_TRAIN_REPEATED = False # things repeated even though random.\n",
    "\n",
    "base_dir = os.path.expanduser(\"~/data2\")\n",
    "drawmonkey_dir = \"/home/lucast4/drawmonkey\"\n",
    "expt = \"gridbiasdir\" #run for chunkbyshape1, chunkbyshape2\n",
    "animal_list = [\"Diego\"]\n",
    "expt_list = [\"gridbiasdir1\", \"gridbiasdir1c\"]\n",
    "\n",
    "def get_rulelist(animal, expt):\n",
    "    # rule is stored in 'theta' variable for gridbiasdir1\n",
    "    return ['zero', 'pi'] # zero: L-R, pi: R-L\n",
    "    \n",
    "    #from pythonlib.dataset.dataset_preprocess.general import extract_expt_metadat\n",
    "    #list_expts = extract_expt_metadat(animal=animal, expt=expt, metadat_dir=f\"{base_dir}/expt_metadat\") ##CHANGE## metadat_dir if necessary\n",
    "    #rulelist = [e[1] for e in list_expts]\n",
    "    #assert len(rulelist)>0\n",
    "    #return rulelist\n",
    "\n",
    "#     if expt==\"gridlinecircle\":\n",
    "#         if animal == \"Pancho\":\n",
    "#             rule = [\"baseline\", \"linetocircle\", \"circletoline\", \"lolli\"]\n",
    "#         elif animal ==\"Diego\":\n",
    "#             rule = [\"baseline\", \"circletoline\", \"linetocircle\", \"lolli\"]\n",
    "#         elif animal ==\"Red\":\n",
    "#             rule = [\"baseline\", \"Ltoline\"]\n",
    "#         else:\n",
    "#             assert False\n",
    "#     elif expt==\"figures9\":\n",
    "#         rule = [\"bent\", \"straight\"]\n",
    "#     elif expt in [\"chunkbyshape1\", \"chunkbyshape2\"]:\n",
    "#         rule = [\"vert\", \"horiz\"]\n",
    "#     elif expt in [\"chunkbyshape4\"]:\n",
    "#         rule = [\"baseline\", \"horiz\", \"vert\"]\n",
    "#     else:\n",
    "#         assert False\n",
    "#     return rule\n",
    "\n",
    "\n",
    "# --------------- RUN\n",
    "# for animal in [\"Pancho\", \"Diego\", \"Red\"]:\n",
    "for expt in expt_list:\n",
    "    animal = \"Diego\"\n",
    "    rulelist = get_rulelist(animal, expt)\n",
    "    D = Dataset([])\n",
    "    D.load_dataset_helper(animal, expt, ver=\"mult\", rule=rulelist) #TODO: change load_dataset_helper or params.\n",
    "    D.load_tasks_helper()\n",
    "\n",
    "    ##### PREPROCESS to extract features for each trial\n",
    "    # 1) model-free features\n",
    "    from pythonlib.dataset.dataset_preprocess.general import preprocessDat\n",
    "    D, GROUPING, GROUPING_LEVELS, FEATURE_NAMES, SCORE_COL_NAMES = preprocessDat(D, expt)\n",
    "    #GROUPING_LEVELS = [zero, pi]\n",
    "    #GROUPING = 'theta'/'epoch' i.e. column name signifying rule\n",
    "    \n",
    "    # 1) extract supervision params to Dat\n",
    "    from pythonlib.tools.pandastools import applyFunctionToAllRows\n",
    "    def F(x, abbrev = True):\n",
    "        S = x[\"supervision_params\"]\n",
    "\n",
    "        # sequence\n",
    "        if S[\"sequence_on\"]==False:\n",
    "            seq = \"off\"\n",
    "        else:\n",
    "            seq = S[\"sequence_ver\"]\n",
    "\n",
    "        # dynamic\n",
    "        dyna = S[\"guide_dynamic_strokes\"]\n",
    "        if dyna:\n",
    "            d = 1\n",
    "        else:\n",
    "            d=0\n",
    "\n",
    "        # color\n",
    "        colo = S[\"guide_colored_strokes\"]\n",
    "        if colo:\n",
    "            c=1\n",
    "        else:\n",
    "            c=0\n",
    "\n",
    "        # give a condensed name\n",
    "        if seq==\"off\":\n",
    "            s = 0\n",
    "        elif seq==\"v3_remove_and_show\":\n",
    "            s = 3\n",
    "        elif seq==\"v3_remove_and_fadein\":\n",
    "            s = 4\n",
    "        elif seq==\"v3_noremove_and_show\":\n",
    "            s = 5\n",
    "        elif seq==\"v3_noremove_and_fadein\":\n",
    "            s = 6\n",
    "        elif seq==\"v4_fade_when_touch\":\n",
    "            s=1\n",
    "        elif seq==\"v4_remove_when_touch\":\n",
    "            s=2\n",
    "        elif seq==\"unknown\":\n",
    "            s=7\n",
    "        elif seq==\"objectclass_active_chunk\":\n",
    "            s=8\n",
    "        else:\n",
    "            print(seq)\n",
    "            print(\"what is this?\")\n",
    "            assert False\n",
    "\n",
    "        if abbrev:\n",
    "            return (s,d,c)\n",
    "        else:\n",
    "            return seq, dyna, colo\n",
    "    if expt in [\"chunkbyshape1\"]:\n",
    "        # TODO: define supervision stage using ObjectCalss only\n",
    "        if False:\n",
    "            # Start from here\n",
    "            TT = T.Params[\"input_params\"]\n",
    "            Tnew = TT.get_tasknew()\n",
    "            Tnew[\"Objects\"][\"Params\"]\n",
    "        else:\n",
    "            # For now, just hack it \n",
    "            Fthis = lambda x: (0,0,0)\n",
    "    else:\n",
    "        Fthis = F\n",
    "    D.Dat = applyFunctionToAllRows(D.Dat, Fthis, \"supervision_stage\")\n",
    "\n",
    "\n",
    "    SDIR_MAIN = f\"{base_dir}/analyses/main/simple_summary/{animal}-{expt}-{'_'.join(rulelist)}\"\n",
    "    os.makedirs(SDIR_MAIN, exist_ok=True)\n",
    "    SAVEDIR_FIGS = f\"{SDIR_MAIN}/FIGS/drawfigs\"\n",
    "    os.makedirs(SAVEDIR_FIGS, exist_ok=True)\n",
    "    MAX_COLS = 20\n",
    "\n",
    "    #### PLOT OVERVIEW OF EXPERIMENT\n",
    "    if PLOT_OVERVIEW:\n",
    "        figlist = D.plotOverview()\n",
    "        for i, fig in enumerate(figlist):\n",
    "            fig.savefig(f\"{SDIR_MAIN}/overview_{i}.pdf\")\n",
    "\n",
    "        # Plot overview, separating by supervision\n",
    "        fig = sns.catplot(data=D.Dat, x=\"block\", y=\"supervision_stage\", col=\"task_stagecategory\", row=\"date\")\n",
    "        fig.savefig(f\"{SDIR_MAIN}/overview_supervisionstage.pdf\")\n",
    "\n",
    "    ##### Any random tasks that are repeated?\n",
    "    \n",
    "    \n",
    "    ## Plot all test tasks stimuli\n",
    "    # Get all uique test tasks\n",
    "    if PLOT_TEST_TASK_IMAGES:\n",
    "        # TODO: split by set category, and features.\n",
    "        # get list of unique tasks\n",
    "        df = D.filterPandas({\"monkey_train_or_test\":[\"test\"]}, \"dataframe\")\n",
    "        tasklist = sorted(df[\"character\"].unique())\n",
    "        if len(tasklist)>50:\n",
    "            assert False, \"break it up into multiple plots\"\n",
    "        \n",
    "        # Only keep one per unique task\n",
    "        indices = []\n",
    "        for task in tasklist:\n",
    "            ind = D.filterPandas({\"character\":[task]})[0] # get the first\n",
    "            indices.append(ind)\n",
    "        \n",
    "        # Plot and save\n",
    "        sdirthis = f\"{SAVEDIR_FIGS}/all_test_task_images\"\n",
    "        os.makedirs(sdirthis, exist_ok=True)\n",
    "\n",
    "        fig, axes, _ = D.plotMultTrials2(indices, \"strokes_task\", titles=tasklist, SIZE=3.2, \n",
    "                          plotkwargs={\"naked_axes\":True});\n",
    "        fig.savefig(f\"{sdirthis}/all_images_1.pdf\")\n",
    "        \n",
    "\n",
    "    ##### Plot drawing behavior over entire experiment.\n",
    "    ############ 1) Fixed tasks (train and test), plot mult trials per task, in structured way (GOOD)\n",
    "    # sort based on supervision stage\n",
    "    stages = D.Dat[\"supervision_stage\"].unique()\n",
    "    # temporary fix for halted run; remove after done\n",
    "    print(\"stages\", stages)\n",
    "    for s in stages[5:]:\n",
    "    # end temporary fix\n",
    "    #for s in stages:\n",
    "        for t in [\"test\", \"train\"]:\n",
    "            Dthis = D.filterPandas({\"random_task\":[False], \"monkey_train_or_test\":[t], \"supervision_stage\":[s]}, \"dataset\")\n",
    "            if len(Dthis.Dat)>0:\n",
    "\n",
    "                #### 2) A single category, over all time\n",
    "                if PLOT_EXAMPLE_DATEVTASK:\n",
    "                    from pythonlib.dataset.dataset_analy.summary import plot_summary_drawing_examplegrid\n",
    "                    plot_summary_drawing_examplegrid(Dthis, SAVEDIR_FIGS, f\"{t}/stage{s}\", \n",
    "                                     \"date\")\n",
    "                    plot_summary_drawing_examplegrid(Dthis, SAVEDIR_FIGS, f\"{t}/stage{s}\", \n",
    "                                     \"epoch\")\n",
    "\n",
    "    #                 sdirthis = f\"{SAVEDIR_FIGS}/date_vs_task/stage_{s}\"\n",
    "    #                 os.makedirs(sdirthis, exist_ok=True)\n",
    "    #                 from pythonlib.dataset.plots import plot_beh_grid_grouping_vs_task\n",
    "\n",
    "    #                 taskcats = Dthis.Dat[\"task_stagecategory\"].unique()\n",
    "    #                 for tc in taskcats:\n",
    "    #                     tasklist = Dthis.Dat[Dthis.Dat[\"task_stagecategory\"]==tc][\"character\"].unique()\n",
    "    #                     row_variable = \"date\"\n",
    "\n",
    "    #                     LIST_N_PER_GRID = [1]\n",
    "    # #                     LIST_N_PER_GRID = [1, 3]\n",
    "    #                     for max_n_per_grid in LIST_N_PER_GRID:\n",
    "    #                         if max_n_per_grid==1:\n",
    "    #                             n = 4\n",
    "    #                         else:\n",
    "    #                             n=1\n",
    "    #                         for i in range(n):\n",
    "    #                             # Iterate, since is single plots.\n",
    "    #                             figb, figt = plot_beh_grid_grouping_vs_task(Dthis.Dat, row_variable, \n",
    "    #                                                                         tasklist, \n",
    "    #                                                                         max_n_per_grid=max_n_per_grid, \n",
    "    #                                                                        plotkwargs={\"strokes_by_order\":True})\n",
    "    #                             figb.savefig(f\"{sdirthis}/{tc}-npergrid{max_n_per_grid}-iter{i}-beh.pdf\");\n",
    "    #                             figt.savefig(f\"{sdirthis}/{tc}-npergrid{max_n_per_grid}-iter{i}-task.pdf\");\n",
    "    #                         plt.close(\"all\")\n",
    "\n",
    "                #### 1) A single task, over time.\n",
    "                if PLOT_ALL_EACHTRIAL:\n",
    "                    assert(\"fix this so that it splits train/test into separate plots -- see above\", False)\n",
    "\n",
    "                    sdirthis = f\"{SAVEDIR_FIGS}/each_task_all_trials/stage_{s}\"\n",
    "                    os.makedirs(sdirthis, exist_ok=True)\n",
    "\n",
    "                    tasklist = Dthis.Dat[\"character\"].unique()\n",
    "\n",
    "                    from pythonlib.dataset.plots import plot_beh_grid_singletask_alltrials, plot_beh_grid_flexible_helper\n",
    "                    for task in tasklist:\n",
    "                    #     task = \"mixture2_1-savedset-50-39276\"\n",
    "                        row_variable = \"date\"\n",
    "                        if False:\n",
    "                            # Old version\n",
    "                            figb, figt = plot_beh_grid_singletask_alltrials(D, task, row_variable, max_cols = MAX_COLS)\n",
    "                        else:\n",
    "                            # New version, plotting trialcode, and coloring by order\n",
    "                            Dthis = D.filterPandas({\"character\":task}, \"dataset\")\n",
    "                            figb, figt = plot_beh_grid_flexible_helper(Dthis, \"date\", \"trial\", plotkwargs={\"strokes_by_order\":True})\n",
    "                        figb.savefig(f\"{sdirthis}/{task}-beh.pdf\");\n",
    "                        figt.savefig(f\"{sdirthis}/{task}-task.pdf\");\n",
    "                        plt.close(\"all\")\n",
    "            else:\n",
    "                print(\"Dthis is blank, maybe something wrong with data selection?\")\n",
    "                    \n",
    "    ######### 2) Plot all training tasks (lumps random and fixed) into random grids.\n",
    "    if PLOT_TRAIN_GRID:\n",
    "        Dtrain = D.filterPandas({\"monkey_train_or_test\":[\"train\"]}, \"dataset\")\n",
    "\n",
    "        # 1) plot random train tasks (beh + test)\n",
    "        # separate plots for each level of grouping (e..g, epoch), each block, each supervision kind, date\n",
    "        stages = Dtrain.Dat[\"supervision_stage\"].unique()\n",
    "        dates = Dtrain.Dat[\"date\"].unique()\n",
    "        blocks = Dtrain.Dat[\"block\"].unique()\n",
    "\n",
    "        nrand = 20\n",
    "        niter = 1 \n",
    "        nmin = 3\n",
    "        \n",
    "        for lev in GROUPING_LEVELS:\n",
    "            for d in dates:\n",
    "                for s in stages:\n",
    "\n",
    "                    for b in blocks:\n",
    "                        inds = Dtrain.filterPandas({\n",
    "                            GROUPING:[lev],\n",
    "                            \"date\":[d],\n",
    "                            \"supervision_stage\":[s],\n",
    "                            \"block\":[b]})\n",
    "                        if len(inds)>nmin:\n",
    "                            print(\"running: \", (lev, d, s, b))\n",
    "\n",
    "                            # Savedir\n",
    "                            sdirthis = f\"{SDIR_MAIN}/FIGS/drawfigs/traintasks_randomgrid/{lev}/{d}/{s}\"\n",
    "                            os.makedirs(sdirthis, exist_ok=True)\n",
    "\n",
    "                            if len(inds)<1.5*nrand:\n",
    "                                # dont plot multipel times if not that many trials.\n",
    "                                niterthis = 1\n",
    "                            else:\n",
    "                                niterthis = niter\n",
    "\n",
    "                            for i in range(niterthis):\n",
    "                                # Plot these inds\n",
    "                                figbeh, indsthis = Dtrain.plotMultTrials(inds, \"strokes_beh\", return_idxs=True, nrand=nrand,\n",
    "                                                                        naked_axes=True, add_stroke_number=False)\n",
    "                                figtask = Dtrain.plotMultTrials(indsthis, \"strokes_task\", return_idxs=False, nrand=nrand,\n",
    "                                                                       naked_axes=True, add_stroke_number=False)\n",
    "\n",
    "                                # SAVE\n",
    "                                figbeh.savefig(f\"{sdirthis}/block{b}-iter{i}-beh.pdf\")\n",
    "                                figtask.savefig(f\"{sdirthis}/block{b}-iter{i}-task.pdf\")             \n",
    "\n",
    "                            plt.close(\"all\")\n",
    "    \n",
    "    ####### Random train tasks that were repeated.\n",
    "    if PLOT_TRAIN_REPEATED:\n",
    "        nmin_trials= 6 # min num repeated trials.\n",
    "        nmin_tasksplot = 100 # take top 100 tasks.\n",
    "        Dtrain = D.filterPandas({\"random_task\":[True], \"monkey_train_or_test\":[\"train\"]}, \"dataset\")\n",
    "\n",
    "        # Hash each task shapes\n",
    "        list_shapeshash = [] # each tasks shape hash\n",
    "\n",
    "        for i in range(len(Dtrain.Dat)):\n",
    "            list_shapeshash.append(Dtrain.Dat.iloc[i][\"Task\"].get_shapes_hash())\n",
    "\n",
    "\n",
    "        ## Get all inds that have the same task (defined by shapes)\n",
    "        # 1) assign back into dat the hash\n",
    "        Dtrain.Dat[\"Task_shapeshash\"] = list_shapeshash\n",
    "\n",
    "        # 2) plot one\n",
    "        if False:\n",
    "            import random\n",
    "            thishash = random.choice(list_shapeshash)\n",
    "            inds = Dtrain.filterPandas({\"Task_shapeshash\":[thishash]})\n",
    "            print(inds)\n",
    "            Dtrain.plotMultTrials(inds);\n",
    "            inds = inds[:20]\n",
    "            Dtrain.plotMultTrials(inds, \"strokes_task\");\n",
    "\n",
    "        sdirthis = f\"{SDIR_MAIN}/FIGS/drawfigs/traintasks_repeatedtrials\"\n",
    "        sdirthis_raw = f\"{SDIR_MAIN}/FIGS/drawfigs/traintasks_repeatedtrials/each_task\"\n",
    "        os.makedirs(sdirthis, exist_ok=True)\n",
    "        os.makedirs(sdirthis_raw, exist_ok=True)\n",
    "\n",
    "        # How often where tasks repeated?\n",
    "        a = Dtrain.Dat[\"Task_shapeshash\"].value_counts().tolist()\n",
    "\n",
    "        fig = plt.figure()\n",
    "        plt.hist(a, range(max(a)), log=True);\n",
    "        plt.ylabel('counts');\n",
    "        plt.xlabel('num repetitions per unique task')\n",
    "        plt.title(f\"This many unique tasks: {len(a)}\")\n",
    "        fig.savefig(f\"{sdirthis}/num_repeats_per_task-1.pdf\")\n",
    "\n",
    "        fig = plt.figure()\n",
    "        plt.hist(a, range(max(a)), log=False);\n",
    "        plt.ylabel('counts');\n",
    "        plt.xlabel('num repetitions per unique task')\n",
    "        plt.title(f\"This many unique tasks: {len(a)}\")\n",
    "        fig.savefig(f\"{sdirthis}/num_repeats_per_task-2.pdf\")\n",
    "\n",
    "\n",
    "        ### For each unique task, plot across days\n",
    "        # get all unique tasks with greater than N trials\n",
    "        a = Dtrain.Dat[\"Task_shapeshash\"].value_counts()\n",
    "        a = a[a>nmin_trials]\n",
    "        a = a[:nmin_tasksplot]\n",
    "\n",
    "        # save a dict of names\n",
    "        from pythonlib.tools.expttools import writeDictToYaml\n",
    "        namesdict = {i:a.index[i] for i in range(len(a))}\n",
    "        writeDictToYaml(namesdict, f\"{sdirthis}/tasknames.yaml\")\n",
    "\n",
    "        for i in range(len(a)):\n",
    "            name = a.index[i]\n",
    "            inds = Dtrain.filterPandas({\"Task_shapeshash\":[name]})\n",
    "            inds = inds[:20]\n",
    "            figbeh = Dtrain.plotMultTrials(inds);\n",
    "            figtask = Dtrain.plotMultTrials(inds, \"strokes_task\");\n",
    "\n",
    "            figbeh.savefig(f\"{sdirthis_raw}/task{i}-beh.pdf\")\n",
    "            figtask.savefig(f\"{sdirthis_raw}/task{i}-task.pdf\");\n",
    "\n",
    "            plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b0d340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull out chunkorder / 'rule'\n",
    "\n",
    "t = D.Dat.iloc[40]['Task']\n",
    "t.Params['input_params'].get_tasknew()['Objects']['ChunkList'] #implicitly defined via spatial locations\n",
    "\n",
    "# can also hard-code SESSION (column in dataframe), and check notes to see what rule used\n",
    "\n",
    "# look in pythonlib/preprocessDat and above, _groupingParams (add line with grouping='session',\n",
    "# grouping_levels=[zero, pi]), plantime_cats={1:zero, 2:pi...5:zero}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c5527f",
   "metadata": {},
   "source": [
    "#### investigating dataset for missing values..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377ebef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = D.Dat\n",
    "\n",
    "# slice out specific stage\n",
    "missing1 = df[\"supervision_stage\"]\n",
    "print(missing1.value_counts())\n",
    "\n",
    "# slice out specific date\n",
    "date = df[df[\"datetime\"].str.contains(\"220308\")]\n",
    "\n",
    "# plot all trials in each session\n",
    "import matplotlib.pyplot as plt\n",
    "for i in range(1,3):\n",
    "    session = date[date[\"session\"].eq(i)]\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    plt.plot(session[\"trial\"],session[\"task_stagecategory\"], \"-o\")\n",
    "\n",
    "# slice out specific task\n",
    "date[date[\"unique_task_name\"].str.contains(\"51788\")]\n",
    "\n",
    "# examine all TEST tasks, see if they are displayed in both horiz/vert (for chunkbyshape2)\n",
    "test_data = df.loc[df[\"monkey_train_or_test\"]==\"test\"]\n",
    "test_task_names = test_data[\"unique_task_name\"].unique()\n",
    "\n",
    "print(\"TEST TASKS\")\n",
    "for t in test_task_names:\n",
    "    all_tasks_with_name_t = test_data.loc[test_data[\"unique_task_name\"]==t]\n",
    "    print(all_tasks_with_name_t[\"epoch\"].unique()) # should print ['vert', 'horiz'] for ALL tasks\n",
    "    \n",
    "# examine all TRAIN tasks, see if they are displayed in both horiz/vert (for chunkbyshape2)\n",
    "train_data = df.loc[df[\"monkey_train_or_test\"]==\"train\"]\n",
    "train_task_names = train_data[\"unique_task_name\"].unique()\n",
    "\n",
    "print(\"TRAIN TASKS\")\n",
    "for t in train_task_names:\n",
    "    all_tasks_with_name_t = train_data.loc[train_data[\"unique_task_name\"]==t]\n",
    "    print(all_tasks_with_name_t[\"epoch\"].unique())\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ba090b",
   "metadata": {},
   "source": [
    "##### doing some basic data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc15473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates behclass for all trials\n",
    "D.behclass_generate_alltrials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b1e517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "df = D.Dat\n",
    "trial = random.choice(range(len(df)))\n",
    "\n",
    "D.plotSingleTrial(trial);\n",
    "beh = df.iloc[trial][\"BehClass\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f30b97",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "beh.alignsim_compute(True)\n",
    "beh.alignsim_plot_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c035e061",
   "metadata": {},
   "outputs": [],
   "source": [
    "beh.alignsim_extract_datsegs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202ffa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### pipeline for calculating verticality/diagonality scores for all trials\n",
    "\n",
    "#D.behclass_generate_alltrials()\n",
    "df = D.Dat\n",
    "df[\"verticality\"] = np.nan\n",
    "df[\"diagonality\"] = np.nan\n",
    "\n",
    "#### loop\n",
    "for i in range(0, len(df)):\n",
    "    # grab behclass object for this row\n",
    "    beh = df.iloc[i][\"BehClass\"]\n",
    "    # compute alignsim\n",
    "    beh.alignsim_compute(True)\n",
    "    # get list of datsegs\n",
    "    datsegs = beh.alignsim_extract_datsegs()\n",
    "\n",
    "    # calculations\n",
    "    num_v = 0\n",
    "    num_h = 0\n",
    "    num_d = 0\n",
    "    \n",
    "    # count total h,v moves\n",
    "    for j in range(0, len(datsegs)):\n",
    "        seg = datsegs[j]\n",
    "        hv_prev = seg[\"h_v_move_from_prev\"]\n",
    "\n",
    "        if hv_prev==\"vertical\":\n",
    "            num_v = num_v + 1\n",
    "        elif hv_prev==\"horizontal\":\n",
    "            num_h = num_h + 1\n",
    "        elif hv_prev==\"diagonal\":\n",
    "            num_d = num_d + 1\n",
    "        else: #start\n",
    "            print(\"hv_prev is start\")\n",
    "\n",
    "    verticality = np.nan\n",
    "    diagonality = np.nan\n",
    "    # ratio score between [0,1]; 1 is VERTICAL, 0 is HORIZONTAL, 0.5 is EQUAL\n",
    "    if num_v == 0 and num_h == 0: # likely just 1 stroke\n",
    "        verticality = -1\n",
    "    else:\n",
    "        verticality = num_v/(num_v + num_h)\n",
    "    \n",
    "    # ratio score between [0, 1]; 1 is super diagonal, lots of switching\n",
    "    if num_d == 0: # two cases\n",
    "        if num_v == 0 and num_h == 0:\n",
    "            diagonality = -1 # likely just 1 stroke\n",
    "        else:\n",
    "            diagonality = 0.0 # could be horizontal, or vertical, task; true diagonality score 0.0\n",
    "    else:\n",
    "        diagonality = num_d/(num_d + num_v + num_h)\n",
    "        \n",
    "    df.loc[i,[\"verticality\"]] = verticality ## must use loc, not chained-indexing\n",
    "    df.loc[i,[\"diagonality\"]] = diagonality ## must use loc, not chained-indexing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a70197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the construct validity of verticality/diagonality scores\n",
    "\n",
    "# compare verticality score to actual plot\n",
    "import random\n",
    "trial = random.choice(range(len(df)))\n",
    "\n",
    "print(\"trial: \", trial)\n",
    "print(\"verticality: \", df.iloc[trial]['verticality'])\n",
    "D.plotSingleTrial(trial);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cb8157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each TASK: find average verticality score in VERTICAL, HORIZONTAL conditions; plot 2-pt line\n",
    "import math\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "df_cleaned = df[df['verticality']!=-1]\n",
    "df_cleaned = df_cleaned[df_cleaned[\"monkey_train_or_test\"]==\"test\"]\n",
    "# get all unique task names\n",
    "unique_tasks = df_cleaned['unique_task_name'].unique()\n",
    "\n",
    "# create blank dataframe\n",
    "plot_data = pd.DataFrame(columns=['unique_task_name', 'verticality_average', 'epoch'])\n",
    "\n",
    "# loop through unique task names\n",
    "for t in unique_tasks:\n",
    "    t_done = df_cleaned[df_cleaned['unique_task_name']==t]\n",
    "    \n",
    "    # get task data from vertical sessions\n",
    "    td_vert = t_done[t_done['epoch']=='vert']\n",
    "    tdv_avg = td_vert['verticality'].mean()\n",
    "    \n",
    "    # get task data from horizontal sessions\n",
    "    td_horiz = t_done[t_done['epoch']=='horiz']\n",
    "    tdh_avg = td_horiz['verticality'].mean()\n",
    "    \n",
    "    # add entry to plot_dataframe\n",
    "    if math.isnan(tdv_avg) or math.isnan(tdh_avg):\n",
    "        continue # task wasn't shown in both vertical/horizontal epochs; or data was all pruned for one\n",
    "    else:\n",
    "        plot_data.loc[len(plot_data)] = [t, tdv_avg, 1.0]\n",
    "        plot_data.loc[len(plot_data)] = [t, tdh_avg, 2.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cfe4c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8998360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot both upward/downward sloping\n",
    "\n",
    "# df_total = pd.concat([plot_data1,plot_data2]).reset_index(drop=True)\n",
    "sns.lineplot(data=plot_data[plot_data[\"epoch\"].isin([1.0, 2.0])], x=\"epoch\", y=\"verticality_average\",\n",
    "             hue=\"unique_task_name\",legend=False)\n",
    "print(\"n:\", len(plot_data)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8ac918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paired t-test\n",
    "from scipy import stats\n",
    "\n",
    "pd_v = plot_data[plot_data[\"epoch\"]==1].reset_index(drop=True)[\"verticality_average\"]\n",
    "pd_h = plot_data[plot_data[\"epoch\"]==2].reset_index(drop=True)[\"verticality_average\"]\n",
    "\n",
    "stats.ttest_rel(pd_v, pd_h) # H0: two samples are the SAME; if p < 0.05, can reject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7a2ea7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# histogram of differences\n",
    "diff = pd_v - pd_h\n",
    "plt.hist(diff)\n",
    "plt.axvline(0,color='red')\n",
    "plt.xlabel('difference in verticality (epoch_v-epoch_h)')\n",
    "plt.ylabel('occurrences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413e0eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add delta into new version of plot_data; contains 1 copy of each test task\n",
    "plot_data_epoch1 = plot_data[plot_data[\"epoch\"]==1].reset_index(drop=True)\n",
    "plot_data_epoch1['verticality_delta'] = diff\n",
    "\n",
    "# compare actual trial to difference\n",
    "import random\n",
    "task_num = random.choice(range(len(plot_data_epoch1)))\n",
    "task_name = plot_data_epoch1.iloc[task_num]['unique_task_name']\n",
    "task_delta = plot_data_epoch1.iloc[task_num]['verticality_delta']\n",
    "\n",
    "print(\"unique_task_name: \", task_name)\n",
    "print(\"verticality_delta: \", task_delta)\n",
    "\n",
    "# plot 1 trial from vertical (epoch=1) and 1 from horizontal (epoch=2)\n",
    "trial_pool = df[df['unique_task_name']==task_name]\n",
    "trial_v = trial_pool[trial_pool['epoch']=='vert'].index\n",
    "trial_h = trial_pool[trial_pool['epoch']=='horiz'].index\n",
    "\n",
    "# and see if calculated delta matches up to difference b/w vertical(top) & horizontal(bottom) figs\n",
    "if len(trial_v)>0 and len(trial_h)>0:\n",
    "    trial_num_v = trial_v[random.choice(range(len(trial_v)))]\n",
    "    trial_num_h = trial_h[random.choice(range(len(trial_v)))]\n",
    "    print(\"trial_num_v: \", trial_num_v)\n",
    "    print(\"trial_num_h: \", trial_num_h)\n",
    "    D.plotSingleTrial(trial_num_v);\n",
    "    D.plotSingleTrial(trial_num_h);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c42307",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[\"tvalfake\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa13e325",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[\"datetime\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f104aac0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot verticality vs time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot tvalfake vs. verticality (first step)\n",
    "fig,axes = plt.subplots(1,1,figsize=(12, 5))\n",
    "axes.plot(df[\"tvalfake\"], df[\"verticality\"], \".k\")\n",
    "axes.set_xlabel(\"tval\")\n",
    "axes.set_ylabel(\"verticality_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e22686b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## DETAILED VERTICALITY PLOTS\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "df_cleaned = df[df[\"verticality\"]!=-1]\n",
    "# replicate above plot (tvalfake vs. verticality)\n",
    "sns.pairplot(df_cleaned, x_vars=['tvalfake'], y_vars=['verticality'], height=5, aspect=2, hue='epoch')\n",
    "\n",
    "# split into two separate plots, by epoch\n",
    "sns.relplot(data=df_cleaned, x='tvalfake', y='verticality', height=5, aspect=2, row='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f01483",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# now plot each day's mean, against time\n",
    "# plots 95% confidence interval (of mean, b/c of noise)\n",
    "\n",
    "sns.catplot(data=df_cleaned, x='tvalday', y='verticality', height=5, aspect=2, hue='epoch',\n",
    "            kind='point', row='monkey_train_or_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0a7694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIAGONALITY PLOTS\n",
    "import seaborn as sns\n",
    "\n",
    "df_cleaned = df[df[\"diagonality\"]!=-1]\n",
    "# replicate above plot (tvalfake vs. verticality)\n",
    "sns.pairplot(df_cleaned, x_vars=['tvalfake'], y_vars=['diagonality'], height=5, aspect=2, hue='epoch')\n",
    "\n",
    "# split into two separate plots, by epoch\n",
    "sns.relplot(data=df_cleaned, x='tvalfake', y='diagonality', height=5, aspect=2, row='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39e9084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now plot each day's mean, against time\n",
    "# plots 95% confidence interval (of mean, b/c of noise)\n",
    "\n",
    "sns.catplot(data=df_cleaned, x='tvalday', y='diagonality', height=5, aspect=2, hue='epoch',\n",
    "            kind='point', row='monkey_train_or_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ebd700",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
