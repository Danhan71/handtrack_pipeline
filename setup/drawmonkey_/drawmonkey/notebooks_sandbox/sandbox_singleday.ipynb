{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T12:54:10.975491219Z",
     "start_time": "2024-03-19T12:54:10.970198644Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "DEVELOPing stuff, \n",
    "single day anslysis\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T12:54:10.980460179Z",
     "start_time": "2024-03-19T12:54:10.970396459Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%cd ..\n",
    "\n",
    "from tools.utils import * \n",
    "from tools.plots import *\n",
    "from tools.analy import *\n",
    "from tools.calc import *\n",
    "from tools.analyplot import *\n",
    "from tools.preprocess import *\n",
    "from tools.dayanalysis import *\n",
    "from analysis.line2 import *\n",
    "from analysis.probedatTaskmodel import *\n",
    "from pythonlib.drawmodel.analysis import *\n",
    "from pythonlib.tools.stroketools import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD SINGLE DAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load filedata, quick\n",
    "# a = \"Pancho\"\n",
    "# d = 231220\n",
    "# e = \"gramdirstimpancho5c\"\n",
    "# s = 1\n",
    "\n",
    "# mult prims\n",
    "# a = \"Pancho\"\n",
    "# d = 221125\n",
    "# e = \"dirshapecolor4b\"\n",
    "# s = 1\n",
    "\n",
    "\n",
    "a = \"Diego\"\n",
    "d = 230615\n",
    "e = \"priminvar5\"\n",
    "s = 2\n",
    "\n",
    "fd = loadSingleDataQuick(a, d, e, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in getIndsTrials(fd):\n",
    "    print(i, \"---\", getTrialsUniqueTasknameGood(fd, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd[\"trials\"][80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd[\"params\"][\"n_trialoutcomes\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find a filename -- a general helper, works across diff directories (e.g., server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal = \"Diego\"\n",
    "date = 240510\n",
    "extension = \"bhv2\"\n",
    "findFilename(animal, date, extension=extension, take_larger_file=False, return_all_files=True, return_files_sorted=True,\n",
    "             return_file_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal = \"Diego\"\n",
    "date = 240510\n",
    "extension = \"bhv2\"\n",
    "outdict = findFilenamesGood(animal, date, extension)\n",
    "outdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_decide_storage_server_where_load(animal, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatically generating dataset and making simple summaries`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate metadat\n",
    "exptname = 'concrete2b'\n",
    "date = 230426\n",
    "animal = \"Diego\"\n",
    "from analysis.dataset import generate_metadat\n",
    "generate_metadat(exptname, date, date, animal, overwrite=True)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Generate the datsaet file from raw\n",
    "from analysis.dataset import generate_dataset_file_from_raw\n",
    "\n",
    "rulelist = [str(date)]\n",
    "generate_dataset_file_from_raw(animal, exptname, dataset_dir_kind=\"daily\", rulelist=rulelist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot, simple summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get trials peanut extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd[\"BlockParamsByTrial\"][\"RunParams\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fd[\"TrialRecord\"][\"User\"][\"RunParams\"][\"BlockParams\"][\"10\"][\"params_task\"]\n",
    "fd[\"TrialRecord\"][\"User\"][\"RunParams\"][\"BlockParamsHotkeyUpdatedv2\"][\"1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BlockParams = getTrialsBlockParamsDefault_(fd, None, \"RunParams\")\n",
    "BlockParams[\"10\"][\"params_task\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getTrialsPeanutSampCollisExt(fd, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot ACTUAL peanut size (look into adapter params) [4/28/23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_pnut = []\n",
    "trials = getIndsTrials(fd)\n",
    "for t in trials:\n",
    "    bb = getTrialsAdapterParams(fd, t)\n",
    "    if len(bb)>0:\n",
    "        pnut = bb[\"PnutSampCollisExt\"]\n",
    "    else:\n",
    "        pnut = np.nan\n",
    "        \n",
    "    list_pnut.append(pnut)\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(trials, list_pnut, \"xk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with probedat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis.modelexpt import loadProbeDatWrapper, loadMultDataForExpt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load\n",
    "# a = \"Pancho\"\n",
    "# d = 220328\n",
    "# e = \"concretechunk1\"\n",
    "a = \"Pancho\"\n",
    "d = 240524\n",
    "e = \"primpancho1h\"\n",
    "# s = 1\n",
    "\n",
    "dattoget = (e, a, d)\n",
    "FD = loadMultData([dattoget])\n",
    "# FD = loadMultDataForExpt(e, a, whichdates=[0,1])\n",
    "PROBEDAT = loadProbeDatWrapper(FD)\n",
    "P  = ProbedatTaskmodel(PROBEDAT)\n",
    "\n",
    "P = P.clean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract blockparams for saving dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BlockParamsByDateSessBlock = P.extractBlockParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BlockParamsByDateSessBlock[(220812, 1, 5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract rule and taskresequencibg stuff for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INGORE; instead, save the blockparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conditioned on a fixed task, plot scores over day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.plotOverviewTaskPerformance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pp = P.pandas()\n",
    "Pp.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "P.plotOverviewTaskPerformance(\"/tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.extract_feature_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### try extracting taskclass representation fo tasks (as wuld do in dataset extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule = 'horiz'\n",
    "extraction_params = {\n",
    "    \"expt\":e,\n",
    "    \"animal\":a,\n",
    "    \"rule\":rule,\n",
    "    \"probedat_filter_params\":{},\n",
    "    \"pix_add_to_sketchpad_edges\":20,\n",
    "#     \"savedir\":f\"{base_dir}/database/BEH\",\n",
    "    \"savenote\":''\n",
    "                    }\n",
    "ProbedatFiltered = P.filterByBehPerformance(extraction_params[\"probedat_filter_params\"])\n",
    "\n",
    "# Reconstruct P\n",
    "P = ProbedatTaskmodel(ProbedatFiltered, P.Metadat)\n",
    "P = P.clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tasks = P.extractTasksAsClass(\"all\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = Tasks[-1]\n",
    "\n",
    "T.plotTaskOnAx()\n",
    "T.objects_extract()\n",
    "\n",
    "dat = T.planclass_extract_all()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert tasks to TasksGeneral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonlib.drawmodel.taskgeneral import TaskClass as TaskClassGen\n",
    "\n",
    "# convert all to general class\n",
    "Tasks2 = []\n",
    "for i, T in enumerate(Tasks):\n",
    "    Tasks2.append(TaskClassGen())\n",
    "    Tasks2[-1].initialize(ver=\"ml2\", params=T)\n",
    "Tasks = Tasks2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fixing task setnum extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pythonlib.drawmodel.tasks import get_task_probe_info\n",
    "get_task_probe_info(T.Task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = Tasks[505]\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "T.plotTaskOnAx(ax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot list of handpicked trials, by unique name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pp = P.pandas()\n",
    "\n",
    "Pp[\"unique_task_name\"]\n",
    "\n",
    "list_names = [\n",
    "    'mixture2_1-savedset-18-58507-136216', \n",
    "]\n",
    "inds = Pp[\"unique_task_name\"].isin(list_names)\n",
    "inds = Pp[inds].index.tolist()\n",
    "\n",
    "P.plotMultTrials(inds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### print out all tasks and N trials on those tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONE! See this in preprocess code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.print_all_tasknames(\"/tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter, so only print for probe tasks\n",
    "P.print_all_tasknames(\"/tmp\", pandasfilter = {'probe':[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Doesnt do anything\n",
    "    P.pandasAddBasicColumns({\"unique_task_name\":lambda fd, t: getTrialsUniqueTasknameGood(fd, t, nhash=8)}, if_exists=\"overwrite\")\n",
    "    Pp = P.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonlib.tools.pandastools import applyFunctionToAllRows\n",
    "TaskClass(Pp.iloc[0][\"Task\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonlib.drawmodel.tasks import TaskClass\n",
    "T = TaskClass([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Print out scores for all trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.print_all_feature_scores(\"/tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.extract_feature_list_frompandas(True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.print_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.ListFeatureNames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.getTrialsHelper('getTrialsOutcomesWrapper', [0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### General purpose, adding new columns to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See this\n",
    "P._pandasAddComputedColumns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.ListFeatureNames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Overlay score thresholds on top of beh features distrubitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurelist = P.extract_feature_list()\n",
    "fdirthis = f\"{fdir}/beh_eval_features\"\n",
    "os.makedirs(fdirthis, exist_ok=True)\n",
    "\n",
    "blocks_to_plot = list(set(P.pandas()[\"block\"]))\n",
    "for bk in blocks_to_plot:\n",
    "    filtdict = {\"block\":[bk]}\n",
    "    PD = P.filterProbedat(filtdict, modify_in_place = False)\n",
    "    if len(PD)==0:\n",
    "        continue\n",
    "    Pfilt = ProbedatTaskmodel(PD)\n",
    "    if len(Pfilt.getIndsTrials())>3:\n",
    "        featurelist_this = [f for f in featurelist if f in Pfilt.pandas().columns]\n",
    "        try:\n",
    "            fig = sns.pairplot(Pfilt.pandas(), vars=featurelist_this, hue=\"kind\")\n",
    "            fig.savefig(f\"{fdirthis}/allfeatures_pairwise-block{bk}.pdf\")\n",
    "        except Exception as err:\n",
    "            pass\n",
    "            # print(\"pandas:\", Pfilt.pandas())\n",
    "            # print(\"featurelist\", featurelist_this)\n",
    "            # raise err\n",
    "featurelist_this = [f for f in featurelist if f in P.pandas().columns]\n",
    "fig = sns.pairplot(P.pandas(), vars=featurelist_this, hue=\"kind\")\n",
    "fig.savefig(f\"{fdirthis}/allfeatures_pairwise.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurelist_this = [f for f in feature_list if f in P.pandas().columns]\n",
    "fig = sns.pairplot(P.pandas(), vars=featurelist_this, hue=\"kind\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "P.plot_featuredists_overlying_params(\"/tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = {\n",
    "    \"ErrorCode\":\"getTrialsErrorCode\",\n",
    "    \"IsAbort\":\"getTrialsIsAbort\"\n",
    "}\n",
    "P.pandasAddBasicColumns(col_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = P.filterPandas({\"IsAbort\":[False]}, return_indices=True)\n",
    "P2 = P.subsampleProbedat(inds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P2.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = P.fd(300)\n",
    "T = getTrialsTask(fd, 206)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getTrialsOutcomesWrapper(fd, 206)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T[\"TaskNew\"][\"Objects\"][\"StrokindsDone\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getTrialsBlock(fd, 206)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getTrialsBlock(fd, 166)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot/print peanut size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.getTrialsHelper(\"getTrialsPnutSampCollisExt\", [0,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot over trials\n",
    "cols_to_add = {\"pnut_ext\":P.getTrialsHelper(\"getTrialsPnutSampCollisExt\", \"all\")}\n",
    "Pp = P.pandas()\n",
    "P.pandasAddColumns(cols_to_add)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Print reason for abort (objectclass rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess plots that work on fd (not on probedat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pnut size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "x = {1:{2:3}}\n",
    "y = x\n",
    "y[1][2] = 4\n",
    "print(x)\n",
    "\n",
    "x = {1:{2:3}}\n",
    "y = copy.deepcopy(x)\n",
    "y[1][2] = 4\n",
    "print(x)\n",
    "\n",
    "x = {1:{2:3}}\n",
    "y = {k:copy.copy(v) for k, v in x.items()}\n",
    "y[1][2] = 4\n",
    "print(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BlockParams = {1:{\"field1\":{\"field2\":1}}}\n",
    "\n",
    "# BlockParams[block][field1][field2]\n",
    "BlockParams2[1] = copy.deepcopy(BlockParams[1])\n",
    "# BlockParams2 = BlockParams\n",
    "# BlockParams2 = {k:v for k, v in BlockParams.items()}\n",
    "BlockParams2[1][\"field1\"][\"field2\"] = copy.deepcopy(BlockParams[1][\"field1\"][\"field2\"])\n",
    "BlockParams2[1][\"field1\"][\"field2\"] = 2\n",
    "\n",
    "BlockParams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.utils import getTrialsPeanutSampCollisExt, getMultTrialsBlockParamsAllHotkeyUpdated_, getTrialsBlockParamsAllHotkeyUpdatedv2_\n",
    "BP = getMultTrialsBlockParamsAllHotkeyUpdated_(fd, \"RunParams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BP = getTrialsBlockParamsAllHotkeyUpdatedv2_(fd, 10, \"TaskParams\")\n",
    "BP.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check, plot peanutsize\n",
    "\n",
    "pnut_by_block = {}\n",
    "trial_by_block = {}\n",
    "for trial in getIndsTrials(fd):\n",
    "    \n",
    "    bk = getTrialsBlock(fd, trial)\n",
    "    pnutsize = getTrialsPeanutSampCollisExt(fd, trial)\n",
    "    \n",
    "    print(trial, bk, pnutsize)\n",
    "    if bk not in pnut_by_block.keys():\n",
    "        pnut_by_block[bk] = [pnutsize]\n",
    "        trial_by_block[bk] = [trial]\n",
    "    else:\n",
    "        pnut_by_block[bk].append(pnutsize)\n",
    "        trial_by_block[bk].append(trial)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "list_bks = list(pnut_by_block.keys())\n",
    "for bk in list_bks[10:20]:\n",
    "#     bk = 39\n",
    "    trials = trial_by_block[bk]\n",
    "    pnuts = pnut_by_block[bk]\n",
    "    plt.plot(trials, pnuts, '-o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PRINT PNUT SIZES For each block.\n",
    "{k:set(v) for k, v in pnut_by_block.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonlib.tools.monkeylogictools import dict2list2\n",
    "\n",
    "# x = {'1': 'transform', '2': np.array([[0.]])}\n",
    "# x = {'1': {'transform':1}, '2':{\"test\":np.array([[0.]])}}\n",
    "\n",
    "# x = {'1': {'transform':\"test\"}, '2': {'transform':\"test\"}}\n",
    "x = {'transform':\"test\"}\n",
    "dict2list2(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Other stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.analy import extractSessionDf\n",
    "\n",
    "fd = P.fd(0)\n",
    "df = extractSessionDf(fd)\n",
    "featurestoplot = getMultTrialsBehEvalFeatures(fd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[~(df[\"trial_end_method\"]==\"online_abort\")]\n",
    "plotBehSortedByScore(df, fd, \"hausdorff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Drawings sorted by scores, plot this separately for each block (not just all blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONE - added to preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for block in df[\"block\"].unique():\n",
    "    print(block)\n",
    "\n",
    "    SDIR = \"/tmp/trialsSortedByScore_ByBlock\"\n",
    "    os.makedirs(SDIR, exist_ok=True)\n",
    "\n",
    "    blocknum = 2\n",
    "    dfthis = df[df[\"block\"]==blocknum]\n",
    "    scoretypes = featurestoplot\n",
    "    scoretypes.extend([\"behscore\", \"reward\"])\n",
    "\n",
    "    for score_type in scoretypes:\n",
    "        FIGS = plotBehSortedByScore(dfthis, fd, score_type)\n",
    "\n",
    "        for ver, figs in FIGS.items():\n",
    "            for i, fig in enumerate(figs):\n",
    "                fig.savefig(f\"{SDIR}/{score_type}_{ver}_{i}_.pdf\")\n",
    "                assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert to dataset, if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis.dataset import Probedat2Dat\n",
    "pix_add_to_sketchpad_edges = 20\n",
    "extraction_params = {\n",
    "    \"expt\":e,\n",
    "    \"animal\":a,\n",
    "#                 \"probedat_filter_params\":{\n",
    "#                     \"hausdorff_filter\":True,\n",
    "#                     \"hausdorff_filter_prctile\":2.5,\n",
    "#                     },\n",
    "    \"rule\":[],\n",
    "    \"probedat_filter_params\":{},\n",
    "    \"pix_add_to_sketchpad_edges\":pix_add_to_sketchpad_edges,\n",
    "    \"savedir\":\"/data2/analyses/database/BEH\",\n",
    "    \"savenote\":\"\"\n",
    "}\n",
    "\n",
    "# # ==== filter trials based on behavioral criteria, to throw out noise.\n",
    "# ProbedatFiltered = P.filterByBehPerformance(extraction_params[\"probedat_filter_params\"])\n",
    "\n",
    "# # Reconstruct P\n",
    "# P = ProbedatTaskmodel(ProbedatFiltered, P.Metadat)\n",
    "\n",
    "# === Convert Probedat to DAT\n",
    "DAT, METADAT = Probedat2Dat(P, extraction_params, save=False, keep_all_in_probedat=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To cleanup dir in preprocessing (no redundant files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"Pancho\"\n",
    "d = 240510\n",
    "e = \"primpancho1d\"\n",
    "# s = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "glob.glob(os.path.splitext(fnames[0])[0] + \".pkl\")\n",
    "\n",
    "\n",
    "OVERWRITE=False\n",
    "filedata = loadSingleData(a, d, e, 1, resave_as_dict=False, \n",
    "                          load_resaved_data=False, resave_overwrite=False)\n",
    "\n",
    "\n",
    "date, time, expt, animal, sess, ext = filename2params(fnames[0], return_ext=True)\n",
    "\n",
    "findFilename(a, d, expt, sess, extension=\"pkl\", return_all_files=True,\n",
    "                                 doprint=False)\n",
    "\n",
    "# f = \"data2/animals/Red/210517/210517_152802_plan3_Red_1.pkl\"\n",
    "f = \"/data2/animals/Red/210520/210520_000000_plan5_Red_1.bhv2\"\n",
    "pathdir, pathend = os.path.split(f)\n",
    "pathname, ext = os.path.splitext(pathend)\n",
    "from pythonlib.tools.expttools import fileparts, modPathFname\n",
    "# fileparts(f)\n",
    "modPathFname(f, \"test\", \"test\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from tools.preprocess import cleanupDir\n",
    "cleanupDir(a, d)\n",
    "\n",
    "from pythonlib.tools.expttools import findPath\n",
    "s =1\n",
    "index = [\"/data2/animals\", [[a], [d], [d, a, s]], None, \"bhv2\"]\n",
    "pathlist1 = findPath(*index, sort_by=\"date\")\n",
    "pathlist2 = findPath(*index, sort_by=\"size\")\n",
    "\n",
    "# sort by size and date. \n",
    "assert pathlist1[-1] == pathlist2[-1], \"latest file must also be the largest\"\n",
    "\n",
    "for path in pathlist[:-1]:\n",
    "    modPathFname(path, \"IGNORE\")\n",
    "\n",
    "# Load, as in preprocess, loading h5\n",
    "OVERWRITE=True\n",
    "filedata = loadSingleData(a, d, e, 1, resave_as_dict=True, \n",
    "                          load_resaved_data=False, resave_overwrite=OVERWRITE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"Red\"\n",
    "d = \"210605\"\n",
    "from tools.preprocess import cleanupDir\n",
    "cleanupDir(a, d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RANDOM stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T21:37:07.458546790Z",
     "start_time": "2024-01-02T21:37:07.328412933Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%cd ..\n",
    "\n",
    "from tools.utils import * \n",
    "from tools.plots import *\n",
    "from tools.analy import *\n",
    "from tools.calc import *\n",
    "from tools.analyplot import *\n",
    "from tools.preprocess import *\n",
    "from tools.dayanalysis import *\n",
    "from analysis.line2 import *\n",
    "from analysis.probedatTaskmodel import *\n",
    "from pythonlib.drawmodel.analysis import *\n",
    "from pythonlib.tools.stroketools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T12:55:10.349709405Z",
     "start_time": "2024-03-19T12:55:07.052947684Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load filedata, quick\n",
    "# a = \"Pancho\"\n",
    "# d = 231220\n",
    "# e = \"gramdirstimpancho5c\"\n",
    "# s = 1\n",
    "\n",
    "# mult prims\n",
    "# a = \"Pancho\"\n",
    "# d = 221125\n",
    "# e = \"dirshapecolor4b\"\n",
    "# s = 1\n",
    "\n",
    "a = \"Diego\"\n",
    "d = 240515\n",
    "e = \"primdiego1g\"\n",
    "s = 3\n",
    "\n",
    "fd = loadSingleDataQuick(a, d, e, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T12:55:23.334234886Z",
     "start_time": "2024-03-19T12:55:13.648383956Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fd[\"trials\"][80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T22:37:00.193403715Z",
     "start_time": "2024-01-02T22:37:00.026755328Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fd[\"params\"][\"n_trialoutcomes\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print reasons failed for objectclass failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for t in getIndsTrialsSimple(fd):\n",
    "    o = getTrialsOutcomesWrapper(fd, t)\n",
    "    \n",
    "    if o[\"trial_end_method\"]==\"online_abort\" and o[\"online_abort\"][\"failure_mode\"] == \"failed_rule_objectclass\":\n",
    "#         print(t)\n",
    "    \n",
    "        T = getTrialsTaskClass(fd, t)\n",
    "        T.objectclass_extract_all()\n",
    "        O = T.ObjectClass\n",
    "        for rule, failcount in zip(O[\"RuleList\"], O[\"RuleFailureTracker\"]):\n",
    "            if failcount>0:\n",
    "                print(t, rule[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.ObjectClass[\"RuleList\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 12/21/22 - check strokes for \"novel_prims\"\n",
    "\n",
    "##### ALSO: finalizing primitives extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "GLC = True\n",
    "for ind in getIndsTrials(fd):\n",
    "#     T = getTrialsTaskClass(fd, ind)\n",
    "#     if len(T.Strokes)==0:\n",
    "#         print(ind)\n",
    "    print(ind)\n",
    "    T = getTrialsTaskClassGeneral(fd, ind)\n",
    "    \n",
    "#     strokes_task = getTrialsTaskAsStrokes(fd, ind)\n",
    "#     if len(strokes_task)==0:\n",
    "#         print(ind)\n",
    "        \n",
    "#     taskstruct = getTrialsTask(fd, ind)\n",
    "#     if len(taskstruct[\"strokes\"])==0:\n",
    "    T.tokens_generate(hack_is_gridlinecircle=GLC)\n",
    "    \n",
    "    if len(T.Strokes)>10:\n",
    "        assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.info_extract_all_prim_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_grid_ver = []\n",
    "for ind in getIndsTrials(fd):\n",
    "#     T = getTrialsTaskClass(fd, ind)\n",
    "#     if len(T.Strokes)==0:\n",
    "#         print(ind)\n",
    "    T = getTrialsTaskClassGeneral(fd, ind)\n",
    "    list_grid_ver.append(T.get_grid_ver())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Single task, print and plot useful things\n",
    "\n",
    "ind = random.choice(getIndsTrials(fd))\n",
    "plotTrialSimple(fd, ind)\n",
    "\n",
    "T = getTrialsTaskClassGeneral(fd, ind)\n",
    "\n",
    "display(T.get_grid_params())\n",
    "display(T.get_grid_ver())\n",
    "display(T.tokens_generate(hack_is_gridlinecircle=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 11/30/22 - new condensed version of blockparams hotkey updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.analy import extractSessionDf\n",
    "\n",
    "df = extractSessionDf(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "# sns.lineplot(ax=ax, data=df, x=\"trial\", y=\"reward_max\", label=\"reward_max\")\n",
    "sns.lineplot(ax=ax, data=df, x=\"trial\", y=\"reward_max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in getIndsTrials(fd):\n",
    "    print(getTrialsBlockParamsHotkeyUpdated(fd, t)[\"progression\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.utils import *\n",
    "# RP = getTrialsBlockParamsAllHotkeyUpdated(fd, ver=\"RunParams\")\n",
    "ver = \"RunParams\"\n",
    "trial = 500\n",
    "getTrialsBlockParamsAllHotkeyUpdated(fd, trial, ver=ver, DEBUG=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BP1 = getTrialsBlockParamsHotkeyUpdated(fd, 100)\n",
    "BP2 = getTrialsBlockParamsHotkeyUpdated(fd, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getTrialsBlockParamsDefault(fd, 363, \"TaskParams\")[\"TaskParams\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getTrialsTaskParams(fd, 363)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getTrialsSketchpad(fd, 363)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fd, 363)\n",
    "getTrialsUniqueTasknameGood(fd, 363)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in getIndsTrials(fd):\n",
    "    print(i)\n",
    "    getTrialsUniqueTasknameGood(fd, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Looking at blockparams hotkey updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd[\"TrialRecord\"][\"User\"][\"RunParams\"][\"BlockParamsHotkeyUpdated\"][\"80\"][\"trial_of_update\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd[\"TrialRecord\"][\"User\"][\"RunParams\"][\"BlockParamsDefaults\"][\"params_task\"].keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del fd[\"BlockParamsByTrial\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "BP = getMultTrialsBlockParamsAllHotkeyUpdated_(fd, \"RunParams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks_updated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BP[int(block)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extracting actual color used on each trial in adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd[\"TrialRecord\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd[\"TrialRecord\"][\"User\"][\"AdapterParams\"][\"5\"][\"guide\"][\"strokes\"][\"colors\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd[\"TrialRecord\"][\"User\"][\"AdapterParams\"][\"5\"][\"bb\"][\"InkColorsByPt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Doing things with TrialRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "vals = fd[\"TrialRecord\"][\"User\"][\"tasks_previously_done\"][\"x_character_85\"]\n",
    "x = np.arange(len(vals))\n",
    "plt.bar(x, vals.squeeze())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getTrialsBlock(fd, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "BP = getTrialsBlockParams(fd, 500)\n",
    "BP[\"TaskSet\"][\"load_old_set\"][\"4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BP[\"probes\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### using TaskGeneral to load tasks and name them (matching Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tools.utils import getTrialsTaskClassGeneral, getTrialsTaskClass\n",
    "\n",
    "for ind in getIndsTrials(fd):\n",
    "    # getTrialsTaskClass(fd, ind)\n",
    "    name1 = getTrialsUniqueTasknameGood(fd, ind)\n",
    "    Task = getTrialsTaskClassGeneral(fd, ind)\n",
    "    name2 = Task.Params[\"input_params\"].info_generate_unique_name()\n",
    "    name3 = Task.get_unique_name()\n",
    "    print(ind, name1, '|', name2, '|', name3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Task.Params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### extraction of tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getTrialsTask(fd, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_keys(['onsets', 'guide_dot_coords', 'guide_dot_pos', 'metadat', 'area', 'str', 'stage', 'stagenum', 'alwaysnogap', 'ignore_edge_requirement', 'TaskNew', 'savedTaskSet', 'transforms', 'probe', 'feedback_ver', 'feedback_ver_prms', 'constraints_to_skip', 'ParamsMod', 'reward_multiplier', 'num_pres_even_if_no_engage', 'num_presentations', 'num_successes', 'num_fixations', 'ignore', 'strokes', 'sketchpad', 'fixpos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in getTrialsOutcomesWrapper(fd, 9).items():\n",
    "    print(k, '--',v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Print trial outcomr for each trial\n",
    "for i in getIndsTrials(fd):\n",
    "    print(i, \":\", getTrialsOutcomesWrapper(fd, i)[\"trial_end_method\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fix bug - extraction of motor timing stats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plotTrialSingleOverview(fd, 2\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in getIndsTrials(fd):\n",
    "    print(i, getTrialsOutcomesWrapper(fd, i)[\"trial_end_method\"])\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plotTrialSingleOverview(fd, 434);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "getTrialsTouched(fd, 434)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getTrialsTimesOfMotorEvents(fd, 434)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getTrialsOutcomesWrapper(fd, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getTrialsIsAbort(fd, 145)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotTrialSingleOverview(fd, 145)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getTrialsOutcomesWrapper(fd, 145)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getTrialsOutcomesWrapper(fd, 434)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getTrialsBehCodes(fd, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params_task = getTrialsBlockParamsHotkeyUpdated(fd, 100)[\"params_task\"][\"donebutton_criterion\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in getIndsTrials(fd):\n",
    "    if getTrialsOutcomesWrapper(fd, i)[\"trial_end_method\"]==\"postscene_hotkey_abort\":\n",
    "        print(i, getTrialsOutcomesWrapper(fd, i)[\"online_abort\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in getIndsTrials(fd):\n",
    "    if getTrialsOutcomesWrapper(fd, i)[\"online_abort\"][\"failure_mode\"]==\"hotkey_abort\":\n",
    "        print(i, getTrialsBehCodes(fd, i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in getIndsTrials(fd):\n",
    "    print(i, getTrialsOutcomesWrapper(fd, i)[\"trial_end_method\"])\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getTrialsBehEvaluation(fd, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oc = getTrialsOutcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd[\"TrialRecord\"][\"User\"][\"AdapterParams\"][f\"{100}\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getTrialsAdapterParams(fd, 100, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonlib.tools.datetools import getDateList\n",
    "a = getDateList(sdate=210320)\n",
    "\n",
    "a[0] < 210820\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"---\"\n",
    "[aa for aa in a if aa==\"-\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getTrialsAdapterParams(fd, i)[\"FailureMode\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# featurelist = getMultTrialsBehEvalFeatures(P.fd(0)) # takes first fd, but across all trials, to extract entire set of features.\n",
    "# featurelist.extend([\"score_final\", \"binary_evaluation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.plot_featuredists_byblock(\"/tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SUPERVISION PARAMS\n",
    "# i = 200\n",
    "# fd, t = P.fd_trial(i)\n",
    "# getTrialsBlockParams(fd, t)[\"sequence\"]\n",
    "\n",
    "\n",
    "# guide = getTrialsAdapterParams(fd, t, \"guide\")\n",
    "\n",
    "# guide[\"dynamic\"].keys()\n",
    "# guide[\"dynamic\"][\"version\"]\n",
    "\n",
    "\n",
    "# bb = getTrialsAdapterParams(fd, t)\n",
    "# bb[\"InkColorsByPt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.asarray(1.)\n",
    "\n",
    "isinstance(a, list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=200\n",
    "fd, t = P.fd_trial(i)\n",
    "getTrialsBlockParams(fd, t)[\"sequence\"]\n",
    "bb = getTrialsAdapterParams(fd, t)\n",
    "\n",
    "bb[\"abortmodes\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  SCRATCH, developing extraction of supervision params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.filterProbedat({\"session\":[2]}, True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place supervision features into pandas, just for easy plotting below\n",
    "\n",
    "Pp = P.pandas()\n",
    "\n",
    "tmp = P.getTrialsHelper(\"getTrialsSupervisionParams\", \"all\")\n",
    "\n",
    "sver = [t[\"sequence_ver\"] for t in tmp]\n",
    "Pp[\"sequence_ver\"] = sver\n",
    "\n",
    "Pp[\"col_draw\"] = [t[\"draw_colored_strokes\"] for t in tmp]\n",
    "Pp[\"col_guide\"] = [t[\"guide_colored_strokes\"] for t in tmp]\n",
    "Pp[\"guide_dyn\"] = [t[\"guide_dynamic_strokes\"] for t in tmp]\n",
    "Pp[\"keep_ink_on\"] = [t[\"keep_ink_on\"] for t in tmp]\n",
    "Pp[\"play_hit_sound\"] = [t[\"play_hit_sound\"] for t in tmp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Examine these plots, along with looking at (video), google \"behavior\" spreadsheet, and blockparams in matlab\n",
    "# To make sure that the extract supervisions are as expected.\n",
    "\n",
    "sns.catplot(data=Pp, x=\"block\", y=\"keep_ink_on\", hue=\"play_hit_sound\", aspect=3)\n",
    "sns.catplot(data=Pp, x=\"block\", y=\"sequence_ver\", hue=\"guide_dyn\", aspect=3)\n",
    "sns.catplot(data=Pp, x=\"block\", y=\"col_draw\", hue=\"col_guide\", aspect=3)\n",
    "# sns.catplot(data=Pp, x=\"block\", y=\"guide_dyn\", aspect=3)\n",
    "\n",
    "# --- useful to see if any mistakes (same block, diff params over sets?)\n",
    "# sns.catplot(data=Pp, x=\"block\", y=\"sequence_ver\", hue=\"saved_setnum\", aspect=3)\n",
    "# sns.catplot(data=Pp, x=\"block\", y=\"col_draw\", hue=\"saved_setnum\", aspect=3)\n",
    "# sns.catplot(data=Pp, x=\"block\", y=\"col_guide\", hue=\"saved_setnum\", aspect=3)\n",
    "# sns.catplot(data=Pp, x=\"block\", y=\"guide_dyn\", hue=\"saved_setnum\", aspect=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "P.clean()\n",
    "P.plotOverviewBlokks(\"/tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try cleaning up a dir\n",
    "from tools.preprocess import cleanupDir\n",
    "cleanupDir(\"Diego\", \"210609\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PD = P.filterProbedat({\"session\":[1]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.plotOverviewBlokks(\"/tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from analysis.probedatTaskmodel import plotAllTrialsByErrorCode\n",
    "\n",
    "sdir = \"/tmp\"\n",
    "plotAllTrialsByErrorCode(P, sdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = extractSessionDf(fd)\n",
    "\n",
    "featurestoplot = getMultTrialsBehEvalFeatures(fd)\n",
    "#     featurestoplot = \n",
    "#     for key, val in getTrialsBlockParams(fd, 1)[\"behEval\"][\"beh_eval\"].items():\n",
    "#         if val[\"weight\"][0][0]>0:\n",
    "#             featurestoplot.append(val[\"feature\"])\n",
    "# #         if val[\"feature\"] ==\"hausdorff\" and val[\"weight\"][0][0]>0:\n",
    "# #             featurestoplot.append(\"hausdorff\")\n",
    "# #         if val[\"feature\"] ==\"frac_touched\" and val[\"weight\"][0][0]>0:\n",
    "# #             featurestoplot.append(\"frac_touched\")\n",
    "featurestoplot.append(\"score_offline\")\n",
    "\n",
    "fig1, fig2, fig3, fig4 = plotOverview_(df, featurestoplot=featurestoplot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotMultTrialsSimple(fd, [200, 201], strokes_ver=\"peanuts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.plo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pp.reset_index("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for g in Pp.groupby([\"task_stagecategory\", \"hausdorff_binned\"]):\n",
    "    print(g[0])\n",
    "#     asdfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ppsorted = Ppsorted.set_index(\"block\", drop=True)\n",
    "Ppsorted.index.names = ['index']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ppsorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Pp = P.pandas()\n",
    "Ppsorted = Pp.sort_values(\"hausdorff\")\n",
    "\n",
    "from pythonlib.tools.pandastools import *\n",
    "\n",
    "\n",
    "# fdict = {\"task_stagecategory\":[\"mixture1\"]}\n",
    "# out = P.filterPandas(fdict, return_indices=False)\n",
    "\n",
    "def F(x):\n",
    "    return len(x)\n",
    "out, dfgroup = aggregThenReassignToNewColumn(Ppsorted, F, [\"task_stagecategory\"], \"test\", return_grouped_df=True)\n",
    "out = out[out[\"task_stagecategory\"]==\"mixture2\"]\n",
    "set(out[\"test\"])\n",
    "# # check that indices are not changed\n",
    "# print(np.all(out[\"trialcode\"].values==P.pandas()[\"trialcode\"].values))\n",
    "# print(np.all(out.index == P.pandas().index))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# binColumn(Pp, \"hausdorff\", 10)\n",
    "\n",
    "\n",
    "# getCount(Pp, [\"task_stagecategory\"], \"hausdorff\")\n",
    "\n",
    "# def F(x):\n",
    "#     return x[\"trial\"]\n",
    "# # applyFunctionToAllRows(Pp, F, newcolname=\"test\")\n",
    "# Ppsorted = Pp.sort_values(\"hausdorff\").reset_index()\n",
    "# applyFunctionToAllRows(Ppsorted, F, newcolname=\"test\")\n",
    "\n",
    "\n",
    "# aggregGeneral(Pp, group=[\"task_stagecategory\", \"hausdorff_binned\"], \n",
    "#               values=[\"hausdorff_binned\", \"rew_total\"], aggmethod=[\"mean\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pp.pivot_table(index=[\"trial\"], values=[\"hausdorff\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ppdown = Pp.groupby([\"task_stagecategory\"]).mean().reset_index(drop=True)\n",
    "Ppdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ppsorted.reset_index().merge(Ppdown, how=\"left\", on=\"task_stagecategory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a.reset_index().merge(b, how=\"left\").set_index('index')\n",
    "Ppsorted.reset_index().merge(Ppdown, how=\"left\", on=\"task_stagecategory\").set_index(\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pp.apply("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F(x):\n",
    "    return x[\"trial\"]\n",
    "# applyFunctionToAllRows(Pp, F, newcolname=\"test\")\n",
    "Ppsorted = Pp.sort_values(\"hausdorff\")\n",
    "applyFunctionToAllRows(Ppsorted, F, newcolname=\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.pandas()\n",
    "tmp = pd.merge(P.pandas(), dfgroup, how=\"left\", on=[\"task_stagecategory\"], indicator=True)\n",
    "# tmp = P.pandas().merge(dfgroup, how=\"left\", )\n",
    "tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out[\"trialcode\"].values[-15:])\n",
    "\n",
    "print(P.pandas()[\"trialcode\"].values[-15:])\n",
    "\n",
    "print(tmp[\"trialcode\"].values[-15:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reducing size of filedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reducing size of fd, only keeping things needed\n",
    "from pythonlib.tools.pytools import get_size\n",
    "fd = P.Probedat[0][\"filedata\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for k, v in fd[\"TrialRecord\"][\"User\"].items():\n",
    "    print(\"--\")\n",
    "    print(k)\n",
    "    print(get_size(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd[\"TrialRecord\"][\"TaskInfo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd[\"trials\"][1][\"AnalogData\"][\"Button\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This shows that almost all of memory of Probedat is for filedata\n",
    "\n",
    "print(\"size of entire probedat\")\n",
    "print(get_size(P)/1000)\n",
    "\n",
    "print(\"size of filedata for just the first trial\")\n",
    "print(get_size(P.Probedat[0][\"filedata\"])/1000)\n",
    "\n",
    "print(\"size of filedata for just the last trial\")\n",
    "print(get_size(P.Probedat[-1][\"filedata\"])/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This shows that almost all of memory of Probedat is for filedata\n",
    "\n",
    "print(\"size of entire probedat\")\n",
    "print(get_size(P)/1000)\n",
    "\n",
    "print(\"size of filedata for just the first trial\")\n",
    "print(get_size(P.Probedat[0][\"filedata\"])/1000)\n",
    "\n",
    "print(\"size of filedata for just the last trial\")\n",
    "print(get_size(P.Probedat[-1][\"filedata\"])/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"in filedata, size is mostly in trialrecord and trials\")\n",
    "for k in fd.keys():\n",
    "    print(k)\n",
    "    print(get_size(fd[k])/1000)\n",
    "# print(get_size(fd[\"trials\"])/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trhere is only one filedata that all trials reference to.\n",
    "# Confirmed taht this holds even when there are multiple days.\n",
    "[id(P.Probedat[t][\"filedata\"]) for t in range(len(P.Probedat))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ==== CODE TO DO REDUCING\n",
    "from tools.preprocess import cleanupFiledata\n",
    "\n",
    "print(fd[\"TrialRecord\"].keys())\n",
    "fd = P.Probedat[0][\"filedata\"]\n",
    "cleanupFiledata(fd, True)\n",
    "print(fd[\"TrialRecord\"].keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  DEVELOPING BETTER SINGLE DAY SUMMARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SDIR = \"/tmp/blokkfigs\"\n",
    "P.plotOverviewBlokks(SDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SDIR = \"/data2/animals/Pancho/210311/figures/probeanalysis/tmp\"\n",
    "os.makedirs(SDIR, exist_ok=True)\n",
    "P.plotOverviewBlokks(SDIR);\n",
    "P.plotOverviewTaskPerformance(SDIR);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### compare a single task across days - check whether positions match exactlty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"Diego\"\n",
    "d = 210624\n",
    "e = \"linecircle4\"\n",
    "s = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = loadSingleDataQuick(a, d, e, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for a given set of tasks\n",
    "\n",
    "FD = loadMultData([[e, a, d]]);\n",
    "\n",
    "PD = PROBEDATfromFD(FD)\n",
    "\n",
    "from analysis.probedatTaskmodel import *\n",
    "P = ProbedatTaskmodel(PD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter\n",
    "if False:\n",
    "    Pp = P.filterPandas({\"session\": [7]})\n",
    "else:\n",
    "    Pp = P.pandas()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = \"posterior\"\n",
    "sns.catplot(data = Pp, x=\"block\", y=f, aspect=2.5, kind=\"boxen\")\n",
    "sns.catplot(data = Pp, x=\"block\", y=f, aspect=2.5, kind=\"point\")\n",
    "sns.catplot(data = Pp, x=\"block\", y=f, aspect=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(set(P.pandas()[\"unique_task_name\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = {\n",
    "    \"saved_setnum\":[25],\n",
    "}\n",
    "f = {\n",
    "    \"unique_task_name\":[\"mixture2_1-savedset-25\"],\n",
    "}\n",
    "\n",
    "P.filterProbedat(f)\n",
    "\n",
    "P.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plotMultTrialsSimple(P.fd(0), P.getTrialNums(), clean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_type = \"frac_touched\"\n",
    "FIGS = plotBehSortedByScore(df, fd, score_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = extractSessionDf(fd)\n",
    "featurestoplot = getMultTrialsBehEvalFeatures(fd)\n",
    "featurestoplot.append(\"score_offline\")\n",
    "featurestoplot.extend([\"behscore\", \"reward\"])\n",
    "for score_type in featurestoplot:\n",
    "    FIGS = plotBehSortedByScore(df, fd, score_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLOT TRIALS, LOOK AT WHY FAILED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = P.Probedat[0][\"filedata\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 806\n",
    "P = getTrialsAdapterParams(fd, t)\n",
    "\n",
    "getTrialsOutcomesWrapper(fd, t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P[\"assignstrokes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P[\"DonenessTracker\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBEDAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dattoget = (e, a, d)\n",
    "FD = loadMultData([dattoget])\n",
    "\n",
    "\n",
    "PROBEDAT = PROBEDATfromFD(FD)\n",
    "\n",
    "\n",
    "from analysis.probedatTaskmodel import *\n",
    "\n",
    "P  = ProbedatTaskmodel(PROBEDAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks_to_plot = list(set(P.pandas()[\"block\"]))\n",
    "blocks_to_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# = PAIRWISE SCATTER COMAPRING ALL BEH FEATURES\n",
    "# (MOVED TO PREPROCCESS).\n",
    "featurelist = getMultTrialsBehEvalFeatures(P.fd(1))\n",
    "featurelist.extend([\"score_final\", \"binary_evaluation\"])\n",
    "\n",
    "blocks_to_plot = [10]\n",
    "for bk in blocks_to_plot:\n",
    "    filtdict = {\"block\":[bk]}\n",
    "    PD = P.filterProbedat(filtdict, modify_in_place = False);\n",
    "    Pfilt = ProbedatTaskmodel(PD)\n",
    "    fig = sns.pairplot(Pfilt.pandas(), vars=featurelist, hue=\"kind\")\n",
    "    fig.savefig(\"/tmp/test.pdf\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(P.pandas(), vars=featurelist, hue=\"kind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print task names\n",
    "for t in getIndsTrials(fd):\n",
    "    print(getTrialsTask(fd, t)[\"str\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tstr = \"3linePlusL_67\"\n",
    "for t in getIndsTrials(fd):\n",
    "    task = getTrialsTask(fd, t)\n",
    "    if task[\"str\"]==tstr:\n",
    "        strokes1 = getTrialsTaskAsStrokes(fd, t)\n",
    "\n",
    "tstr = \"3linePlusL_67\"\n",
    "for t in getIndsTrials(fd2):\n",
    "    task = getTrialsTask(fd2, t)\n",
    "    if task[\"str\"]==tstr:\n",
    "        strokes2 = getTrialsTaskAsStrokes(fd2, t)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot both overlaid\n",
    "for s1, s2 in zip(strokes1, strokes2):\n",
    "    assert np.all(np.isclose(s1, s2))\n",
    "print(\"strokes match!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fd[\"TrialRecord\"][\"User\"][\"TrialData\"][f\"{1}\"][\"CurrentTask\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIXING extracting of blockparams - after splitting BlockParams to RunParams and BP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load filedata, quick\n",
    "a = \"Pancho\"\n",
    "# d = 220218\n",
    "# e = \"primoverlap3c\"\n",
    "d = 220224\n",
    "e = \"chunkbyshape1\"\n",
    "s = 1\n",
    "\n",
    "fd = loadSingleDataQuick(a, d, e, s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = loadSingleData(a, d, e, s, False, False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getTrialsBlockParamsDefault(fd, 100, \"TaskParams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd[\"params\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
