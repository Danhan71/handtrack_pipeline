{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n\\nSINGLE notebook for merging all things related to clustering strokes, and stroke similarity metrics.\\n\\n# NOTES: combines code from:\\n\\n# ALSO SEE:\\n# devo_strokemanifold_analysis_040321, has some very old plots, and old notes.\\n\\n'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "SINGLE notebook for merging all things related to clustering strokes, and stroke similarity metrics.\n",
    "\n",
    "# NOTES: combines code from:\n",
    "\n",
    "# ALSO SEE:\n",
    "# devo_strokemanifold_analysis_040321, has some very old plots, and old notes.\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T22:19:47.462702142Z",
     "start_time": "2024-02-26T22:19:47.379184928Z"
    }
   },
   "id": "outer-structure"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from pythonlib.tools.plottools import savefig\n",
    "from pythonlib.dataset.dataset_analy.primitives import *\n",
    "from pythonlib.dataset.dataset_preprocess.primitives import *\n",
    "from pythonlib.dataset.dataset import Dataset, load_dataset_daily_helper\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T22:19:47.714532936Z",
     "start_time": "2024-02-26T22:19:47.620179248Z"
    }
   },
   "id": "aging-movement"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching using this string:\n",
      "/home/lucast4/code/drawmonkey/expt_metadat/*221220-*Pancho.**\n",
      "Found this many paths:\n",
      "0\n",
      "Searching using this string:\n",
      "/home/lucast4/code/drawmonkey/expt_metadat_daily/*221220-*Pancho.**\n",
      "Found this many paths:\n",
      "1\n",
      "---\n",
      "/home/lucast4/code/drawmonkey/expt_metadat_daily/priminvar3j-221220-Pancho.yaml\n",
      "Loading this dataset Pancho priminvar3j 221220\n",
      "Searching using this string:\n",
      "/gorilla1/analyses/database/*Pancho-*priminvar3j-*221220-*/*dat*.pkl\n",
      "-- Splitting off dir from fname\n",
      "Found this many paths:\n",
      "0\n",
      "Searching using this string:\n",
      "/gorilla1/analyses/database/BEH/*Pancho-*priminvar3j-*221220-*/*dat*.pkl\n",
      "-- Splitting off dir from fname\n",
      "Found this many paths:\n",
      "1\n",
      "---\n",
      "/gorilla1/analyses/database/BEH/Pancho-priminvar3j-221220-221230_151621\n",
      "Searching using this string:\n",
      "/mnt/Freiwald_kgupta/kgupta/analyses/database/*Pancho-*priminvar3j-*221220-*/*dat*.pkl\n",
      "-- Splitting off dir from fname\n",
      "Found this many paths:\n",
      "0\n",
      "Searching using this string:\n",
      "/mnt/Freiwald_kgupta/kgupta/analyses/database/BEH/*Pancho-*priminvar3j-*221220-*/*dat*.pkl\n",
      "-- Splitting off dir from fname\n",
      "Found this many paths:\n",
      "1\n",
      "---\n",
      "/mnt/Freiwald_kgupta/kgupta/analyses/database/BEH/Pancho-priminvar3j-221220-221230_151621\n",
      "----------------\n",
      "Currently loading dataset pkl: /mnt/Freiwald_kgupta/kgupta/analyses/database/BEH/Pancho-priminvar3j-221220-221230_151621\n",
      ".. Done!\n",
      "Loaded metadat:\n",
      "{'sketchpad_edges': array([[-311.84, -224.8 ],\n",
      "       [ 311.84,  429.6 ]]), 'metadat_probedat': {'sdate': '221220', 'edate': '221220', 'strokmodel_kind': None, 'strokmodel_tstamp': None, 'datecategories': {'221220': 1}, 'dates_for_summary': [], 'matchedstrokes': None, 'exptnames': ['priminvar3j', 'priminvar3k'], 'T1': [], 'G1': [], 'G2': [], 'G3': [], 'G4': [], 'description': '', 'finalized': False, 'good_expt': True, 'expt': 'priminvar3j', 'animal': 'Pancho', 'ssess': None, 'esess': None, 'task_train_test': {'probe1_liketrain': 'train', 'probe1_nostrokeconstraint': 'train', 'probe2_liketrain': 'train', 'probe2_nostrokeconstraint': 'train', 'probe3_hdpos': 'test', 'probe1': 'train', 'probe2': 'train', 'probe3': 'test', 'probe4': 'test', 'train': 'train'}}, 'filedata_params': {'pix_per_deg': array([[ 26.64621164],\n",
      "       [-26.64621164]]), 'resolution': (1024, 768), 'animal': 'Pancho', 'basedir': '/gorilla1/animals', 'sample_rate': array([500.]), 'beh_codes': {9: 'start', 10: 'fix cue', 11: 'fix cue visible', 13: 'frame skip', 14: 'manual rew', 15: 'guide', 16: 'FixationOnsetWTH', 17: 'FixationDoneSuccessWTH', 18: 'end', 19: 'FixationRaiseFailWTH', 20: 'go (draw)', 21: 'guide_on_GA', 30: 'DelayWhatIsThis', 40: 'GoWhatIsThis', 41: 'samp1 on', 42: 'samp1 off', 45: 'done', 46: 'post', 50: 'reward', 51: 'free reward', 61: 'DoneButtonVisible', 62: 'DoneButtonTouched', 63: 'DragAroundSuccess', 64: 'DragAroundAbort', 65: 'DragAroundFirstAbortNow', 70: 'hotkey_x', 71: 'DAstimevent_firstpres', 72: 'DAstimoff_finibeforepause', 73: 'DAstimoff_fini', 74: 'DAsamp1_visible_change', 75: 'DAnewpnutthisframe', 76: 'DAsound_samp1touched', 78: 'DAsound_gotallink', 80: 'ttl_trialon', 81: 'ttl_trialoff', 91: 'GAstimevent_firstpres', 92: 'GAstimoff_fini', 101: 'fix_square_on', 102: 'fix_square_off', 103: 'fix_square_on_pd', 111: 'photodiode_force_off', 120: 'DAsound_chunk', 121: 'DAsound_strokedone', 122: 'DAsound_chunkupdate', 123: 'DAsound_chunkdone', 124: 'DAsound_firstraise', 131: 'fix_cue_colored_on', 132: 'fix_cue_colored_on_v2', 133: 'fix_cue_colored_off', 134: 'fix_cue_colored_off_v2', 135: 'new_color_cue_off', 200: 'skipped_movie_frame'}, 'screen_hz': 59, 'screen_period': 0.01694915254237288}}\n",
      "Loading BlockParamsByDateSessBlock!\n",
      "----\n",
      "Resetting index\n",
      "=== CLEANING UP self.Dat ===== \n",
      "Deleted unused columns from self.Dat\n",
      "applying monkey train test names\n",
      "* UDPATEING onset of first stroke [too close to fixation] (trial, new onset index):\n",
      "-- CHECKING  origin\n",
      "--- idat, trialcode, strok inds to remove, len strokes beofre remofe, len strokes after:\n",
      "-- CHECKING  donepos\n",
      "--- idat, trialcode, strok inds to remove, len strokes beofre remofe, len strokes after:\n",
      "Updated columns: insummarydates, using Metadats\n",
      "Searching using this string:\n",
      "/gorilla1/analyses/database/TASKS_GENERAL/Pancho-priminvar3j-221220-all/*Tasks*pkl\n",
      "Found this many paths:\n",
      "1\n",
      "---\n",
      "/gorilla1/analyses/database/TASKS_GENERAL/Pancho-priminvar3j-221220-all/Tasks.pkl\n",
      "--- Loading tasks pkl file:  /gorilla1/analyses/database/TASKS_GENERAL/Pancho-priminvar3j-221220-all/Tasks.pkl\n",
      "added new column self.Dat[Task]\n",
      "=== CLEANING UP self.Dat (_cleanup_reloading_saved_state) ===== \n",
      "0 _behclass_alignsim_compute\n",
      "200 _behclass_alignsim_compute\n",
      "400 _behclass_alignsim_compute\n",
      "600 _behclass_alignsim_compute\n",
      "800 _behclass_alignsim_compute\n",
      "1000 _behclass_alignsim_compute\n",
      "Running D._behclass_tokens_extract_datsegs\n",
      "0 _behclass_tokens_extract_datsegs\n",
      "200 _behclass_tokens_extract_datsegs\n",
      "400 _behclass_tokens_extract_datsegs\n",
      "600 _behclass_tokens_extract_datsegs\n",
      "800 _behclass_tokens_extract_datsegs\n",
      "1000 _behclass_tokens_extract_datsegs\n",
      "stored in self.Dat[BehClass]\n",
      "Removing these trials: \n",
      "[]\n",
      "self.Dat starting legnth:  1160\n",
      "Modified self.Dat, keeping only the inputted inds\n",
      "self.Dat final legnth:  1160\n",
      "- starting/ending len (grouping params):\n",
      "1160\n",
      "1160\n",
      "- starting/ending len (getting sequence):\n",
      "1160\n",
      "1160\n",
      "--- Removing nans\n",
      "start len: 1160\n",
      "- num names for each col\n",
      "not removing nans, since columns=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function flush_figures at 0x7eff7b3318b0> (for post_execute):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "animal = \"Pancho\"\n",
    "date = \"221220\"\n",
    "\n",
    "# Diego, char with clust labels\n",
    "# animal = \"Diego\"\n",
    "# date = \"231204\"\n",
    "\n",
    "rename_shapes_if_cluster_labels_exist = True\n",
    "D = load_dataset_daily_helper(animal, date, rename_shapes_if_cluster_labels_exist=rename_shapes_if_cluster_labels_exist)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T22:20:43.311417361Z",
     "start_time": "2024-02-26T22:19:47.829836675Z"
    }
   },
   "id": "6a0bebd5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Entire pipeline from character_cluster.py is below"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f23415aa23679e72"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# WHICH_LEVEL = \"shapemean\"\n",
    "WHICH_LEVEL = \"trial\"\n",
    "WHICH_BASIS_SET = \"Pancho\"\n",
    "# WHICH_BASIS_SET = \"Diego\"\n",
    "WHICH_TASK_KIND = \"character\"\n",
    "# WHICH_TASK_KIND = \"prims_on_grid\"\n",
    "SUBSAMPLE = False"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "affd54a7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Keep only characters\n",
    "D.Dat = D.Dat[D.Dat[\"task_kind\"] == WHICH_TASK_KIND].reset_index(drop=True)\n",
    "\n",
    "if SUBSAMPLE:\n",
    "    # OPTIONAL\n",
    "    D.subsampleTrials(1, 1)\n",
    "from pythonlib.dataset.dataset_strokes import preprocess_dataset_to_datstrokes\n",
    "from pythonlib.dataset.dataset_strokes import DatStrokes\n",
    "DS = preprocess_dataset_to_datstrokes(D, \"clean_chars\")\n",
    "SDIR = D.make_savedir_for_analysis_figures_BETTER(f\"strokes_clustering_similarity/{WHICH_LEVEL}-basis_{WHICH_BASIS_SET}\")\n",
    "# SDIR = f\"{SDIR}/{WHICH_LEVEL}\"\n",
    "# print(SDIR)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16e262e63c7ffaa4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "WHICH_FEATURE = \"beh_motor_sim\" \n",
    "\n",
    "##### Perform clustering\n",
    "ClustDict, ParamsDict, ParamsGeneral, dfdat = DS.features_wrapper_generate_all_features(WHICH_LEVEL, \n",
    "                                                                                        which_basis_set=WHICH_BASIS_SET)\n",
    "plt.close(\"all\")\n",
    "\n",
    "# For each trial, extracting clustering score, etc.\n",
    "DS.clustergood_assign_data_to_cluster(ClustDict, ParamsDict, \n",
    "            ParamsGeneral, dfdat,\n",
    "            which_features = WHICH_FEATURE,\n",
    "            trial_summary_score_ver=\"clust_sim_max\")\n",
    "plt.close(\"all\")\n",
    "\n",
    "##################### PLOTS\n",
    "Cl = ClustDict[WHICH_FEATURE]\n",
    "list_shape_basis = ParamsDict[WHICH_FEATURE][\"list_shape_basis\"]\n",
    "list_strok_basis = ParamsDict[WHICH_FEATURE][\"list_strok_basis\"]\n",
    "\n",
    "#### QUICK SMALL PLOTS\n",
    "# yvar = \"clust_sim_max\"\n",
    "savedir = f\"{SDIR}/sim_score_histograms\"\n",
    "os.makedirs(savedir, exist_ok=True)        \n",
    "list_yvar = [\"sims_max\", \"sims_concentration\", \"sims_concentration_v2\", \"sims_entropy\"]\n",
    "ncols = 2\n",
    "nrows = int(np.ceil(len(list_yvar)/ncols))\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*5, nrows*3))\n",
    "for yvar, ax in zip(list_yvar, axes.flatten()):\n",
    "    vals = Cl.cluster_extract_data(\"max_sim\")[yvar]\n",
    "    if yvar==\"sims_entropy\":\n",
    "        ax.hist(vals, bins=50);\n",
    "    else:\n",
    "        ax.hist(vals, bins=np.linspace(0., max(vals), num=50));\n",
    "#     ax.hist(DS.Dat[yvar], bins=np.linspace(0., 1., num=50));\n",
    "    ax.set_xlabel(yvar)\n",
    "path = f\"{savedir}/hist-{yvar}.pdf\"\n",
    "savefig(fig, path)\n",
    "plt.close(\"all\")\n",
    "    \n",
    "### [Good] main plots\n",
    "##### Plots of heatmaps, raw results, for feature spaces and clustering \n",
    "DS.clustergood_plot_raw_results(ClustDict, ParamsDict, ParamsGeneral, dfdat, SDIR)\n",
    "plt.close(\"all\")\n",
    "\n",
    "##### Figures previously in characters\n",
    "# Plot results for characters\n",
    "from pythonlib.dataset.dataset_analy.characters import plot_clustering, plot_learning_and_characters, plot_prim_sequences\n",
    "plot_clustering(DS, list_strok_basis, list_shape_basis, SDIR)\n",
    "plt.close(\"all\")\n",
    "\n",
    "# Get indices spanning range of vals\n",
    "savedir = f\"{SDIR}/example_trials/{WHICH_FEATURE}\"\n",
    "os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "valname = \"clust_sim_max\"\n",
    "vals = DS.Dat[valname].tolist()\n",
    "nplot = 40\n",
    "from pythonlib.tools.listtools import random_inds_uniformly_distributed\n",
    "indsplot = random_inds_uniformly_distributed(vals, nplot, return_original_values=False)\n",
    "for ind in indsplot:\n",
    "    val = vals[ind]\n",
    "    prefix = f\"{valname}-{val:.2f}\"\n",
    "    DS.clustergood_plot_single_dat(ind, savedir=savedir, prefix=prefix)\n",
    "    plt.close(\"all\")    \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab7a7a38545eb3bc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "#### DIM REDUCTIONS\n",
    "savedir = f\"{SDIR}/dim_reduction\"\n",
    "os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "gmm_n_mixtures = range(4, 20)\n",
    "things_to_do = (\"tsne\", \"gmm\", \"gmm_using_tsne\")\n",
    "perplist = [15]\n",
    "gmm_tsne_perp_to_use = 15\n",
    "Cl.cluster_compute_all(gmm_n_mixtures=gmm_n_mixtures, perplist=perplist, \n",
    "                       things_to_do=things_to_do, \n",
    "                       gmm_tsne_perp_to_use=gmm_tsne_perp_to_use)\n",
    "\n",
    "# Note: these corrs means that should (i) get better features and (ii) do PCA first before clsutering. \n",
    "simmat = Cl.Xinput\n",
    "simmat_rownorm = simmat/np.sum(simmat, axis=1, keepdims=True)\n",
    "cc = np.corrcoef(simmat_rownorm.T)\n",
    "fig, _, _, _, _ = Cl._plot_heatmap_data(cc, labels_row=Cl.LabelsCols, labels_col=Cl.LabelsCols);\n",
    "savefig(fig, f\"{savedir}/xcorr_of_simmat.pdf\")\n",
    "\n",
    "##### PCA of sim mat\n",
    "\n",
    "# Plot basis set\n",
    "if False:\n",
    "    # THis is plotted elsewhere\n",
    "    shapes = Params[\"list_shape_basis\"]\n",
    "    # shapes = RES[\"list_shape_basis\"]\n",
    "    labels_col = Cl.LabelsCols\n",
    "    assert shapes == labels_col\n",
    "    strokes = Params[\"list_strok_basis\"]\n",
    "    fig, axes = DS.plot_multiple_strok(strokes, overlay=False, titles = shapes, ncols = len(shapes))\n",
    "#     fig.savefig(f\")\n",
    "\n",
    "Cl.cluster_pca_plot_all(savedir=savedir)\n",
    "\n",
    "\n",
    "for ver in [\"gmm\", \"gmm_using_tsne\"]:\n",
    "    gmm_n_best, list_n, list_crossval, list_bic = Cl.cluster_gmm_extract_best_n(ver=ver)\n",
    "\n",
    "    fig, axes = plt.subplots(2,2)\n",
    "\n",
    "    ax = axes.flatten()[0]\n",
    "    ax.plot(list_n, list_crossval, \"-ok\")\n",
    "    ax.set_ylabel(\"crossval\")\n",
    "    ax.set_xlabel(\"gmm_n\")\n",
    "\n",
    "    ax = axes.flatten()[1]\n",
    "    ax.plot(list_n, list_bic, \"-ok\")\n",
    "    ax.set_ylabel(\"bic\")\n",
    "    ax.set_xlabel(\"gmm_n\")\n",
    "\n",
    "    savefig(fig, f\"{savedir}/gmm_scores_using-ver_{ver}.pdf\")\n",
    "\n",
    "if False:\n",
    "    # Plot, label by \"alignsim\" shapes (not useful for characters)\n",
    "    list_perp = Cl.cluster_tsne_extract_list_perp()\n",
    "    for perp in list_perp:\n",
    "        Cl.cluster_plot_scatter(\"tsne\", perp=perp, dims=[0, 1])    \n",
    "\n",
    "# Plot in tsne space, using gmm labels\n",
    "gmm_n_best, list_n, list_crossval, list_bic = Cl.cluster_gmm_extract_best_n()\n",
    "for space in [\"tsne\", \"pca\"]:\n",
    "    for label in [\"shape\", None, \"col_max_sim\", \"gmm\", \"gmm_using_tsne\"]:\n",
    "        fig, axes = Cl.cluster_plot_scatter(space, label=label, gmm_n=gmm_n_best, perp=15, dims=[0, 1])\n",
    "        savefig(fig, f\"{savedir}/scatter-space_{space}-label_{label}.pdf\")\n",
    "\n",
    "### SAVE\n",
    "import pickle\n",
    "path = f\"{SDIR}/DS.pkl\"\n",
    "with open(path, \"wb\") as f:\n",
    "    pickle.dump(DS, f)\n",
    "print(\"Saved to: \", path)\n",
    "\n",
    "plt.close(\"all\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "673bfa827706ddd0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Finalizing \"lumping\" into distinct shape categories"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8ecef739370ca46"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# PCA (which basis strokes are lumped together?)\n",
    "Cl.cluster_pca_plot_all(savedir=savedir)\n",
    "\n",
    "  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b653c8bd30667be4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pcamodel = Cl.ClusterComputeAllResults[\"pca_model\"]\n",
    "pcamodel.explained_variance_ # decresaeing\n",
    "pcamodel.components_ # components_ndarray of shape (n_components, n_features), sorted by decreasing explained var."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7707f0200e379986"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "WHICH_FEATURE = \"beh_motor_sim\" \n",
    "Cl = ClustDict[WHICH_FEATURE]\n",
    "list_shape_basis = ParamsDict[WHICH_FEATURE][\"list_shape_basis\"]\n",
    "list_strok_basis = ParamsDict[WHICH_FEATURE][\"list_strok_basis\"]\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "576c201883605f86"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Do PCA after first z-scor\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b10833f780d4536"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Make a PCA reuslts cluster class\n",
    "from pythonlib.cluster.clustclass import Clusters\n",
    "pc_num_decreasing = list(range(pcamodel.components_.shape[0]))\n",
    "feature_shapes = Cl.LabelsCols\n",
    "assert list_shape_basis==feature_shapes\n",
    "ClPCA = Clusters(pcamodel.components_, pc_num_decreasing, feature_shapes, ver=\"pca\", params={\"pcs_explained_var\":pcamodel.explained_variance_})\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f8adfa6bae5ea7f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Xsorted, labelsSorted = ClPCA.sort_by_labels(labels=ClPCA.Params[\"pcs_explained_var\"])\n",
    "# ClPCA.plot_heatmap_data(sort_rows_by=ClPCA.Params[\"pcs_explained_var\"], diverge=True);\n",
    "ClPCA.plot_heatmap_data(diverge=True, ylabel=\"pc num (0=max var)\");\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88e32a26fd558a20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Cl.plot_save_hier_clust()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb47831e6c994e64"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DS.Dat.iloc[182]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "548914459a1f1910"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DS.Dat[\"clust_sim_max\"]\n",
    "DS.plot_multiple_after_slicing_within_range_values(\"clust_sim_max\", 1, 1.2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9f97a30684a58de"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### LUMPING sahpes into shape_label BY HAND"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "454ce1e3d33d74bf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Keep only if score about this\n",
    "threshold_circle = 0.9\n",
    "threshold_all = 1.2\n",
    "\n",
    "a = (DS.Dat[\"clust_sim_max_colname\"]==\"circle-6-1-0\") & (DS.Dat[\"clust_sim_max\"]>threshold_circle)\n",
    "b = (DS.Dat[\"clust_sim_max_colname\"]!=\"circle-6-1-0\") & (DS.Dat[\"clust_sim_max\"]>threshold_all)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c88bee4de385a82"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Prune DS to keep just the good ones\n",
    "DSpruned = DS.copy()\n",
    "DSpruned.Dat = DSpruned.Dat[a | b].reset_index(drop=True)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9aaee48e60f12835"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Lump together (done by hand)\n",
    "map_shapelump_to_shapes = {}\n",
    "map_shape_to_shapelump = {}\n",
    "for grp in [\n",
    "    [\"V-2-4-0\", \"arcdeep-4-4-0\"],\n",
    "    [\"arcdeep-4-1-0\", \"V-2-1-0\"]]:\n",
    "    \n",
    "    assert animal==\"Pancho\", \"tjhis is hard coded for him\"\n",
    "    # name it after the first\n",
    "    name = f\"L|{grp[0]}\"\n",
    "    map_shapelump_to_shapes[name] = grp\n",
    "    \n",
    "    for g in grp:\n",
    "        map_shape_to_shapelump[g] = name\n",
    "        \n",
    "list_shapes_final = []\n",
    "for ind in range(len(DSpruned.Dat)):\n",
    "    shape = DSpruned.Dat.iloc[ind][\"clust_sim_max_colname\"]\n",
    "    if shape in map_shape_to_shapelump.keys():\n",
    "        list_shapes_final.append(map_shape_to_shapelump[shape])\n",
    "    else:        \n",
    "        list_shapes_final.append(shape)\n",
    "DSpruned.Dat[\"shape_label\"] = list_shapes_final"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b39859d00ce52560"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DSpruned.Dat[\"shape_label\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd95c80e862e7d38"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Give it other labels (\"motor\" level)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f86ac653aee81f60"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# initial angle\n",
    "DSpruned.features_compute_velocity_binned()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e01882311ebf3318"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Location\n",
    "DSpruned.Dat[:2]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "446fafb4cfe6364f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Save this DS, so can later load into constructing Snippets [e.g., RSA plots]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59ee221310106b5f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"NOTEBOOK\":\"230623_STROKES_CLUSTERING_SIMILARITY\",\n",
    "    \"WHICH_LEVEL\":WHICH_LEVEL,\n",
    "    \"WHICH_BASIS_SET\":WHICH_BASIS_SET,\n",
    "    \"ParamsGeneral\":ParamsGeneral,\n",
    "    \"ParamsDict\":ParamsDict,\n",
    "    \"WHICH_FEATURE\":WHICH_FEATURE,\n",
    "    \"threshold_circle\":threshold_circle,\n",
    "    \"threshold_all\":threshold_all,\n",
    "    \"map_shapelump_to_shapes\":map_shapelump_to_shapes,\n",
    "    \"map_shape_to_shapelump\":map_shape_to_shapelump}\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d39649bd0207b8f8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DIR = f\"/gorilla1/analyses/recordings/main/EXPORTED_BEH_DATA/DS/{animal}/{date}\"\n",
    "os.makedirs(DIR, exist_ok=True)\n",
    "DSpruned.export_dat(DIR, params)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97cacb369e938e79"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Loading presaved cluster labelsb"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3311b3e0bd5dca40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# IGNORE, Stuff used in development. this is all now done automaticlaly in preprocessing.."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "351c6e4b769efb38"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# [IGNORE] Try cleaning up dataset (buygs?)\n",
    "D._cleanup_preprocess_each_time_load_dataset()\n",
    "D.tokens_generate_replacement_from_raw()\n",
    "\n",
    "# Wrapper to exatract and replace columns in D. \n",
    "D.charclust_shape_labels_extract_presaved_from_DS()\n",
    "\n",
    "# 1) Replace all tokens with extracted shapes.\n",
    "D.tokens_generate_replacement_from_raw()\n",
    "\n",
    "# 2) delete older tokens\n",
    "D.sequence_tokens_clear_behclass_and_taskclass_and_lock()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77688ee81a279089"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### [MAIN STUFF that would break due to labels]\n",
    "\n",
    "# 3. \n",
    "print(D.TokensVersion)\n",
    "\n",
    "# 4) get sequence context stuff\n",
    "D.seqcontext_preprocess()\n",
    "D.seqcontext_plot_examples_and_print_context(10)\n",
    "\n",
    "# Simulating what would do when load Snippets..\n",
    "Dcopy = D.copy()\n",
    "\n",
    "Dcopy.seqcontext_preprocess()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82e52e5e9ca8a40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# [EXTRACT DS]\n",
    "from pythonlib.dataset.dataset_strokes import preprocess_dataset_to_datstrokes\n",
    "DS = preprocess_dataset_to_datstrokes(D, \"all_no_clean\")\n",
    "# DS = preprocess_dataset_to_datstrokes(D, \"clean_chars_clusters_without_reloading\")\n",
    "# DS = preprocess_dataset_to_datstrokes(D, \"clean_chars\")\n",
    "# clean_chars_clusters_without_reloading\n",
    "DS.plotcheck_compare_to_dataset(1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd0dcacf187bdbdb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DS.Dat[\"shape\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "238364ce3710a276"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DS.plotshape_singleshape_egstrokes_overlaid(shape=\"arcdeep-85-3.8-5\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cfb31bc74c431982"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "inds = DS.Dat[DS.Dat[\"shape\"] == \"Lcentered-86-2.4-5\"].index.tolist()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3368471affa08492"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# DS.plot_single_overlay_entire_trial(104, overlay_beh_or_task=\"task\")\n",
    "# DS.plot_single_overlay_entire_trial(119, overlay_beh_or_task=\"task\")\n",
    "DS.plot_multiple_overlay_entire_trial(inds[:10], overlay_beh_or_task=\"task\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "761f93efb7e2ba44"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Rename them by comparing to canonical images\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a3df265423669c4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "19dd454462a010d0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### plot examples of all the prims (shapes, and example trials)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c21b3c4c732b475"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "DS.plotshape_multshapes_egstrokes(ver_behtask=\"beh\");"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e016bd2a7dba2043"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Debugging - loading of saved shape clusters in DS"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44cbe8d63558a6ad"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pythonlib.dataset.dataset_strokes import preprocess_dataset_to_datstrokes\n",
    "DS = preprocess_dataset_to_datstrokes(D, \"all_no_clean\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "392b1ade9c97456"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DS.clustergood_load_saved_cluster_shape_classes()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d3ce1ed43e82b4a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "D.Dat[~(D.Dat[\"task_kind\"]==\"character\")][\"seqc_0_shape\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c7214707bf9c707"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DS.Dat[DS.Dat[\"task_kind\"]==\"character\"][\"shape\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1db0b7e282bbc66"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DS.Dat[\"shape\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e1d8a0ce4658726"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DS.Dat[:10]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db0852f68425ac60"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "path = '/gorilla1/analyses/recordings/main/EXPORTED_BEH_DATA/DS/Diego/231204/DS_data.pkl'\n",
    "with open(path, \"rb\") as f:\n",
    "    df = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b21c5a4f169e43b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[\"shape_label\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e0cfbf7b57f2072"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Test out extraction of Snippets dataset for char trials"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6d82a32cfcc3adc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from neuralmonkey.metadat.analy.anova_params import dataset_apply_params\n",
    "D, DS, params = dataset_apply_params(D, None, \"chartrial\", animal, date) # prune it\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b4a088167a6c95c6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "D.plotSingleTrial(40)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef4eff7ddc61fbe4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "fd87d3d4a79f000"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Tk1 = list(D.TokensTask.values())[0]               \n",
    "Tk2 = list(D.TokensTask.values())[0]\n",
    "Tk1==Tk2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65b2983d9d0fb532"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Save matrix of pairwise diatnces between strokes (e.g., for RSA analysis later)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82d6270a293219cd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##### Should generate the distmat a single time, using batch function.\n",
    "strokes = [S() for S in DS.Dat[\"Stroke\"].tolist()]\n",
    "distance_ver = \"dtw_vels_2d\"\n",
    "similarity_matrix = DS._cluster_compute_sim_matrix_with_good_params(strokes, strokes, distance_ver) \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6fe0643353b60d49"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save trialcodes and stroke indices \n",
    "trialcodes = DS.Dat[\"trialcode\"].tolist()\n",
    "stroke_indices = DS.Dat[\"stroke_index\"].tolist()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8dc3b56cdefa4b74"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save this in a permanent location\n",
    "DIR = f\"/gorilla1/analyses/recordings/main/EXPORTED_BEH_DATA/DS/{animal}/{date}/distance_matrix_all_strokes_pairwise/{distance_ver}\"\n",
    "os.makedirs(DIR, exist_ok=True)\n",
    "path = f\"{DIR}/data.npy\"\n",
    "with open(path, \"wb\") as f:\n",
    "    np.save(f, similarity_matrix)\n",
    "path = f\"{DIR}/trialcodes_in_order.npy\"\n",
    "with open(path, \"wb\") as f:\n",
    "    np.save(f, trialcodes)\n",
    "    \n",
    "path = f\"{DIR}/stroke_indices_in_order.npy\"\n",
    "with open(path, \"wb\") as f:\n",
    "    np.save(f, stroke_indices)\n",
    "    \n",
    "path = f\"{DIR}/params.yaml\"\n",
    "params = {\n",
    "    \"distance_ver\":distance_ver,\n",
    "    \"trialcodes_in_order\":trialcodes, \n",
    "    \"stroke_indices_in_order\":stroke_indices\n",
    "}\n",
    "\n",
    "from pythonlib.tools.expttools import writeDictToYaml\n",
    "writeDictToYaml(params, path)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9596fadcbba000f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: \n",
    "# Plot the cases that have score less than threshold.\n",
    "# - do they require a different basis set?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2463e356dbedc81f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### DEVELOPING DISTANCE METRIC (low-level plots, to plot examples)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f5a84fdc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "##### Good - pick one trial (stroke), and compare it to entire basis set"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8ff14f0eaee72b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "ind = random.sample(range(len(DS.Dat)), 1)[0]\n",
    "print(ind) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61e7469454499f08"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Given a distance metric, quickly use it scored against beh, and plot results,\n",
    "# for a single trial\n",
    "\n",
    "dfbasis, _, _ = DS.stroke_shape_cluster_database_load_helper(\n",
    "    which_basis_set=None,\n",
    "    which_shapes=None)\n",
    "list_strok_basis = dfbasis[\"strok\"].tolist()\n",
    "list_shape_basis = dfbasis[\"shape\"].tolist()\n",
    "\n",
    "# for distancever in [\"euclidian_diffs\", \"euclidian\", \"hausdorff_alignedonset\", \"dtw_vels_1d\", \"dtw_vels_2d\"]:\n",
    "for distancever in [\"euclidian_diffs\", \"dtw_vels_2d\"]:\n",
    "    # distancever = \"dtw_vels_2d\"\n",
    "    \n",
    "    ###############\n",
    "    strokes_data = [DS.Dat.iloc[ind][\"strok\"]]    \n",
    "    cl = DS._cluster_compute_sim_matrix(strokes_data, list_strok_basis, distancever=distancever,\n",
    "                                   return_as_Clusters=True)\n",
    "    scores = cl.Xinput[0,:]\n",
    "    DS._clustergood_plot_single_dat(ind, scores, list_strok_basis, list_shape_basis, YLIM=None)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71c50ce0be150483"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Same, but pick out a specific few basis strokes, and plot deeper DEBUG plots"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e68835931741c2a8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Pick out specific task stroke to compare to:\n",
    "# Given a distance metric, quickly use it scored against beh, and plot results,\n",
    "# for a single trial\n",
    "\n",
    "inds_basis = [2, 13]\n",
    "DEBUG=True\n",
    "distancever = \"dtw_vels_2d\"\n",
    "    \n",
    "###############\n",
    "strokes_data = [DS.Dat.iloc[ind][\"strok\"]]\n",
    "\n",
    "dfbasis, _, _ = DS.stroke_shape_cluster_database_load_helper(\n",
    "    which_basis_set=None,\n",
    "    which_shapes=None)\n",
    "list_strok_basis = dfbasis[\"strok\"].tolist()\n",
    "list_shape_basis = dfbasis[\"shape\"].tolist()\n",
    "\n",
    "list_strok_basis = [list_strok_basis[i] for i in inds_basis]\n",
    "list_shape_basis = [list_shape_basis[i] for i in inds_basis]\n",
    "\n",
    "cl = DS._cluster_compute_sim_matrix(strokes_data, list_strok_basis, distancever=distancever,\n",
    "                               return_as_Clusters=True, DEBUG=DEBUG)\n",
    "\n",
    "scores = cl.Xinput[0,:]\n",
    "DS._clustergood_plot_single_dat(ind, scores, list_strok_basis, list_shape_basis, YLIM=None)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa743203968d7586"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Devo, towards good char - neural analysis"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2980ab720c29bbdb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### MAIN TODO:\n",
    "# Use DTW - optimize it.\n",
    "\n",
    "# goal: Different distance matrices based on different sets of features (hypotheses).\n",
    "# - Motor spaces\n",
    "# - Abstract spaces: First give it a category, then use that category.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b679853ff162f8b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1) For each feature space, assign each trial it's best fitting value.\n",
    "\n",
    "# 2) Prune data to just cases with good fit.\n",
    "ClustDict\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e0f88a47f363233"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO (in _features_wrapper_generate_all_features):\n",
    "# Give each row a tuple label --> allows plotting using sortings by the different labels.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "970d2d7f7fd4cb4d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# For each trial, extracting clustering score, etc.\n",
    "assert False, --> extract lsit of tuples that can assign to each of the Cl objects, i.e, not just that single which_feagures."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e12084b9a4ec771"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "####"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d9c7d33a751d0f1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### OTHER STUFF"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d0ddc227a7f0aaa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# IN PROGRESS...\n",
    "plot_learning_and_characters(D, SDIR)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "184ec843"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ##### Plot histogram of clust sim scores, agg across all data\n",
    "# \n",
    "# savedir = f\"{SDIR}/sim_score_histograms\"\n",
    "# os.makedirs(savedir, exist_ok=True)\n",
    "# # Compare different metrics for concentration...\n",
    "# x1 = DS.Dat[\"clust_sim_concentration_v2\"]\n",
    "# x2 = simmat_entropy\n",
    "# fig,ax = plt.subplots()\n",
    "# ax.plot(x1, x2, \"xk\")\n",
    "# WHICH_FEATURE = \"beh_motor_sim\"\n",
    "# savedir = f\"{SDIR}/example_trials/{WHICH_FEATURE}\"\n",
    "# os.makedirs(savedir, exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70c982b7fdd065b1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### [Prep for Kavli talk] TODO"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "280e4050"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1) Given set of trials, get score relative to different kinds of basis sets.\n",
    "# 2) Visaulize/score how clustered it is --> e.g, what fraction trials are well-assigned, which are not?\n",
    "# 3) Split trials based on image\n",
    "\n",
    "# Then redo using different similarity metrics --> (i) image, (ii) motor (iii) combined (as it is now). \n",
    "# Then compare to null model.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e400692"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Older code that doesnt work, but is obsolete (folded into above already)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8da4c97698a4db"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from pythonlib.dataset.dataset_analy.characters import debug_eyeball_distance_metric_goodness\n",
    "\n",
    "debug_eyeball_distance_metric_goodness(D)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c451e41e941d834"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Older "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "576e511dffc51396"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Plot\n",
    "RES = {\n",
    "    \"Cl\":Cl,\n",
    "    \"DS\":DS\n",
    "}\n",
    "# Cl = RES[\"Cl\"]\n",
    "# DS = RES[\"DS\"]\n",
    "\n",
    "DS.plot_single_overlay_entire_trial(ind, overlay_beh_or_task=\"beh\");\n",
    "DS.plot_single_overlay_entire_trial(ind, overlay_beh_or_task=\"task\");\n",
    "\n",
    "# Plot basis strokes\n",
    "titles = [f\"{x:.4}\" for x in cl.Xinput[0,:]]\n",
    "DS.plot_multiple_strok(list_strok_basis, ncols=10, overlay=False, titles=titles);\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "# plot line plot "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce59c329413d6d6e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DS.plot_single_overlay_entire_trial([ind], overlay_beh_or_task=\"task\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8f400025947dd05"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Given a feature, plot example trials, using the abstracted features\n",
    "which_feature = \"task_shape_cat_abstract\"\n",
    "Cl = DS.Clusters_ClustDict[which_feature]\n",
    "\n",
    "RES = {\n",
    "    \"Cl\":Cl,\n",
    "    \"DS\":DS\n",
    "}\n",
    "# Cl = RES[\"Cl\"]\n",
    "# DS = RES[\"DS\"]\n",
    "\n",
    "##### DEBUGGING\n",
    "from pythonlib.dataset.dataset_analy.characters import plot_example_trials\n",
    "plot_example_trials(RES)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f2f541e7734c82b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##### Todo (clustering)\n",
    "\n",
    "# pull out casees that dont score well against any prim\n",
    "# use average prim (not just single example\n",
    "# include direction variations?\n",
    "# shapes are also define by reflection\n",
    "# improve strok dist using both euclidian and euclidiandiff`b?\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b968764e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MORE PLOTS"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c90e6b1b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### in progress below"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2fd4db0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##### Which are the \"most informative\" tasks?\n",
    "\n",
    "# for each datapt, compute its sim vector for each kind of representation. \n",
    "# Informative means you have diff similiarity vectors.\n",
    "assert False, \"replace D below, or else will delete dataset\"\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "for key, Cl in ClustDict.items():\n",
    "    D = squareform(pdist(Cl.Xinput))\n",
    "    Cl.DistanceMatrices = {}\n",
    "    Cl.DistanceMatrices[\"eucl\"] = D\n",
    "    \n",
    "\n",
    "\n",
    "# For each row, compute the pairwise correlations between row vectors across all pairs of representations.\n",
    "numrows = len(dfdat)\n",
    "list_rep = ClustDict.keys()\n",
    "list_shape = dfdat[\"shape\"].values.tolist()\n",
    "\n",
    "mat_r = []\n",
    "mat_meandist = []\n",
    "list_rep = [\"beh_motor_sim\", \"task_image_sim\"]\n",
    "for i in range(numrows):\n",
    "    \n",
    "    # Raw feature vectors\n",
    "    \n",
    "    \n",
    "    list_vec = []\n",
    "    for rep in list_rep:\n",
    "        Cl = ClustDict[rep]\n",
    "        \n",
    "        assert Cl.Labels == list_shape\n",
    "        # get row vector, ignoring the diagnoanl\n",
    "        vec = np.r_[Cl.DistanceMatrices[\"eucl\"][i, :i], Cl.DistanceMatrices[\"eucl\"][i, i+1:]]\n",
    "        list_vec.append(vec)\n",
    "    \n",
    "    # compute corr for each pair of vec\n",
    "    from scipy.stats import pearsonr\n",
    "    list_r = []\n",
    "    for i in range(len(list_vec)):\n",
    "        for ii in range(len(list_vec)):\n",
    "            if ii>i:\n",
    "                r, _ = pearsonr(list_vec[i], list_vec[ii])\n",
    "                list_r.append(r)\n",
    "    mat_r.append(list_r)\n",
    "    \n",
    "    # mean distance to others\n",
    "    list_mean_dist = []\n",
    "    for vec in list_vec:\n",
    "        mean_dist = np.mean(vec)\n",
    "        list_mean_dist.append(mean_dist)\n",
    "    mat_meandist.append(list_mean_dist)\n",
    "    \n",
    "    # \n",
    "    \n",
    "    \n",
    "mat_r = np.array(mat_r)\n",
    "mat_meandist = np.array(mat_meandist)\n",
    "\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(2,2, figsize=(10, 14))\n",
    "    \n",
    "ax = axes.flatten()[0]\n",
    "# ax.plot(list_shape, np.min(mat_r, axis=1))\n",
    "ax.plot(np.min(mat_r, axis=1), list_shape, \"or\", label=\"min\")\n",
    "ax.plot(np.mean(mat_r, axis=1), list_shape, \"ok\", label=\"mean\")\n",
    "ax.grid()\n",
    "ax.axvline(0)\n",
    "ax.set_title(\"r\")\n",
    "ax.legend()\n",
    "    \n",
    "ax = axes.flatten()[1]\n",
    "# ax.plot(list_shape, np.min(mat_r, axis=1))\n",
    "ax.plot(np.mean(mat_meandist, axis=1), list_shape, \"ok\", label=\"mean\")\n",
    "ax.plot(np.min(mat_meandist, axis=1), list_shape, \"or\", label=\"min\")\n",
    "ax.plot(np.max(mat_meandist, axis=1), list_shape, \"ob\", label=\"max\")\n",
    "ax.grid()\n",
    "# ax.axvline(0)\n",
    "ax.set_title(\"distance\")\n",
    "ax.legend()\n",
    "    \n",
    "ax = axes.flatten()[2]\n",
    "# ax.plot(list_shape, np.min(mat_r, axis=1))\n",
    "for i in range(len(list_rep)):\n",
    "    ax.plot(mat_meandist[:,i], list_shape, \"o\", label=list_rep[i])\n",
    "ax.grid()\n",
    "# ax.axvline(0)\n",
    "ax.set_title(\"distance\")\n",
    "ax.legend()\n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5060958"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##### Find pairs which are (same stim, diff beh) or (same beh, diff stim)\n",
    "\n",
    "# Plot the sample sizes for each shape\n",
    "for shape in list_shape_reordered:\n",
    "    n = sum(DS.Dat[\"shape_oriented\"]==shape)\n",
    "    print(shape, ' ----------- ', n)\n",
    "\n",
    "# Compare beh and task distances\n",
    "list_shape = dfdat[\"shape\"].values.tolist()\n",
    "Cl_beh = ClustDict[\"beh_motor_sim\"]\n",
    "Cl_task = ClustDict[\"task_image_sim\"]\n",
    "assert Cl_beh.Labels==list_shape\n",
    "assert Cl_task.Labels==list_shape\n",
    "# - iterate over all pairs of shapes\n",
    "dat = []\n",
    "for i, shape1 in enumerate(list_shape):\n",
    "    for ii, shape2 in enumerate(list_shape):\n",
    "        if ii<=i:\n",
    "            continue\n",
    "                \n",
    "        sim_beh = Cl_beh.find_dat_by_label(shape1, shape2)\n",
    "        sim_task = Cl_task.find_dat_by_label(shape1, shape2)\n",
    "        \n",
    "        dat.append({\n",
    "            \"ind_shape1\":i,\n",
    "            \"ind_shape2\":ii,\n",
    "            \"ind_shape_pair\":f\"{i}-{ii}\",\n",
    "            \"shape1\":shape1,\n",
    "            \"shape2\":shape2,\n",
    "            \"sim_beh\":sim_beh,\n",
    "            \"sim_task\":sim_task\n",
    "        })\n",
    "        \n",
    "        print(shape1, shape2, sim_beh, sim_task)\n",
    "\n",
    "df = pd.DataFrame(dat)\n",
    "df[\"sim_abs_diff\"] = np.abs(df[\"sim_beh\"] - df[\"sim_task\"])\n",
    "df[\"sim_beh_minus_task\"] = df[\"sim_beh\"] - df[\"sim_task\"]\n",
    "\n",
    "\n",
    "# dfthis = df[df[\"sim_beh_minus_task\"]>0.5].reset_index(drop=True)\n",
    "dfthis = df[df[\"sim_beh_minus_task\"]<-0.5].reset_index(drop=True)\n",
    "dfthis\n",
    "\n",
    "sns.pairplot(data=dfthis, vars=[\"sim_beh\", \"sim_task\"])\n",
    "\n",
    "sns.scatterplot(data=dfthis, x=\"ind_shape_pair\", y=\"sim_abs_diff\")\n",
    "sns.scatterplot(data=dfthis, x=\"ind_shape_pair\", y=\"sim_abs_diff\")\n",
    "\n",
    "# for each pair, extract task and beh and plot\n",
    "\n",
    "ind = 7\n",
    "\n",
    "sh1 = dfthis.iloc[ind][\"shape1\"]\n",
    "sh2 = dfthis.iloc[ind][\"shape2\"]\n",
    "i1 = dfthis.iloc[ind][\"ind_shape1\"]\n",
    "i2 = dfthis.iloc[ind][\"ind_shape2\"]\n",
    "sim_beh = dfthis.iloc[ind][\"sim_beh\"]\n",
    "sim_task = dfthis.iloc[ind][\"sim_task\"]\n",
    "\n",
    "strok_beh_1 = dfdat.iloc[i1][\"strok\"]\n",
    "strok_beh_2 = dfdat.iloc[i2][\"strok\"]\n",
    "strok_task_1 = dfdat.iloc[i1][\"strok_task\"]\n",
    "strok_task_2 = dfdat.iloc[i2][\"strok_task\"]\n",
    "\n",
    "list_stroke_this = [strok_beh_1, strok_beh_2, strok_task_1, strok_task_2]\n",
    "DS.plot_multiple_strok(list_stroke_this, overlay=False, ncols=2)\n",
    "\n",
    "print(\"shapes:\", sh1, \" | \", sh2)\n",
    "print(\"sim_beh:\", sim_beh)\n",
    "print(\"sim_task:\", sim_task)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "465657f7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO:\n",
    "1. Replace this with clustergood_featurespace_project\n",
    "ClustDict = DS.features_generate_clusters_from_dataset(dfdat)\n",
    "2. Conmbine all plots\n",
    "3. Add features (e.g., starting stroke, etc)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "19d8efcc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### OLD PRIM PLOTS"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53210d7f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### CHARACTERS"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5530e988"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##### Generate data\n",
    "from pythonlib.dataset.dataset_analy.characters import pipeline_generate_and_plot_all, generate_data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86f4fe79"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "do_plots = True\n",
    "RES, savedir = pipeline_generate_and_plot_all(D, do_plots, filter_taskkind=None)\n",
    "\n",
    "if False:\n",
    "    filter_taskkind = None\n",
    "\n",
    "    # list_distance_ver=[\"hausdorff_alignedonset\"]\n",
    "    # list_distance_ver=[\"euclidian_diffs\", \"euclidian\"]\n",
    "    # list_distance_ver=[\"euclidian\"]\n",
    "    list_distance_ver = None\n",
    "\n",
    "    # RES, savedir = pipeline_generate_and_plot_all(D, False, filter_taskkind=filter_taskkind,\n",
    "    #                                              list_distance_ver=list_distance_ver)\n",
    "\n",
    "    ds_filterdict = {\"gridsize\":[\"rig3_3x3_small\"]}\n",
    "    ds_clean_methods = [\"remove_if_multiple_behstrokes_per_taskstroke\", \"stroke_too_short\"]\n",
    "    ds_clean_params = {\n",
    "        \"min_stroke_length\":50\n",
    "    }\n",
    "\n",
    "    RES = generate_data(D, list_distance_ver=list_distance_ver, filter_taskkind=filter_taskkind,\n",
    "                 ds_clean_methods=ds_clean_methods, ds_clean_params=ds_clean_params,\n",
    "                 ds_filterdict=ds_filterdict)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14c7b6b0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Plot exmaple strokes, sorted by labels"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a32a85b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1) give dummy columnname in DS, based on label\n",
    "\n",
    "gmm_n = 11\n",
    "# gmm_n = 17\n",
    "\n",
    "labels = Cl.cluster_extract_label(\"gmm_using_tsne\", gmm_n)\n",
    "DS.Dat[\"labels_dummy\"] = labels\n",
    "fig = DS.plotshape_multshapes_trials_grid(\"labels_dummy\", nrows=4)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2a2d87b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### PLOTS originally made for single prims"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df83dce1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SDIR = D.make_savedir_for_analysis_figures_BETTER(\"clustering_prims_new\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a7f97ea"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DS.featu"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9f7e333"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##### Distributions of combos of shapes used in each char\n",
    "\n",
    "\n",
    "# Which shapes tend to be drawn correctly?\n",
    "\n",
    "from pythonlib.dataset.dataset_analy.characters import plot_prim_sequences\n",
    "plot_prim_sequences(RES, D, savedir)\n",
    "\n",
    "\n",
    "# 2) Cluster stroke, along different dimensions --> plot ordered by cluster.\n",
    "\n",
    "# 3) For each unique character, plot it and the strokes used for it\n",
    "\n",
    "\n",
    "\n",
    "# For each character, get each stroke's clustering score.\n",
    "\n",
    "\n",
    "# 3) For each unique character, plot it and the strokes used for it\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c5c6128"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Compare feature spaces\n",
    "\n",
    "e.g., theymake different predictions for similarity space in neural activity"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad239121"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e34b827"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PUTTING ALL TOGETHER"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed1d728b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "use both trials and shape average.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79d4dec2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "72e79c73"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Getting random parses from primitives"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7325f349"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "doesnt make sense...\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6aface96"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TESTING - getting random parses"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3eb1f914"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from pythonlib.drawmodel.parsing import get_parses_from_strokes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "586524ff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "USE_TORCH = False\n",
    "# VERY FAST\n",
    "params_parse = {\n",
    "    \"configs_per\":5,\n",
    "    \"trials_per\":10,\n",
    "    \"max_ntrials\":25, \n",
    "    \"max_nwalk\":25,\n",
    "    \"max_nstroke\":25\n",
    "}\n",
    "return_in_strokes_coords = True\n",
    "kparses = 10\n",
    "# animal = \"Red\"\n",
    "\n",
    "use_extra_junctions=True\n",
    "# score_ver = \"travel\"\n",
    "score_ver = \"travel_from_orig\" # better, since differentiates 2 tasks thjat are just flipped (and so will not throw one of them out)\n",
    "score_norm = \"negative\"\n",
    "image_WH = 105\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70244a07"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get sketchpad edges\n",
    "maxes = []\n",
    "for k, v in D.Metadats.items():\n",
    "    maxes.append(np.max(np.abs(v[\"sketchpad_edges\"].flatten())))\n",
    "canvas_max_WH = np.max(maxes)\n",
    "\n",
    "# origin for tasks, is generally around (0, 50) to (0,100). so just hard code here.\n",
    "# origin = torch.tensor([image_WH/2, -(0.45*image_WH)])\n",
    "origin = np.array([image_WH/2, -(0.45*image_WH)])\n",
    "# alternatively, could look into D.Dat[\"origin\"], and convert to image coords [(1,105), (-105, -1)]\n",
    "\n",
    "# For each row, parse its task\n",
    "score_fn = lambda parses: score_function(parses, ver=score_ver, \n",
    "                                         normalization=score_norm, use_torch=USE_TORCH, origin=origin)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d43dd50"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Just testing, pick a random trial\n",
    "    import random\n",
    "    ind = random.sample(range(len(D.Dat)), 1)[0]\n",
    "    strokes = D.Dat[\"strokes_task\"].values[ind]\n",
    "\n",
    "    if False:\n",
    "        from pythonlib.tools.stroketools import strokesInterpolate2\n",
    "        strokes = strokesInterpolate2(strokes, N=[\"npts\", 100])\n",
    "    else:\n",
    "        pass\n",
    "    parses, log_probs_k = get_parses_from_strokes(strokes, canvas_max_WH, \n",
    "                                                  use_extra_junctions=use_extra_junctions, plot=True,\n",
    "                                                 return_in_strokes_coords=True, k=5)\n",
    "\n",
    "# save params\n",
    "params_parse[\"canvas_max_WH\"] = canvas_max_WH\n",
    "params_parse[\"use_extra_junctions\"] = use_extra_junctions\n",
    "params_parse[\"return_in_strokes_coords\"] = return_in_strokes_coords\n",
    "params_parse[\"score_ver\"] = score_ver\n",
    "params_parse[\"score_norm\"] = score_norm\n",
    "\n",
    "# OLD VERSION - GET EACH ROW\n",
    "#     # Collect parses\n",
    "#     PARSES = []\n",
    "#     for row in D.Dat.iterrows():\n",
    "#         if row[0]%100==0:\n",
    "#             print(row[0])\n",
    "#         strokes = row[1][\"strokes_task\"]\n",
    "#         index = row[0]\n",
    "#         trial_id = row[1][\"trialcode\"]\n",
    "#         unique_task_name = row[1][\"unique_task_name\"]\n",
    "#         character = row[1][\"character\"]\n",
    "\n",
    "#         parses, log_probs = get_parses_from_strokes(strokes, canvas_max_WH, \n",
    "#                                                   use_extra_junctions=use_extra_junctions, plot=False,\n",
    "#                                                  return_in_strokes_coords=return_in_strokes_coords, k=k,\n",
    "#                                                    configs_per = params_parse[\"configs_per\"],\n",
    "#                                                    trials_per = params_parse[\"trials_per\"],\n",
    "#                                                    max_ntrials = params_parse[\"max_ntrials\"],\n",
    "#                                                    max_nstroke = params_parse[\"max_nstroke\"],\n",
    "#                                                    max_nwalk = params_parse[\"max_nwalk\"],\n",
    "#                                                    )\n",
    "\n",
    "#         PARSES.append(\n",
    "#             {\"strokes_task\":strokes,\n",
    "#              \"index_dat\":index,\n",
    "#              \"trial_id\":trial_id,\n",
    "#              \"unique_task_name\":unique_task_name,\n",
    "#              \"character\":character,\n",
    "#              \"parses\":parses,\n",
    "#              \"parses_log_probs\":log_probs}\n",
    "#         )\n",
    "\n",
    "# NEW VERSION - only do once for each unique task\n",
    "tasklist = sorted(list(set(D.Dat[\"unique_task_name\"])))\n",
    "# Collect parses\n",
    "PARSES = []\n",
    "\n",
    "for i, task in enumerate(tasklist):\n",
    "    # find the first row that has this task\n",
    "    row = D.Dat[D.Dat[\"unique_task_name\"]==task].iloc[0]\n",
    "    assert row[\"unique_task_name\"]==task\n",
    "\n",
    "#         if i%20==0:\n",
    "#             print(i, \"-\",  task)\n",
    "    print(i, \"-\",  task)\n",
    "    strokes = row[\"strokes_task\"]\n",
    "\n",
    "    parses, log_probs = get_parses_from_strokes(strokes, canvas_max_WH, \n",
    "                                              use_extra_junctions=use_extra_junctions, \n",
    "                                              score_fn=score_fn,\n",
    "                                                plot=False, image_WH=image_WH,\n",
    "                                                return_in_strokes_coords=return_in_strokes_coords, \n",
    "                                                k=kparses, configs_per = params_parse[\"configs_per\"],\n",
    "                                               trials_per = params_parse[\"trials_per\"],\n",
    "                                               max_ntrials = params_parse[\"max_ntrials\"],\n",
    "                                               max_nstroke = params_parse[\"max_nstroke\"],\n",
    "                                               max_nwalk = params_parse[\"max_nwalk\"],\n",
    "                                               )\n",
    "    assert len(parses)>0, \"why?\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92456298"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    PARSES.append(\n",
    "        {\"strokes_task\":strokes,\n",
    "         \"unique_task_name\":task,\n",
    "         \"parses\":parses,\n",
    "         \"parses_log_probs\":log_probs}\n",
    "    )\n",
    "\n",
    "# === save as dataframe\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "DIR, FNAME = os.path.split(paththis)\n",
    "fname_parse = f\"{DIR}/{FNAME}/parses.pkl\"\n",
    "print(\"Saving at:\")\n",
    "print(fname_parse)\n",
    "\n",
    "PARSES  = pd.DataFrame(PARSES)\n",
    "PARSES.to_pickle(fname_parse)\n",
    "\n",
    "fname_params = f\"{DIR}/{FNAME}/parses_params.pkl\"\n",
    "with open(fname_params, \"wb\") as f:\n",
    "    pickle.dump(params_parse, f)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a631b6c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
