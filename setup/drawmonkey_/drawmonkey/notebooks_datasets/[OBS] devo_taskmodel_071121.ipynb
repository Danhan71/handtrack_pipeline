{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndeveloping code for taking\\n- dataset, with parses (pre extracted)\\n- building \"planner\" models (new BehModel classes)\\n- combining into a behmodelhandler, which is general purpose\\n\\nUPDATED: finalized stuff - actually running modeling, traiing models etc, is moved to:\\ndevo_taskmodel_finalized_071721.ipynb\\n\\n[OBSOLETE] - This was used to first develop Task modeling code. It combines (i) Motor Cost and (ii) Dataset task model stuff.\\nNothing here is up to date, some are sort of idfferent branches compared to what I ended up doing. \\nBut hihgly unlikely to use anything here.\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "developing code for taking\n",
    "- dataset, with parses (pre extracted)\n",
    "- building \"planner\" models (new BehModel classes)\n",
    "- combining into a behmodelhandler, which is general purpose\n",
    "\n",
    "UPDATED: finalized stuff - actually running modeling, traiing models etc, is moved to:\n",
    "devo_taskmodel_finalized_071721.ipynb\n",
    "\n",
    "[OBSOLETE] - This was used to first develop Task modeling code. It combines (i) Motor Cost and (ii) Dataset task model stuff.\n",
    "Nothing here is up to date, some are sort of idfferent branches compared to what I ended up doing. \n",
    "But hihgly unlikely to use anything here.\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ## OUTLINE\n",
    "\n",
    "\n",
    "# Tasks module (holds information for this task)\n",
    "\n",
    "# Behavior module (holds behavior on this task)\n",
    "\n",
    "# Parses module\n",
    "# - holds all parses for a given task.\n",
    "# - Representations:\n",
    "#     - strokes\n",
    "#     - symbolic features (number strokes)\n",
    "#     - graph control pts.\n",
    "#     - [optional: consider direction?]\n",
    "\n",
    "# Model module\n",
    "# - Flexible holding of params, can train given new set of tasks, if needed.\n",
    "#     - Prior scorer\n",
    "#     - Likeli scorer\n",
    "#     - Posterior scorer\n",
    "# - can score any input parses, P(parse | prior)\n",
    "#     - Parses represented both as strokes and graph control pts (and objects ?)\n",
    "#     - INPUT: All kinds of models\n",
    "#         - ActionGrammar (log linear version)\n",
    "#             - MotorCost\n",
    "#             - [Flatten out MotorCost so that Action Grammar is flat]\n",
    "#             - [Each of these combined in log linear model]\n",
    "#         - Stroke manifold model (or other form of learned action primitive)\n",
    "#         - GNS\n",
    "# - can score any behavior, P(beh|parse)\n",
    "#     - INPUT: scoring functions (likeli)\n",
    "#         - \n",
    "# - can get summary score (posterior)\n",
    "#     - INPUT: flexible posterior function\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "# # Structure of current taskmodel\n",
    "\n",
    "# Model\n",
    "#     - parser\n",
    "#     - prior fun\n",
    "#     - prior norm\n",
    "#     - likelis\n",
    "#     - posterior\n",
    "\n",
    "# Dataset\n",
    "# - data\n",
    "# - Model\n",
    "# - plots/summaries\n",
    "\n",
    "# getprior\n",
    "# getlikeli\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code from motor cost (devo_taskmodel_motorcost_010321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) New MC model\n",
    "from pythonlib.drawmodel.efficiencycost import Cost\n",
    "MC = Cost()\n",
    "\n",
    "# 1) extract probedat anew\n",
    "Probedat = ProbedatTaskmodel(PROBEDAT, MD)\n",
    "if False:\n",
    "    filtdict = {\n",
    "    #     \"date\":[\"200930\", \"201001\"]}\n",
    "        \"date\":[\"200928\", \"200929\", \"200930\", \"201001\"]}\n",
    "    Probedat.filterProbedat(filtdict);\n",
    "\n",
    "# == define prior function based on MC model\n",
    "def priorFunction(p, trial):\n",
    "    return MC.score(p[\"strokes\"], trial[\"task\"])\n",
    "\n",
    "# 2) apply mdoel\n",
    "params_data, params_model = Probedat.getParams()\n",
    "if False:\n",
    "    params_model[\"priorver\"] = priorFunction\n",
    "params_data[\"standardize\"][\"standardize_strokes\"] = True\n",
    "\n",
    "Probedat.applyModel(params_data, params_model)\n",
    "Probedat.Datamodel.plotPosteriorHist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code from dataset version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) model-based features (task model score)\n",
    "from pythonlib.dataset.analy import taskmodel_assign_score\n",
    "taskmodel_assign_score(Dthis)\n",
    "\n",
    "# model summary score\n",
    "def F(x):\n",
    "    \"\"\"compare two models, retgurns index between -1,1\n",
    "    \"\"\"   \n",
    "    a = x[\"MOD_3line\"]\n",
    "    b = x[\"MOD_linePlusL\"]\n",
    "    return 2*((a/(a+b))-0.5)\n",
    "from pythonlib.tools.pandastools import applyFunctionToAllRows\n",
    "Dthis.Dat = applyFunctionToAllRows(Dthis.Dat, F, \"modelcomp_offline\")\n",
    "# FEATURE_NAMES = list(set(FEATURE_NAMES+ [\"MOD_3line\", \"MOD_linePlusL\", \"mod2_minus_mod1\", \"modelcomp_offline\"]))\n",
    "FEATURE_NAMES = list(set(FEATURE_NAMES+ [\"MOD_3line\", \"MOD_linePlusL\", \"modelcomp_offline\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "from pythonlib.dataset.dataset import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### Load a dataset, to have tasks to test\n",
    "animal = \"Red\"\n",
    "expt = \"lines5\"\n",
    "rule = \"straight\"\n",
    "D = Dataset([])\n",
    "D.load_dataset_helper(animal, expt, rule=rule)\n",
    "D.load_tasks_helper(convert_coords_to_abstract=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only fixed tasks\n",
    "D.filterPandas({\"random_task\":[False]}, \"modify\")\n",
    "# Limit to subset of trials, just for devo reasons\n",
    "D.subsampleTrials(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load presaved parses\n",
    "# keep only fixed tasks\n",
    "D.filterPandas({\"random_task\":[False]}, \"modify\")\n",
    "\n",
    "list_parse_params = [\n",
    "    {\"quick\":True, \"ver\":\"graphmod\", \"savenote\":\"fixed_True\"},\n",
    "    {\"quick\":True, \"ver\":\"nographmod\", \"savenote\":\"fixed_True\"}]\n",
    "\n",
    "list_suffixes = [\"graphmod\", \"nographmod\"]\n",
    "\n",
    "    \n",
    "D.parser_load_presaved_parses(list_parse_params, list_suffixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalize strokes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: Quick way to extract all unique parses from multiple Parsers\n",
    "iterate over all pairs across Parsers. no need to iterate within since already checked that they are unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract parses\n",
    "import random\n",
    "ind = random.randint(0, len(D.Dat))\n",
    "list_colnames = [name for name in D.Dat.columns if \"parser_\" in name]\n",
    "\n",
    "list_parsers = [D.Dat.iloc[ind][name] for name in list_colnames]\n",
    "\n",
    "P1 = list_parsers[0]\n",
    "P2 = list_parsers[1]\n",
    "\n",
    "# MERGE PARSERS\n",
    "# if graphs are the same, \n",
    "print(P1.Graph == P2.Graph)\n",
    "\n",
    "\n",
    "P1.plot_graph()\n",
    "P2.plot_graph()\n",
    "\n",
    "P1.print_graph()\n",
    "P2.print_graph()\n",
    "\n",
    "P1.summarize_parses()\n",
    "P2.summarize_parses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Likeli functions\n",
    "from pythonlib.drawmodel.strokedists import distscalarStrokes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continue analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct model\n",
    "from pythonlib.behmodel.scorer.scorer import Scorer\n",
    "import numpy as np\n",
    "from pythonlib.drawmodel.efficiencycost import Cost\n",
    "\n",
    "\n",
    "######### PRIOR\n",
    "# == define prior function based on MC model\n",
    "MC = Cost()\n",
    "Pr = Scorer()\n",
    "# def F(strokes):\n",
    "#     return MC.score(strokes)\n",
    "# def F(strokes):\n",
    "#     return np.random.rand()\n",
    "def F(strokes):\n",
    "    return score_function([strokes], ver=\"ink\", normalization = \"negative\", test=False,\n",
    "                  use_torch=False, origin=None)\n",
    "Pr.input_score_function(F)\n",
    "\n",
    "######## LIKELI\n",
    "from pythonlib.drawmodel.strokedists import distscalarStrokes\n",
    "Li = Scorer()\n",
    "def F(beh, parse):\n",
    "    return distscalarStrokes(beh, parse, \"dtw_segments\")\n",
    "#     return np.random.rand()\n",
    "Li.input_score_function(F)\n",
    "\n",
    "######## POSTERIOR\n",
    "Po = Scorer()\n",
    "def F(likelis, priors):\n",
    "    return np.dot(likelis, priors)\n",
    "#     return np.random.rand()\n",
    "Po.input_score_function(F)\n",
    "\n",
    "\n",
    "######## TASK (task and parses)\n",
    "from pythonlib.behmodel.score_dataset import prepare_trial\n",
    "ind=1\n",
    "Beh, Task = prepare_trial(D, ind)\n",
    "\n",
    "###### MODEL\n",
    "from pythonlib.behmodel.behmodel import BehModel\n",
    "\n",
    "BM = BehModel()\n",
    "BM.input_model_components(Pr, Li, Po)\n",
    "BM.score_single_trial(Beh, Task)\n",
    "\n",
    "# Model module\n",
    "# - Flexible holding of params, can train given new set of tasks, if needed.\n",
    "#     - Prior scorer\n",
    "#     - Likeli scorer\n",
    "#     - Posterior scorer\n",
    "# - can score any input parses, P(parse | prior)\n",
    "#     - Parses represented both as strokes and graph control pts (and objects ?)\n",
    "#     - INPUT: All kinds of models\n",
    "#         - ActionGrammar (log linear version)\n",
    "#             - MotorCost\n",
    "#             - [Flatten out MotorCost so that Action Grammar is flat]\n",
    "#             - [Each of these combined in log linear model]\n",
    "#         - Stroke manifold model (or other form of learned action primitive)\n",
    "#         - GNS\n",
    "# - can score any behavior, P(beh|parse)\n",
    "#     - INPUT: scoring functions (likeli)\n",
    "#         - \n",
    "# - can get summary score (posterior)\n",
    "#     - INPUT: flexible posterior function\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.extract_all_parses_as_list(\"summary\")[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = D.parser_list_of_parses(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### visualize each parse\n",
    "\n",
    "def plotStrokes(strokes, ax):\n",
    "    \"\"\" wrapper\"\"\"\n",
    "    from pythonlib.drawmodel.strokePlots import plotDatStrokes\n",
    "\n",
    "#     plotDatStrokes(strokes, ax=ax, plotver=\"order\", add_stroke_number=False, each_stroke_separate=False)\n",
    "    plotDatStrokes(strokes, ax=ax, clean_ordered=True)\n",
    "    \n",
    "def plotExampleTrial(BehMod, max_parses = 34, sort_by = None):\n",
    "    \"\"\" useful plots of beahviora nd scores/priors/likelies, for a given trial\n",
    "    trial inds are from 0, 1, 2, ... (i.e,. not to original trial indices in \n",
    "    filedata)\"\"\"\n",
    "    print(\"NOTE: the x and y lims are hacky, should change\")\n",
    "\n",
    "    ncols = 6\n",
    "    Parser = BehMod._task.Parser\n",
    "    nparse = len(Parser.Parses)\n",
    "    nparse = min(max_parses, nparse)\n",
    "    nrows = int(np.ceil((nparse+2)/ncols))\n",
    "\n",
    "    # plt.figure(figsize=(ncols*3,nrows*3))\n",
    "    fig, axes = plt.subplots(nrows = nrows, ncols=ncols, \n",
    "        figsize=(ncols*3,nrows*3), sharex=True, sharey=True, squeeze=False)\n",
    "    \n",
    "    # 1) ==== BEHAVIOR\n",
    "    strokes_beh = BehMod._behavior.Strokes\n",
    "#     strokes_beh = self.trials[trialind][\"behavior\"][\"strokes\"]\n",
    "    # ax = plt.subplot(nrows, ncols, 1)\n",
    "    ax = axes[0,0]\n",
    "    # ax = plotTrialSimple(filedata, 1, ax=ax, plotver=\"empty\", nakedplot=True, plot_task_stimulus=False, plot_drawing_behavior=False)\n",
    "    ax.plot(1,1)\n",
    "    plotStrokes(strokes_beh, ax)\n",
    "    # plt.xlim([-400, 400])\n",
    "    # plt.ylim([-600, 600])\n",
    "    ax.set_title(\"behavior\")\n",
    "\n",
    "    # 2) ==== TASK\n",
    "#     strokes = self.trials[trialind][\"task\"][\"strokes\"]\n",
    "    post = BehMod._posterior_score\n",
    "#     post = self.trials[trialind][\"posterior\"]\n",
    "    # ax = plt.subplot(nrows, ncols, 2)\n",
    "#     ax = axes[0,1]\n",
    "#     plotStrokes(strokes, ax)\n",
    "#     ax.plot(1,1,'o')\n",
    "    # plt.xlim([-400, 400])\n",
    "    # plt.ylim([-600, 600])\n",
    "#     print(post)\n",
    "# #     ax.set_title(f\"task|post={post:.2f}\")\n",
    "#     ax.set_title(f\"task|post={post}\")\n",
    "    \n",
    "    parses = [Parser.Parses[i][\"strokes\"] for i in range(len(Parser.Parses))]\n",
    "    likelis = BehMod._likeli_scores\n",
    "    priors = BehMod._prior_scores\n",
    "#     posteriors = BehMod._post_scores\n",
    "    tmp = [(par, li, pr) for par, li, pr in zip(parses, likelis, priors)]\n",
    "#     parses = self.trials[trialind][\"model_parses\"]\n",
    "    if sort_by is not None:\n",
    "        if sort_by == \"likeli\":\n",
    "            # print(sorted(parses, key=lambda x:x[\"likeli\"])[0])\n",
    "            tmp = sorted(tmp, key=lambda x:-x[1])\n",
    "        elif sort_by == \"prior\":\n",
    "            tmp = sorted(tmp, key=lambda x:-x[2])\n",
    "        else:\n",
    "            print(sort_by)\n",
    "            assert False, \"not coded\"\n",
    "        parses = [t[0] for t in tmp]\n",
    "        likelis = [t[1] for t in tmp]\n",
    "        priors = [t[2] for t in tmp]\n",
    "\n",
    "    for i, (p, li, pr) in enumerate(zip(parses, likelis, priors)):\n",
    "        if i<max_parses:\n",
    "            ax = axes.flatten()[i+2]\n",
    "            # ax = plt.subplot(nrows,ncols,i+3)\n",
    "            plotStrokes(p, ax)\n",
    "            \n",
    "#             ax.set_title(f\"pr{pr:.2f}, li{li:.2f}\")\n",
    "            ax.set_title(f\"pr{pr}, li{li}\")\n",
    "            \n",
    "            # plt.xlim([-400, 400])\n",
    "            # plt.ylim([-400, 400])\n",
    "            \n",
    "plotExampleTrial(BM, sort_by=\"likeli\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score not just one trial, but entire dataset, using a single model.\n",
    "\n",
    "score_list = []\n",
    "for i in range(len(D.Dat)):\n",
    "    if i%10==0: \n",
    "        print(i)\n",
    "    Beh, Task = prepare_trial(D, i)\n",
    "    score_list.append(BM.score_single_trial(Beh, Task))\n",
    "#     assert False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top parses based on prior, likeli.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### TODO:\n",
    "# 1) motor cost model is very slow...\n",
    "# 2) scorer can normalize scores to probabilities (softmax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extractor\n",
    "Single parse (ind in Parser object) --> list of scalar features\n",
    "\n",
    "NOTE: This builds on efficiencycost model (pulls in those things here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for debugging.\n",
    "D.Dat.iloc[indtrial][\"parser_nographmod\"].Parses = D.Dat.iloc[indtrial][\"parser_nographmod\"].Parses[:1]\n",
    "D.Dat.iloc[indtrial][\"parser_graphmod\"].Parses = D.Dat.iloc[indtrial][\"parser_graphmod\"].Parses[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes in a Dataset row, and outputs list of scores, one for each parse..\n",
    "\n",
    "# Prior\n",
    "from pythonlib.behmodel.scorer.prior_functions import prior_feature_extractor\n",
    "Pr = prior_feature_extractor()\n",
    "\n",
    "# Likeli\n",
    "from pythonlib.behmodel.scorer.likeli_functions import likeli_dataset\n",
    "Li = likeli_dataset()\n",
    "\n",
    "# Post\n",
    "Po = Scorer()\n",
    "def F(likelis, priorprobs):\n",
    "    from pythonlib.behmodel.scorer.utils import posterior_score\n",
    "    postscore = posterior_score(likelis, priorprobs, \"weighted\")\n",
    "    return postscore\n",
    "Po.input_score_function(F)\n",
    "\n",
    "###### MODEL\n",
    "from pythonlib.behmodel.behmodel import BehModel\n",
    "indtrial = 6\n",
    "BM = BehModel()\n",
    "BM.input_model_components(Pr, Li, Po)\n",
    "BM.score_single_trial_dataset(D, indtrial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "BM.plot_scatter_likeli_prior()\n",
    "\n",
    "N = 5\n",
    "sort_by = \"prior\"\n",
    "BM.plot_sorted_by(sort_by, N)\n",
    "sort_by = \"likeli\"\n",
    "BM.plot_sorted_by(sort_by, N)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GET SCORES ACROSS ALL TRIALS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonlib.behmodel.score_dataset import score_dataset\n",
    "from pythonlib.behmodel.scorer.prior_functions import prior_feature_extractor\n",
    "from pythonlib.behmodel.scorer.likeli_functions import likeli_dataset\n",
    "from pythonlib.behmodel.scorer.poster_functions import poster_dataset\n",
    "from pythonlib.behmodel.behmodel import BehModel\n",
    "from pythonlib.behmodel.scorer.scorer import Scorer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "from pythonlib.dataset.dataset import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SDIR = \"/data2/analyses/main/model_comp/planner\"\n",
    "\n",
    "### Load a dataset, to have tasks to test\n",
    "animal = \"Red\"\n",
    "expt = \"lines5\"\n",
    "for rule in [\"straight\", \"bent\"]:\n",
    "    D = Dataset([])\n",
    "    D.load_dataset_helper(animal, expt, rule=rule)\n",
    "    D.load_tasks_helper()\n",
    "    D.filterPandas({\"random_task\":[False]}, \"modify\")\n",
    "    list_parse_params = [\n",
    "        {\"quick\":True, \"ver\":\"graphmod\", \"savenote\":\"fixed_True\"},\n",
    "        {\"quick\":True, \"ver\":\"nographmod\", \"savenote\":\"fixed_True\"}]\n",
    "    list_suffixes = [\"graphmod\", \"nographmod\"]\n",
    "    D.parser_load_presaved_parses(list_parse_params, list_suffixes)\n",
    "    \n",
    "    for rule_model in [\"straight\", \"bent\"]:\n",
    "        \n",
    "        # Prior\n",
    "        Pr = prior_feature_extractor(rule=rule_model)\n",
    "        # Likeli\n",
    "        Li = likeli_dataset()\n",
    "        # Post\n",
    "        Po = Scorer()\n",
    "        def F(likelis, priorprobs):\n",
    "            from pythonlib.behmodel.scorer.utils import posterior_score\n",
    "            postscore = posterior_score(likelis, priorprobs, \"weighted\")\n",
    "            return postscore\n",
    "        Po.input_score_function(F)\n",
    "        # Model\n",
    "        BM = BehModel()\n",
    "        BM.input_model_components(Pr, Li, Po)\n",
    "\n",
    "        # savedir\n",
    "        sdir = f\"{SDIR}/pilot/dset_{D.identifier_string()}-vs-mod_{rule_model}\"\n",
    "        import os\n",
    "        os.makedirs(sdir, exist_ok=True)\n",
    "        \n",
    "        # RUN\n",
    "        score_dataset(D, BM, saveon=True, sdir=sdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Holder, given multiple datasets and multiple BMs.\n",
    "\n",
    "\n",
    "# Subsample parses, e.,g to top-likel, bottom-likeli, and uniform in between\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-computed scores (for each dataset - model pair) and combine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load datset, extract its score for each model.\n",
    "animal = \"Red\"\n",
    "expt = \"lines5\"\n",
    "Dlist = []\n",
    "for rule in [\"straight\", \"bent\"]:\n",
    "    D = Dataset([])\n",
    "    D.load_dataset_helper(animal, expt, rule=rule)\n",
    "    D.load_tasks_helper()\n",
    "    D.filterPandas({\"random_task\":[False]}, \"modify\")    \n",
    "    Dlist.append(D)\n",
    "    \n",
    "    for rule_model in [\"bent\", \"straight\"]:\n",
    "        model_id = [\"pilot\", rule_model]\n",
    "        \n",
    "        D.parser_load_precomputed_posteriors(model_id)\n",
    "        \n",
    "    \n",
    "# Combine multiple datasets (must have same models)\n",
    "\n",
    "# Plot scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonlib.dataset.dataset import concatDatasets\n",
    "D = concatDatasets(Dlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.Dat[\"parser_postscores_pilot_bent\"].hist()\n",
    "D.Dat[\"parser_postscores_pilot_straight\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.Dat.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure no repeated trialcodes\n",
    "assert len(D.Dat[\"trialcode\"].unique().tolist()) == len(D.Dat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess to get groups\n",
    "from pythonlib.dataset.analy import preprocessDat\n",
    "\n",
    "D, GROUPING, GROUPING_LEVELS, FEATURE_NAMES, SCORE_COL_NAMES = preprocessDat(D, \"lines5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORE_COL_NAMES = [\"parser_postscores_pilot_bent\", \"parser_postscores_pilot_straight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot, separated by group and model\n",
    "from pythonlib.dataset.beh_model_comparison import plots_cross_prior_and_model\n",
    "\n",
    "plots_cross_prior_and_model(D.Dat, GROUPING, GROUPING_LEVELS, SCORE_COL_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_function(parses, ver=\"ink\", normalization = \"inverse\", test=False,\n",
    "                  use_torch=False, origin=None):\n",
    "    \"\"\" \n",
    "    - ver, str, determines what score to use\n",
    "    --- \"ink\", then total distnace traveled on page\n",
    "    --- \"travel\", then total distance traveled, including\n",
    "    gaps, starting from position of first touch.\n",
    "    - normalization, how to normalize raw distnace. distance will\n",
    "    be that more positive is more worse. \n",
    "    --- inverse take inverse, so that now less positive is worse.\n",
    "    --- negative, ...\n",
    "    \"\"\"\n",
    "    from pythonlib.drawmodel.features import strokeDistances, computeDistTraveled\n",
    "\n",
    "    if test:\n",
    "        # then just return random number, one for each parse\n",
    "        return torch.tensor([random.random() for _ in range(len(parses))])    \n",
    "    \n",
    "    if ver==\"ink\":\n",
    "        # === Total ink used\n",
    "        distances = [np.sum(strokeDistances(strokes)) for strokes in parses]\n",
    "    elif ver==\"travel\":\n",
    "        # conisder origin to be onset of first storke.\n",
    "        # Note: has issue in that a single stroke task, flipped, is idnetical cost to the same task unflipped.\n",
    "        # leads to problems later since unique(score) is used to throw out redundant parses.\n",
    "        distances_traveled = [computeDistTraveled(strokes, origin=strokes[0][0,[0,1]]) for strokes in parses]\n",
    "        distances = distances_traveled\n",
    "    elif ver==\"travel_from_orig\":\n",
    "        # pass in origin. \n",
    "        assert origin is not None, \" must pass in coordinate for origin\"\n",
    "        distances_traveled = [computeDistTraveled(strokes, origin=origin) for strokes in parses]\n",
    "        distances = distances_traveled\n",
    "\n",
    "    elif ver==\"nstrokes\":\n",
    "        # num strokes\n",
    "        # == plit histogram of num strokes\n",
    "        nstrokes = [len(p) for p in parses]        \n",
    "    else:\n",
    "        print(ver)\n",
    "        assert False, \"not codede\"\n",
    "        \n",
    "    if use_torch:\n",
    "        distances = torch.tensor(distances)\n",
    "    else:\n",
    "        distances = np.array(distances)\n",
    "        \n",
    "    if normalization==\"inverse\":\n",
    "        return 1/distances\n",
    "    elif normalization==\"negative\":\n",
    "        return -distances\n",
    "    else:\n",
    "        print(normalization)\n",
    "        assert False, \"not coded\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GENERATIVE MODEL (LIKE BPL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonlib.behmodel.drawmodel.model.model import CharacterModel as DrawModel\n",
    "from pythonlib.behmodel.drawmodel.library.library import Library\n",
    "from pythonlib.bpl.strokesToProgram import plotMP\n",
    "\n",
    "lib = Library()\n",
    "DM = DrawModel(lib)\n",
    "ctype = DM.sample_type()\n",
    "plotMP(ctype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## INFERENCE\n",
    "\n",
    "# 1) For each parsed stroke, assign to max likeli using mixture model.\n",
    "# OR:\n",
    "# 1) assign a prob based on mixture model (posterior score)\n",
    "\n",
    "# 2) Fit model using similar method to Lake\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
