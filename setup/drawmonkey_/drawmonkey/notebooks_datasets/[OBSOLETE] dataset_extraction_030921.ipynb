{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "DEVELOPing stuff, \n",
    "mult day analysis, based on extracting experiments\n",
    "\n",
    "3/14/21 - All moved to analysis/dataset.py. Code below should work in calling those.\n",
    "\n",
    "[OBSOLETE] Extraction is from calling dataset.py as main.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "/data1/code/python/drawmonkey\n",
      "NOTE: need to not overwrite strokes_all_task, because then the orders saved will stop being accurate. Modify\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%cd ..\n",
    "\n",
    "from tools.utils import * \n",
    "from tools.plots import *\n",
    "from tools.analy import *\n",
    "from tools.calc import *\n",
    "from tools.analyplot import *\n",
    "from tools.preprocess import *\n",
    "from tools.dayanalysis import *\n",
    "from analysis.line2 import *\n",
    "from analysis.modelexpt import *\n",
    "from analysis.probedatTaskmodel import *\n",
    "from analysis.dataset import Probedat2Dat\n",
    "\n",
    "from pythonlib.drawmodel.analysis import *\n",
    "from pythonlib.tools.stroketools import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN CODE - generate and save datasetsm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check for a given set of tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is metadat:\n",
      "{'sdate': 210606, 'edate': 210608, 'strokmodel_kind': None, 'strokmodel_tstamp': None, 'datecategories': {'210606': 1, '210607': 1, '210608': 1}, 'dates_for_summary': [], 'matchedstrokes': None, 'exptnames': ['primitives2'], 'T1': [], 'G1': [], 'G2': [], 'G3': [], 'G4': [], 'description': '', 'finalized': False, 'good_expt': False, 'expt': 'primitives2', 'task_train_test': {'probe1_liketrain': 'train', 'probe1_nostrokeconstraint': 'train', 'probe2_liketrain': 'train', 'probe2_nostrokeconstraint': 'train', 'probe3_hdpos': 'test', 'probe1': 'train', 'probe2': 'train', 'probe3': 'test', 'probe4': 'test', 'train': 'train'}}\n",
      "\n",
      "Getting these dates:\n",
      "['210606', '210607', '210608']\n",
      "Pancho 210606 primitives2 0\n",
      "- No h5 file for Pancho, 210606, primitives2, 0 - returning None!\n",
      "Pancho 210606 primitives2 1\n",
      "- No h5 file for Pancho, 210606, primitives2, 1 - returning None!\n",
      "Pancho 210606 primitives2 2\n",
      "- No h5 file for Pancho, 210606, primitives2, 2 - returning None!\n",
      "Pancho 210606 primitives2 3\n",
      "- No h5 file for Pancho, 210606, primitives2, 3 - returning None!\n",
      "Pancho 210606 primitives2 4\n",
      "- No h5 file for Pancho, 210606, primitives2, 4 - returning None!\n",
      "Pancho 210606 primitives2 5\n",
      "- No h5 file for Pancho, 210606, primitives2, 5 - returning None!\n",
      "Pancho 210606 primitives2 6\n",
      "- No h5 file for Pancho, 210606, primitives2, 6 - returning None!\n",
      "Pancho 210606 primitives2 7\n",
      "- No h5 file for Pancho, 210606, primitives2, 7 - returning None!\n",
      "Pancho 210606 primitives2 8\n",
      "- No h5 file for Pancho, 210606, primitives2, 8 - returning None!\n",
      "Pancho 210606 primitives2 9\n",
      "- No h5 file for Pancho, 210606, primitives2, 9 - returning None!\n",
      "Pancho 210607 primitives2 0\n",
      "- No h5 file for Pancho, 210607, primitives2, 0 - returning None!\n",
      "Pancho 210607 primitives2 1\n",
      "-- loaded presaved data: /data2/animals/Pancho/210607/210607_144838_primitives2_Pancho_1.pkl\n",
      "got 38 total trials\n",
      "appending fd for sess 1\n",
      "Pancho 210607 primitives2 2\n",
      "-- loaded presaved data: /data2/animals/Pancho/210607/210607_145806_primitives2_Pancho_2.pkl\n",
      "got 86 total trials\n",
      "appending fd for sess 2\n",
      "Pancho 210607 primitives2 3\n",
      "-- loaded presaved data: /data2/animals/Pancho/210607/210607_154016_primitives2_Pancho_3.pkl\n",
      "got 33 total trials\n",
      "appending fd for sess 3\n",
      "Pancho 210607 primitives2 4\n",
      "-- loaded presaved data: /data2/animals/Pancho/210607/210607_155008_primitives2_Pancho_4.pkl\n",
      "got 456 total trials\n",
      "appending fd for sess 4\n",
      "Pancho 210607 primitives2 5\n",
      "-- loaded presaved data: /data2/animals/Pancho/210607/210607_172856_primitives2_Pancho_5.pkl\n",
      "got 132 total trials\n",
      "appending fd for sess 5\n",
      "Pancho 210607 primitives2 6\n",
      "- No h5 file for Pancho, 210607, primitives2, 6 - returning None!\n",
      "Pancho 210607 primitives2 7\n",
      "- No h5 file for Pancho, 210607, primitives2, 7 - returning None!\n",
      "Pancho 210607 primitives2 8\n",
      "- No h5 file for Pancho, 210607, primitives2, 8 - returning None!\n",
      "Pancho 210607 primitives2 9\n",
      "- No h5 file for Pancho, 210607, primitives2, 9 - returning None!\n",
      "Pancho 210608 primitives2 0\n",
      "- No h5 file for Pancho, 210608, primitives2, 0 - returning None!\n",
      "Pancho 210608 primitives2 1\n",
      "-- loaded presaved data: /data2/animals/Pancho/210608/210608_151718_primitives2_Pancho_1.pkl\n",
      "got 796 total trials\n",
      "appending fd for sess 1\n",
      "Pancho 210608 primitives2 2\n",
      "- No h5 file for Pancho, 210608, primitives2, 2 - returning None!\n",
      "Pancho 210608 primitives2 3\n",
      "- No h5 file for Pancho, 210608, primitives2, 3 - returning None!\n",
      "Pancho 210608 primitives2 4\n",
      "- No h5 file for Pancho, 210608, primitives2, 4 - returning None!\n",
      "Pancho 210608 primitives2 5\n",
      "- No h5 file for Pancho, 210608, primitives2, 5 - returning None!\n",
      "Pancho 210608 primitives2 6\n",
      "- No h5 file for Pancho, 210608, primitives2, 6 - returning None!\n",
      "Pancho 210608 primitives2 7\n",
      "- No h5 file for Pancho, 210608, primitives2, 7 - returning None!\n",
      "Pancho 210608 primitives2 8\n",
      "- No h5 file for Pancho, 210608, primitives2, 8 - returning None!\n",
      "Pancho 210608 primitives2 9\n",
      "- No h5 file for Pancho, 210608, primitives2, 9 - returning None!\n",
      "===== SUMMARY\n",
      "--\n",
      "Pancho-210607-1: ntrials: 39\n",
      "\n",
      "--\n",
      "Pancho-210607-2: ntrials: 86\n",
      "\n",
      "--\n",
      "Pancho-210607-3: ntrials: 33\n",
      "\n",
      "--\n",
      "Pancho-210607-4: ntrials: 456\n",
      "\n",
      "--\n",
      "Pancho-210607-5: ntrials: 133\n",
      "\n",
      "--\n",
      "Pancho-210608-1: ntrials: 797\n",
      "\n",
      "MAKE SURE TO USE loadProbeDatWrapper\n",
      "got 38 total trials\n",
      "got 86 total trials\n",
      "got 33 total trials\n",
      "got 456 total trials\n",
      "got 132 total trials\n",
      "got 796 total trials\n",
      "0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "1200\n"
     ]
    }
   ],
   "source": [
    "# exptlist = [\"arc2\", \"lines5\"]\n",
    "SKIPS = []\n",
    "DOSKIP = False\n",
    "DO_NOT_LET_SKIP= True\n",
    "# exptlist = [\"biasdir2\", \"biasdir3\", \"biasdir4\", \"biasdir5\", \n",
    "#             \"biasdir6\", \"biasdir7_1\", \"biasdir7_2\", \"biasdir8\", \"biasdir9\"]\n",
    "# exptlist = [\"neuralprep8\", \"neuralprep5\", \"neuralprep1\", \"neuralprep2\", \"neuralprep3\", \"neuralprep4\",\n",
    "#             \"neuralprep6\", \"neuralprep7\"]\n",
    "exptlist = [\"primitives2\"]\n",
    "animallist = [\"Pancho\"]\n",
    "rulelist = [\"null\"]\n",
    "# rulelist = [None]\n",
    "# exptlist = [\"neuralprep5\"]\n",
    "for expt in exptlist: \n",
    "    for animal in animallist:\n",
    "        for rule in rulelist:\n",
    "            try:\n",
    "                FD, exptMetaDat = loadMultDataForExpt(expt,animal, metadatonly=False, rule=rule)\n",
    "                PD = loadProbeDatWrapper(FD, exptMetaDat)\n",
    "                P = ProbedatTaskmodel(PD, exptMetaDat)\n",
    "\n",
    "                extraction_params = {\n",
    "                    \"expt\":expt,\n",
    "                    \"animal\":animal,\n",
    "    #                 \"probedat_filter_params\":{\n",
    "    #                     \"hausdorff_filter\":True,\n",
    "    #                     \"hausdorff_filter_prctile\":2.5,\n",
    "    #                     },\n",
    "                    \"probedat_filter_params\":{},\n",
    "                    \"pix_add_to_sketchpad_edges\":20,\n",
    "                    \"savedir\":\"/data2/analyses/database/BEH\",\n",
    "                    \"savenote\":\"formodeling\"\n",
    "                }\n",
    "\n",
    "                # ==== filter trials based on behavioral criteria, to throw out noise.\n",
    "                ProbedatFiltered = P.filterByBehPerformance(extraction_params[\"probedat_filter_params\"])\n",
    "\n",
    "                # Reconstruct P\n",
    "                P = ProbedatTaskmodel(ProbedatFiltered, P.Metadat)\n",
    "\n",
    "                # === Convert Probedat to DAT\n",
    "                DAT, METADAT = Probedat2Dat(P, extraction_params, save=True, keep_all_in_probedat=True)\n",
    "            except:\n",
    "                if DO_NOT_LET_SKIP:\n",
    "                    raise\n",
    "                if DOSKIP:\n",
    "                    SKIPS.append([expt, animal])\n",
    "                else:\n",
    "                    raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DATASET FURTHER EXTRACTION (E..G, PARSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonlib.dataset.dataset import Dataset\n",
    "\n",
    "for animal in [\"Pancho\", \"Red\"]:\n",
    "    for expt in [\"plandir2\"]:\n",
    "        \n",
    "        #### LOAD\n",
    "        D = Dataset([])\n",
    "        D.load_dataset_helper(animal, expt)\n",
    "\n",
    "        # PARSES\n",
    "        D.planner_extract_save_all_parses() # parses\n",
    "        \n",
    "\n",
    "        # DEBUGGING\n",
    "#         Dsub = D.copy()\n",
    "#         Dsub.Dat = Dsub.Dat[:10]\n",
    "\n",
    "#         Dsub.planner_score_beh_against_all_parses() # beh-task scores\n",
    "#         Dsub.planner_score_parses() # parse scores\n",
    "\n",
    "        D.planner_score_beh_against_all_parses() # beh-task scores\n",
    "        D.planner_score_parses() # parse scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCRATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.fd(0)[\"params\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sketchpad_edges_mk = fd[\"TrialRecord\"][\"User\"][\"BlockParams\"][\"1\"][\"sketchpad\"][\"edges_monkey\"]\n",
    "pos = convertCoords(fd, sketchpad_edges_mk.T, \"monkeynorm2centeredmonkeypix\")\n",
    "sketchpad_edges_pixcentered = pos\n",
    "sketchpad_edges_pixcentered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SKIPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.generateDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonlib.tools.pytools import get_size\n",
    "\n",
    "get_size(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_size(P.Dataset)/1000)\n",
    "print(get_size(P.Probedat)/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SF = P.Dataset.flattenToStrokdat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis.strok import *\n",
    "# == Filter\n",
    "params = {\n",
    "    \"align_to_onset\":True,\n",
    "    \"min_stroke_length_percentile\":2,\n",
    "    \"min_stroke_length\":50,\n",
    "    \"max_stroke_length_percentile\":99.5,\n",
    "}\n",
    "SF = preprocessStroks(SF, params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescale_strokes_ver = \"stretch_to_1\"\n",
    "distancever = \"euclidian_diffs\"\n",
    "npts_space = 50\n",
    "Nbasis = 5\n",
    "\n",
    "# 1) Get distance matrix, entire dataset, with random instances chosen for basis\n",
    "# preprocess stroklist\n",
    "stroklist = list(SF[\"strok\"].values)\n",
    "\n",
    "# rescale\n",
    "if rescale_strokes_ver==\"stretch_to_1\":\n",
    "    stroklist = [rescaleStrokes([s])[0] for s in stroklist]\n",
    "else:\n",
    "    print(\"keeping strokes scale unchaged\")\n",
    "\n",
    "# interpolate\n",
    "if distancever in [\"euclidian\", \"euclidian_diffs\"]:\n",
    "    # then need to be same length\n",
    "    stroklist = strokesInterpolate2(stroklist, N=[\"npts\", npts_space], base=\"space\")\n",
    "\n",
    "idxs_stroklist_dat = list(range(len(stroklist)))\n",
    "idxs_stroklist_basis = random.sample(range(len(stroklist)), Nbasis)\n",
    "similarity_matrix = distMatrixStrok(idxs_stroklist_dat, idxs_stroklist_basis, stroklist=stroklist,\n",
    "                   normalize_rows=False, normalize_cols_range01=True, distancever=distancever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F(x):\n",
    "    indprobe = x[\"index_probedat\"]\n",
    "    return x[\"Probedat\"].pandas().iloc[indprobe].to_dict()\n",
    "\n",
    "from pythonlib.tools.pandastools import applyFunctionToAllRows\n",
    "\n",
    "SF = applyFunctionToAllRows(SF, F)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test things about DAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = loadSingleDataQuick(\"Red\", \"210324\", \"figures9\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd[\"TrialRecord\"][\"User\"][\"AdapterParams\"][\"50\"][\"bb\"][\"sequence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(DAT)==len(P.pandas())\n",
    "assert np.all(DAT.index == P.pandas().index)\n",
    "assert (DAT[\"unique_task_name\"] == P.pandas()[\"unique_task_name\"]).all()\n",
    "pd.merge(DAT, P.pandas(), how=\"outer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dtype('float64').itemsize\n",
    "np.dtype('float16').itemsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does it help to convert data type from float64 (double) to float32?\n",
    "# float 16 is a bit too imprecise.\n",
    "# float32 doesnt make much of a difference (try code below)\n",
    "# Therefore, dont make any change.\n",
    "\n",
    "from pythonlib.tools.pytools import get_size\n",
    "\n",
    "TYPE = \"float16\"\n",
    "print(DAT[\"origin\"][0].astype(TYPE))\n",
    "print(get_size(DAT[\"origin\"][0]))\n",
    "get_size(DAT[\"origin\"][0].astype(TYPE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAT[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONFIRMING that two datasets - DAT and DATloaded - are idencial. in places where they arent,  confirm that is different unique names due to diff hash method, or minor thing like in once case did not clean up short strokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pickle\n",
    "fname = \"/data2/analyses/database/expts/Pancho-lines5-formodeling-210309_111226/dat.pkl\"\n",
    "with open(fname, \"rb\") as f:\n",
    "    DATloaded = pickle.load(f)\n",
    "\n",
    "i = 0\n",
    "\n",
    "inds = DAT[\"unique_task_name\"].values==DATloaded[\"unique_task_name\"].values\n",
    "print(DAT[inds][i])\n",
    "print(DATloaded[inds][i])\n",
    "\n",
    "\n",
    "# [np.isclose(d1, d2) for d1, d2 in zip(DAT[\"strokes_beh\"].values, DATloaded[\"strokes_beh\"].values)]\n",
    "collect = 0\n",
    "collect_names = []\n",
    "for d1, d2 in zip(DAT[\"unique_task_name\"].values, DATloaded[\"unique_task_name\"].values):\n",
    "    if d1!=d2:\n",
    "        collect_names.append([d1, d2])\n",
    "#         print(d1)\n",
    "#         print(d2)\n",
    "#         asdfsdf\n",
    "    if len(d1) != len(d2):\n",
    "        collect+=1\n",
    "        if random.random()>0.8:\n",
    "            print(len(d1), len(d2))\n",
    "#             print(d1, d2)\n",
    "\n",
    "            fig, ax = plt.subplots()\n",
    "            plotDatStrokes(d1, ax)\n",
    "            fig, ax = plt.subplots()\n",
    "            plotDatStrokes(d2, ax)\n",
    "#         assert False\n",
    "#     try:\n",
    "#         compareTasks(d1, d2, input_strokes_directly=True)\n",
    "#     except:\n",
    "#         print(d1)\n",
    "#         print(d2)\n",
    "#         safsadf\n",
    "    \n",
    "print(\"this many cases with strokes not matching number\")\n",
    "print(collect)\n",
    "\n",
    "for c in collect_names:\n",
    "    tmp =[]\n",
    "    for cc1, cc2 in zip(c[0], c[1]):\n",
    "        tmp.append(cc1==cc2)\n",
    "    print(np.array(tmp).astype(\"float\"))\n",
    "#     print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## SCRATCH\n",
    "\n",
    "# import seaborn as sns\n",
    "# sns.catplot(data=P.pandas(), x='date')\n",
    "\n",
    "# idx = 1001\n",
    "\n",
    "# xyt = getTrialsPeanutPos(*P.fd_trial(idx))\n",
    "# strokes_beh = getTrialsStrokesByPeanuts(*P.fd_trial(idx))\n",
    "# strokes_task = getTrialsTaskAsStrokes(*P.fd_trial(idx))\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# plotDatStrokes(getTrialsTaskAsStrokes(*P.fd_trial(idx)), ax=ax)\n",
    "# plotDatStrokes(strokes_beh, ax=ax)\n",
    "\n",
    "## Throw out trials that are total garbage\n",
    "## determine threshold for hausdorff.\n",
    "# thresh_hausdorff = -0.97210514 # 2.5th percentile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BELOW, looking at data to decide good extraction params. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding good hausodrff threshold for filtering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(np.isnan(Pp[\"frac_touched\"].values))\n",
    "print(np.percentile(Pp[\"hausdorff\"].values, [1, 2.5, 5, 10]))\n",
    "print(np.percentile(Pp[\"frac_touched\"].values, [1, 5, 10]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=Pp, vars=[\"hausdorff\", \"frac_touched\"], height=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot hist of hausdorffs\n",
    "plt.figure()\n",
    "plt.hist(Pp[\"hausdorff\"].values, 100)\n",
    "plt.figure()\n",
    "plt.hist(Pp[\"frac_touched\"].values, 100)\n",
    "\n",
    "# Pull out trials with bad hausdorff\n",
    "# idxlist = np.where((Pp[\"hausdorff\"].values<-0.8) & (Pp[\"hausdorff\"].values>-1))\n",
    "# idxlist = np.where((Pp[\"hausdorff\"].values<-0.81788333) & (Pp[\"frac_touched\"].values<0.8))\n",
    "idxlist = np.where((Pp[\"hausdorff\"].values<-1.49))\n",
    "print(idxlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plto those trials\n",
    "plotTrialSingleOverview(*P.fd_trial(7121));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BELOW, validating sketchpad, finding tyrials that extend beyond, and proprotions of trials doing so.\n",
    "### Overall validates that the sketchpad (plus like 20 pix) is good\n",
    "### Some extend beyond becuase subjects keep dragging, and is counted as part of stroke due to how peanutpus is extract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find, in practice, edges of behavior\n",
    "def F(x):\n",
    "    out = np.array([\n",
    "        np.min([np.min(xx[:,0]) for xx in x[\"strokes_beh\"]]),\n",
    "        np.max([np.max(xx[:,0]) for xx in x[\"strokes_beh\"]]),\n",
    "        np.min([np.min(xx[:,1]) for xx in x[\"strokes_beh\"]]),\n",
    "        np.max([np.max(xx[:,1]) for xx in x[\"strokes_beh\"]])\n",
    "    ])\n",
    "    return out\n",
    "    \n",
    "from pythonlib.tools.pandastools import applyFunctionToAllRows\n",
    "\n",
    "# For each trial, get its min, max, for x and y\n",
    "DAT2 = applyFunctionToAllRows(DAT, F)\n",
    "\n",
    "\n",
    "\n",
    "# ==== PLOT\n",
    "beh_edges = np.stack(DAT2[\"newcol\"].values)\n",
    "\n",
    "for i in range(beh_edges.shape[1]):\n",
    "    print([np.min(beh_edges[:, i]), np.max(beh_edges[:, i])])\n",
    "\n",
    "\n",
    "## PLOT DISTRIBITUSIONS OF EDGES (BEH) ACROSS ALL TASKS.\n",
    "plt.figure()\n",
    "plt.hist(beh_edges[:,0], 100)\n",
    "plt.figure()\n",
    "plt.hist(beh_edges[:,1], 100)\n",
    "plt.figure()\n",
    "plt.hist(beh_edges[:,2], 100)\n",
    "plt.figure()\n",
    "plt.hist(beh_edges[:,3], 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sketchpad_edges_pixcentered.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Find those trials exceeding sketchpad boundaries...\n",
    "idx = np.where(beh_edges[:, 2]<-307.2)\n",
    "print(idx)\n",
    "print(beh_edges[idx, 2])\n",
    "print(\"sketchpad edges:\")\n",
    "print(sketchpad_edges_pixcentered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get proportion of trials that dont fit into the sketchpad\n",
    "\n",
    "edges = sketchpad_edges_pixcentered.reshape(-1)\n",
    "edges = [edges[0]-10, edges[1]-10, edges[2]+10, edges[3]+10]\n",
    "out = []\n",
    "for i in range(beh_edges.shape[0]):\n",
    "    b = beh_edges[i,:]\n",
    "    tmp = [\n",
    "        b[0]<edges[0], \n",
    "        b[1]>edges[2], \n",
    "        b[2]<edges[1], \n",
    "        b[3]>edges[3], \n",
    "    ]\n",
    "    out.append(any(tmp))\n",
    "\n",
    "print(f\"{np.sum(out)} / {len(out)} ({np.sum(out)/len(out):.3f}) not fitting fully into sketchpad\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## To example a particular trial (by plotting, checking video, etc)\n",
    "ind = 4516\n",
    "\n",
    "# find details of this trial\n",
    "P.pandas().iloc[ind]\n",
    "\n",
    "# Plot behavior on this trial.\n",
    "plotTrialSingleOverview(*P.fd_trial(ind));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
