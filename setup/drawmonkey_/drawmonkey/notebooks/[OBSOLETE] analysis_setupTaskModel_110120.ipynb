{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Setting up analysis for model comparison, where each trial/task is a datapoiont\n",
    "(differs from strok model where each action is datapoint)\n",
    "- This is mostly setup, but some plotting analysis code.\n",
    "- Mostly has been moved to analysis in other notebooks.\n",
    "[OBSOLETE] I think (7/11/21)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.utils import * \n",
    "from tools.plots import *\n",
    "from tools.analy import *\n",
    "from tools.calc import *\n",
    "from tools.analyplot import *\n",
    "from tools.preprocess import *\n",
    "from tools.dayanalysis import *\n",
    "\n",
    "from pythonlib.drawmodel.analysis import *\n",
    "from pythonlib.tools.stroketools import *\n",
    "from pythonlib.drawmodel.strokedists import *\n",
    "from pythonlib.analysis.probedatTaskmodel import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "animal = \"Red\"\n",
    "expt = \"lines5\"\n",
    "# date = 200924\n",
    "date = 200929\n",
    "session = 1\n",
    "\n",
    "fd = loadSingleData(animal, date, expt, session, resave_as_dict=False, load_resaved_data=True, \n",
    "                          resave_overwrite=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper to partition trials into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Load filedata across days\n",
    "from analysis.modelexpt import loadMultDataForExpt\n",
    "from analysis.modelexpt import *\n",
    "    \n",
    "def _loadanimal(a):\n",
    "\n",
    "    expt=\"lines5\"\n",
    "    FD, MD = loadMultDataForExpt(expt, a, \"all\")\n",
    "\n",
    "    # 2) Convert to probedat\n",
    "    PROBEDAT = loadProbeDatWrapper(FD, MD)\n",
    "    \n",
    "    return PROBEDAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitter(probedat, params):\n",
    "    \"\"\" takes a entire PROBEDAT list and outputs list of [\"train\", \"test\", ...]\n",
    "    \"\"\"\n",
    "    # Random split\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    probedat_train, probedat_test = train_test_split(probedat, test_size=params[\"test_size\"])\n",
    "    \n",
    "    return probedat_train, probedat_test\n",
    "\n",
    "# OBSOLETE THIS IS IN PROBEDATTASK CLASSS\n",
    "# def pd2strokes(probedat):\n",
    "#     strokes, tasks, fix, strokes_task = [[], [], [], []]\n",
    "#     for p in probedat:\n",
    "#         fd = p[\"filedata\"]\n",
    "#         t = p[\"trial\"]\n",
    "#         S = getTrialsStrokesByPeanuts(fd, t)\n",
    "#         if len(S)>0:\n",
    "#             strokes.append(S)\n",
    "#             tasks.append(getTrialsTask(fd, t))\n",
    "#             fix.append(getTrialsFix(fd, t)[\"fixpos_pixels\"])\n",
    "#             strokes_task.append(getTrialsTaskAsStrokes(fd, t))\n",
    "#     for s, t, f in zip(strokes_task, tasks, fix):\n",
    "#         t[\"strokes\"] = s\n",
    "#         t[\"fixpos\"] = f\n",
    "#     return strokes, tasks\n",
    "\n",
    "        \n",
    "from pythonlib.drawmodel.analysis import *\n",
    "\n",
    "# OBSOLETE -T HIS IS IN THE PROBEDATTASK CALSS\n",
    "# def _make(probedat_train, priorver=\"distance_travel\", parse_ver=\"permutations\", \n",
    "#          chunkmodel = None, name=\"test\", posterior_ver=\"weighted\"):\n",
    "    \n",
    "#     # ---1) Build model\n",
    "#     PARAMS = {\n",
    "#         \"getPriors\":{\"normscore\":[0.01, True]},\n",
    "#         \"getLikelis\":None,\n",
    "#         \"getPosteriors\":None,\n",
    "#         \"parse\":{\"parses_split\":False, \"parses_bothdir\":False},\n",
    "#         \"posterior_ver\":posterior_ver,\n",
    "#         \"prior_norm_ver\":\"softmax\",\n",
    "#         \"standardize\":{\"standardize_strokes\":False},\n",
    "#         \"run_only_once\":['parse', 'prior_score', 'getLikelis']\n",
    "#     }\n",
    "\n",
    "#     # 3) Prepare model\n",
    "#     PARAMS_MODEL = {\n",
    "#         \"use_presaved_parses\":False,\n",
    "#         \"priorver\":priorver,\n",
    "#         \"likeliver\":\"segments\",\n",
    "#         \"parse_ver\":parse_ver,\n",
    "#         \"chunkmodel\":chunkmodel,\n",
    "#         \"name\":name\n",
    "#     }\n",
    "\n",
    "#     strokes, tasks = pd2strokes(probedat_train)\n",
    "#     mod = Model(PARAMS_MODEL)\n",
    "#     data = Dataset(strokes, tasks, PARAMS=PARAMS)\n",
    "#     data.applyModel(mod)\n",
    "    \n",
    "#     return data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBSOLETE - this is in taskmodel.py\n",
    "def parsefun(task, threshdist=\"adaptive\", returnkeeplist=False):\n",
    "    \"\"\" get all partitions that have at least one\n",
    "    chunk of 2 strokes or more, and no chunks of\n",
    "    3 strokes or more.\n",
    "    - only chunks strokes if they are close\"\"\"\n",
    "    from pythonlib.drawmodel.strokedists import distmatStrokes    \n",
    "    \n",
    "    # -- get stroke indices\n",
    "    strokes_task = convertTask2Strokes(task)\n",
    "    tmp = [i for i in range(len(strokes_task))]\n",
    "    \n",
    "    # -- threshdist make mean of stroke lengths\n",
    "    if threshdist==\"adaptive\":\n",
    "        td = 0.75*np.mean(strokeDistances(strokes_task))\n",
    "    else:\n",
    "        td = threshdist\n",
    "\n",
    "    # -- get all partitions.\n",
    "    from pythonlib.tools.listtools import partition\n",
    "    A = list(partition(tmp))\n",
    "\n",
    "    # -- keep only partitions that have at least one multi-stroke chunk\n",
    "    A = [a for a in A if len(a)<len(tmp)]\n",
    "    # -- dont keep if have a chunk with >2 strokes chunked\n",
    "    def tmp(x):\n",
    "        return [len(xx)>2 for xx in x]\n",
    "    A = [a for a in A if not any(tmp(a))]\n",
    "    \n",
    "    # -- check each chunk proximitiy - only keep if close.\n",
    "    Anew = []\n",
    "    keeplist = []\n",
    "    for a in A:\n",
    "        keep = True\n",
    "        for chunk in a:\n",
    "            if len(chunk)>1:\n",
    "                # check if strokes in this chunk are close\n",
    "                # for all pairs of strok, compute min dist.\n",
    "                # throw out chunk if any pair has dist greater than some thresh\n",
    "                strokes1 = [strokes_task[n] for n in chunk]\n",
    "                d = distscalarStrokes(strokes1, strokes1, ver=\"mindist_offdiag\")\n",
    "                if d>td:\n",
    "                    keep=False\n",
    "#                     f, ax = plt.subplots()\n",
    "#                     plotDatStrokes(strokes1, ax)\n",
    "#                     assert False\n",
    "\n",
    "        if keep:\n",
    "            Anew.append(a)\n",
    "        keeplist.append(keep)\n",
    "    A = Anew\n",
    "    \n",
    "    if returnkeeplist:\n",
    "        return A, keeplist\n",
    "    else:\n",
    "        return A\n",
    "\n",
    "\n",
    "allkeeps =[]\n",
    "for P in probedat:\n",
    "    task = getTrialsTask(P[\"filedata\"], P[\"trial\"])\n",
    "    allkeeps.extend(parsefun(task, threshdist=\"adaptive\", returnkeeplist=True)[1])\n",
    "print(\"frac chunks kept after thresholding\")\n",
    "print(sum(allkeeps)/len(allkeeps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPERIMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try idfferent models (train test split) on same datset. \n",
    "# This is using old version before made  ProbedatTaskmodel class.\n",
    "# Should update this to use that class. should be easey.\n",
    "# each model/data run should be one class instance, need to update\n",
    "# to transfer params to test on test tasks...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = []\n",
    "Nsplits = 20\n",
    "from pythonlib.tools.dicttools import filterDict\n",
    "from pythonlib.tools.modfittools import minimize\n",
    "\n",
    "for animal in [\"Pancho\", \"Red\"]:\n",
    "    \n",
    "    PROBEDAT = _loadanimal(animal)\n",
    "    \n",
    "    for epoch in [1,2]:\n",
    "        partition = {\n",
    "            \"datagetter\":{\n",
    "                \"taskgroup\":\"train_fixed\",\n",
    "                \"epoch\":epoch,\n",
    "                \"insummarydates\":True\n",
    "            },\n",
    "            \"splitter\":splitter,\n",
    "            \"splitparams\":{\n",
    "                \"test_size\":0.1\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # 1) get data pool\n",
    "        probedat = filterDict(PROBEDAT, partition[\"datagetter\"])\n",
    "\n",
    "        for n in range(Nsplits):\n",
    "            probedat_train, probedat_test = splitter(probedat, partition[\"splitparams\"])\n",
    "\n",
    "            for priorver, parse_ver, chunkmodel, name in zip(\n",
    "                [\"distance_travel\", \"uniform\", \"distance_travel\", \"distance_travel\",\"uniform\",\"uniform\"], \n",
    "                [\"permutations\", \"permutations\", \"chunks\", \"chunks\", \"chunks\", \"chunks\"],\n",
    "                [None, None, \"3line\", parsefun, \"3line\", parsefun],\n",
    "                [\"distance_travel\", \"null\", \"3line-dist\", \"linePlusL-dist\", \"3line\", \"linePlusL\"]):\n",
    "\n",
    "                # 1) make dataset/model\n",
    "                data = _make(probedat_train, priorver, parse_ver, chunkmodel, name)\n",
    "\n",
    "                # 2) optimize\n",
    "                params0 = [0.1]\n",
    "                bounds = [(0,1)]\n",
    "                def fun(params):\n",
    "                    data.PARAMS[\"getPriors\"][\"normscore\"][0] = params[0]\n",
    "                    data.run()\n",
    "                    return -np.mean(data.summarizeScore()[\"post_scores\"])\n",
    "                minimize(fun, params0, bounds)\n",
    "\n",
    "                # 3) apply to test set\n",
    "                strokes, tasks = pd2strokes(probedat_test)\n",
    "                datatest = transferParams(data, strokes, tasks)\n",
    "\n",
    "                # 4) collect outputs\n",
    "                out.append({\n",
    "                    \"animal\":animal,\n",
    "                    \"epoch\":epoch,\n",
    "                    \"splitn\":n,\n",
    "                    \"priorver\":priorver,\n",
    "                    \"parse_ver\":parse_ver, \n",
    "                    \"chunkmodel\":chunkmodel,\n",
    "                    \"name\":name,\n",
    "                    \"postscores_train\":data.summarizeScore()[\"post_scores\"],\n",
    "                    \"postscores_test\":datatest.summarizeScore()[\"post_scores\"],\n",
    "                    \"PARAMS_fit\": data.PARAMS[\"getPriors\"][\"normscore\"][0]\n",
    "                })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for o in out:\n",
    "    o[\"score_train\"] = np.mean(o[\"postscores_train\"])\n",
    "    o[\"score_test\"] = np.mean(o[\"postscores_test\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ======= SAVE\n",
    "from pythonlib.tools.expttools import makeTimeStamp\n",
    "\n",
    "expt =\"lines5\"\n",
    "SAVEDIR = f\"{PROBEDAT[0]['filedata']['params']['figuredir_notebook']}/analysis_setuptaskmodel/{expt}/{makeTimeStamp('modelcomp', False)}\"\n",
    "import os\n",
    "os.makedirs(SAVEDIR, exist_ok=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "OUT = pd.DataFrame(out)\n",
    "\n",
    "# OUT\n",
    "fig = sns.catplot(data=OUT, x=\"name\", row=\"animal\", y=\"score_train\", hue=\"epoch\", aspect=3)\n",
    "fig.savefig(f\"{SAVEDIR}/scores_train.pdf\")\n",
    "fig = sns.catplot(data=OUT, x=\"name\", row=\"animal\", y=\"score_test\", hue=\"epoch\", aspect=3)\n",
    "fig.savefig(f\"{SAVEDIR}/scores_test.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sns.catplot(data=OUT, x=\"name\", row=\"animal\", y=\"PARAMS_fit\", hue=\"epoch\", aspect=3)\n",
    "fig.savefig(f\"{SAVEDIR}/params_fit.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## = SAVE\n",
    "import pickle\n",
    "with open(f\"{SAVEDIR}/out.pkl\", \"wb\") as f:\n",
    "    pickle.dump(out, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build multiple models - score each task by comparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OLSOLETE - this is all folded in to probedatOfflineScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# expt=\"lines5\"\n",
    "# for animal in [\"Pancho\", \"Red\"]:    \n",
    "#     PROBEDAT = _loadanimal(animal)\n",
    "\n",
    "#     SAVEDIR = f\"{PROBEDAT[0]['filedata']['params']['figuredir_notebook']}/analysis_setuptaskmodel/{expt}/{makeTimeStamp('modelcomp_rescoring', False)}\"\n",
    "#     import os\n",
    "#     os.makedirs(SAVEDIR, exist_ok=True)\n",
    "#     for taskgroup in [\"train_fixed\", \"G2\", \"G3\"]:\n",
    "#         partition = {\n",
    "#             \"datagetter\":{\n",
    "#                 \"taskgroup\":taskgroup,\n",
    "#                 \"insummarydates\":True\n",
    "#             },\n",
    "#             \"splitter\":splitter,\n",
    "#             \"splitparams\":{\n",
    "#                 \"test_size\":0.1\n",
    "#             }\n",
    "#         }\n",
    "\n",
    "#         # 1) get data pool\n",
    "#         probedat = filterDict(PROBEDAT, partition[\"datagetter\"])\n",
    "#         DATAS = {}\n",
    "        \n",
    "#         for priorver, parse_ver, chunkmodel, posterior_ver, name in zip(\n",
    "#             [\"distance_travel\",\"uniform\",\"uniform\"], \n",
    "#             [\"permutations\", \"chunks\", \"chunks\"],\n",
    "#             [None, \"3line\", parsefun],\n",
    "#             [\"weighted\", \"maxlikeli\", \"maxlikeli\"],\n",
    "#             [\"distance_travel\", \"3line\", \"linePlusL\"]):\n",
    "\n",
    "#             # 1) make dataset/model\n",
    "#             data = _make(probedat, priorver, parse_ver, chunkmodel, name, posterior_ver=posterior_ver)\n",
    "\n",
    "#             # 2) optimize\n",
    "#             params0 = [0.1]\n",
    "#             bounds = [(0,1)]\n",
    "#             def fun(params):\n",
    "#                 data.PARAMS[\"getPriors\"][\"normscore\"][0] = params[0]\n",
    "#                 data.run()\n",
    "#                 return -np.mean(data.summarizeScore()[\"post_scores\"])\n",
    "#             minimize(fun, params0, bounds)\n",
    "\n",
    "#             # save\n",
    "#             DATAS[name] = data\n",
    "        \n",
    "#         for D in DATAS.values():\n",
    "#             assert len(D.trials)==len(probedat)\n",
    "            \n",
    "#         ## reassign model scores \n",
    "#         modelscores = []\n",
    "#         for i, p in enumerate(probedat):\n",
    "#             modelscores.append({})\n",
    "#             modelscores[-1][\"online_modelcomp\"] = p[\"modelcomp\"]\n",
    "#             modelscores[-1][\"epoch\"] = p[\"epoch\"]\n",
    "#             for name, data in DATAS.items():\n",
    "#                 modelscores[-1][name] = data.trials[i][\"posterior\"]\n",
    "#             modelscores[-1][\"offline_modelcomp\"] = 2*(modelscores[-1][\"3line\"]/(modelscores[-1][\"3line\"] + modelscores[-1][\"linePlusL\"])-0.5)\n",
    "\n",
    "#         MS = pd.DataFrame(modelscores)\n",
    "#         fig = sns.pairplot(data=MS, vars = [\"online_modelcomp\", \"offline_modelcomp\", \"distance_travel\", \"3line\", \"linePlusL\"], hue=\"epoch\")\n",
    "#         fig.savefig(f\"{SAVEDIR}/overview_pairplot-{animal}-{taskgroup}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([p for p in probedat if len(getTrialsStrokesByPeanuts(p[\"filedata\"], p[\"trial\"]))>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        MS = pd.DataFrame(modelscores)\n",
    "        fig = sns.pairplot(data=MS, vars = [\"online_modelcomp\", \"offline_modelcomp\", \"distance_travel\", \"3line\", \"linePlusL\"], hue=\"epoch\")\n",
    "        fig.savefig(f\"{SAVEDIR}/overview_pairplot-{animal}-{taskgroup}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stroke2features(strokes, features=[\"center\"]):\n",
    "    \"\"\" convert strokes (list of rray)\n",
    "    to list of scalar values for features\n",
    "    of interest\n",
    "    \"\"\"\n",
    "    featdict = []\n",
    "    for s in strokes:\n",
    "        featdict = \n",
    "        for f in features:\n",
    "            if f==\"center\":\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reassigning scores to probedat\n",
    "#### also trying new class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis.probedatTaskmodel import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROBEDAT = _loadanimal(\"Red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## [IMPORTANT] This is function tthat upadtes probedat correctly.\n",
    "probedat, fig = probedatOfflineScore(probedat, filtdict={\"random_task\":[False]},\n",
    "                        ploton=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize tasks, sorted by model scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract model scores.\n",
    "probedat, PDdict = probedatOfflineScore(probedat, filtdict={\"random_task\":[False]},\n",
    "                        ploton=True, return_all_models=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 2\n",
    "for name, P in PDdict.items():\n",
    "    P.Datamodel.plotExampleTrial(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelis = [p[\"likeli\"] for p in PDdict[\"distance_travel\"].Datamodel.trials[1][\"model_parses\"]]\n",
    "\n",
    "np.argmax(likelis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonlib.tools.expttools import makeTimeStamp\n",
    "expt =\"lines5\"\n",
    "SAVEDIR = f\"{PROBEDAT[0]['filedata']['params']['figuredir_notebook']}/analysis_setuptaskmodel/{expt}/{makeTimeStamp('modelcomp', False)}\"\n",
    "import os\n",
    "os.makedirs(SAVEDIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "from pythonlib.tools.plottools import annotate, colorGradient\n",
    "\n",
    "# 1) Get list of all scores\n",
    "for i, p in enumerate(probedat):\n",
    "    scores.append({\n",
    "        \"modelcomp_offline\":p[\"modelcomp_offline\"],\n",
    "    \"trial\":i})\n",
    "    for name, PD in PDdict.items():\n",
    "        scores[-1][name] = PD.Datamodel.trials[i][\"posterior\"]\n",
    "\n",
    "# 2) for each trials assign bin based on percentile\n",
    "klist = scores[0].keys()\n",
    "N = 10\n",
    "scores_binned = {}\n",
    "for k in klist:\n",
    "    vals = [s[k] for s in scores]\n",
    "    if k==\"trial\":\n",
    "        scores_binned[k] = vals\n",
    "    else:\n",
    "        bins = np.percentile(vals, np.linspace(0, 100, N))\n",
    "#         bins = np.linspace(min(vals), max(vals), N)\n",
    "        vals_binned = np.digitize(vals, bins)\n",
    "\n",
    "        knew = f\"{k}_binned\"\n",
    "        scores_binned[knew] = vals_binned\n",
    "\n",
    "# 3) Make gridplot\n",
    "xfeat = \"3line_binned\"\n",
    "yfeat = \"linePlusL_binned\"\n",
    "\n",
    "xvals = sorted(list(set(scores_binned[xfeat])))\n",
    "yvals = sorted(list(set(scores_binned[yfeat])))\n",
    "\n",
    "def f1(t, ax):\n",
    "    PD = PDdict[\"3line\"] # doesnt matter which, since behavior isa ll same.\n",
    "    PD.Datamodel.plotTrialStrokes(t, ax, ver=\"beh\")\n",
    "def f2(t, ax):\n",
    "    PDdict[\"3line\"].Datamodel.plotTrialStrokes(t, ax, ver=\"task_maxlikeli\")\n",
    "def f3(t, ax):\n",
    "    PDdict[\"linePlusL\"].Datamodel.plotTrialStrokes(t, ax, ver=\"task_maxlikeli\")\n",
    "plotfunlist = [f1, f2, f3]\n",
    "\n",
    "for n in range(10): # diff examples\n",
    "    figlist =[]\n",
    "    axeslist =[]\n",
    "    for _ in range(len(plotfunlist)):\n",
    "        fig, axes = plt.subplots(ncols=len(xvals), nrows=len(yvals), sharex=True, sharey=True, \n",
    "                                 figsize=(3*len(xvals), 3*len(yvals)))\n",
    "        figlist.append(fig)\n",
    "        axeslist.append(axes)\n",
    "\n",
    "    for i, x in enumerate(xvals):\n",
    "        for j, y in enumerate(yvals):\n",
    "\n",
    "            # pick a random trials that matches these bins\n",
    "            idx = np.logical_and(scores_binned[xfeat]==x, scores_binned[yfeat]==y)\n",
    "            idx = np.where(idx)[0]\n",
    "            trials = [scores_binned[\"trial\"][i] for i in idx]\n",
    "\n",
    "            if len(trials)>0:\n",
    "                t = random.sample(trials,1)[0]\n",
    "\n",
    "                for plotfun, axes in zip(plotfunlist, axeslist):\n",
    "\n",
    "                    ax = axes[j][i]\n",
    "                    plotfun(t, ax)\n",
    "\n",
    "                    # -- label the edge panels\n",
    "                    if j==len(yvals)-1:\n",
    "                        ax.set_xlabel(f\"{xfeat}:{x}\")\n",
    "                    if i==0:\n",
    "                        ax.set_ylabel(f\"{yfeat}:{y}\")\n",
    "\n",
    "                    mc = [s[\"modelcomp_offline\"] for s in scores if s[\"trial\"]==t][0]\n",
    "                    m1 = [s[\"3line\"] for s in scores if s[\"trial\"]==t][0]\n",
    "                    m2 = [s[\"linePlusL\"] for s in scores if s[\"trial\"]==t][0]\n",
    "\n",
    "                    # -- annotate with model scores\n",
    "                    col = colorGradient((0.5+mc/2), col1=[0,0,1], col2=[1,0,0])\n",
    "                    ax.set_title(f\"sc{mc:.2f}|3l{m1:.2f}|lL{m2:.2f}\", color=col)\n",
    "\n",
    "\n",
    "    # save \n",
    "    for i, f in enumerate(figlist):\n",
    "        f.savefig(f\"{SAVEDIR}/overviewgrid-{expt}-3lineVslinePlusL-run{n}-{i}.pdf\")    \n",
    "    plt.close(\"all\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figlist[0].savefig(["
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.logical_and(scores_binned[\"modelcomp_offline_binned\"]==6, scores_binned[yfeat]==8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_binned[\"modelcomp_offline_binned\"]==6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where((scores_binned[xfeat]==x) & (scores_binned[yfeat]==y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where((scores_binned[xfeat]==x) & (scores_binned[yfeat]==y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scores_binned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOT EXAMPLE TRAILS ACROSS DISTRIBUTION OF SCORES\n",
    "\n",
    "# for each trial plot behavior + best parse for all models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## [BELOW] trying out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepDat(fd, trials):\n",
    "#     trials = [t for t in trials if getTrialsFixationSuccess(fd, t)]\n",
    "    strokes, tasks, fix, strokes_task = [[], [], [], []]\n",
    "    for t in trials:\n",
    "        S = getTrialsStrokesByPeanuts(fd, t)\n",
    "        if len(S)>0:\n",
    "#             trials = [t for t in trials if len(getTrialsStrokesByPeanuts(fd, t))>0]\n",
    "            strokes.append(S)\n",
    "            tasks.append(getTrialsTask(fd, t))\n",
    "            fix.append(getTrialsFix(fd, t)[\"fixpos_pixels\"])\n",
    "            strokes_task.append(getTrialsTaskAsStrokes(fd, t))\n",
    "    for s, t, f in zip(strokes_task, tasks, fix):\n",
    "        t[\"strokes\"] = s\n",
    "        t[\"fixpos\"] = f\n",
    "    return strokes, tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonlib.drawmodel.analysis import *\n",
    "\n",
    "# 1) Prepare data\n",
    "trials = getIndsTrials(fd)\n",
    "trials = [t for t in trials if not getTrialsTaskProbeKind(fd, t)==\"train\"]\n",
    "strokes, tasks = prepDat(fd, trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # 2) Input dataset\n",
    "# data = Dataset(strokes, tasks)\n",
    "\n",
    "# # 3) Prepare model\n",
    "# PRIORVER = \"uniform\" # CAHNGE THIS\n",
    "# LIKELIVER = \"segments\"\n",
    "# MODELNAME = \"test\"\n",
    "# priorFunction, NORM_VER = makePriorFunction(ver=PRIORVER)\n",
    "# likeliFunction = makeLikeliFunction(ver=LIKELIVER)\n",
    "# mod = Model(MODELNAME, priorFunction, likeliFunction)\n",
    "\n",
    "# # 4) apply model to score all beahvuior\n",
    "# POSTERIOR_VER = \"weighted\"\n",
    "# data.applyModel(mod, prior_ver= NORM_VER, posterior_ver=POSTERIOR_VER)\n",
    "\n",
    "# # 5) Plot posterior scores\n",
    "# data.plotPosteriorHist()\n",
    "# # plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# task = data1.trials[0][\"task\"]\n",
    "# parsefun(task)\n",
    "\n",
    "# from tools.tasks import task2parses\n",
    "# P = task2parses(task, parsefun)\n",
    "# len(P)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ==== KEY: comparison between two models.\n",
    "\n",
    "def fit(modelstr):\n",
    "    # Input dataset\n",
    "    data = Dataset(strokes, tasks)\n",
    "\n",
    "    # 3) Prepare model\n",
    "    PRIORVER = \"uniform\"\n",
    "    LIKELIVER = \"segments\"\n",
    "    MODELNAME = modelstr\n",
    "    priorFunction, NORM_VER = makePriorFunction(ver=PRIORVER)\n",
    "    likeliFunction = makeLikeliFunction(ver=LIKELIVER)\n",
    "    mod = Model(MODELNAME, priorFunction, likeliFunction, parse_ver=\"chunks\", chunkmodel = modelstr)\n",
    "\n",
    "    # 4) apply model to score all beahvuior\n",
    "    POSTERIOR_VER = \"maxlikeli\"\n",
    "    data.applyModel(mod, prior_ver= NORM_VER, posterior_ver=POSTERIOR_VER, parses_bothdir=False)\n",
    "\n",
    "    if False:\n",
    "        # 5) Plot posterior scores\n",
    "        data.plotPosteriorHist()\n",
    "        # plt.title(title)\n",
    "        \n",
    "    return data\n",
    "\n",
    "\n",
    "# 1) get two dataset instaences (same data, diff model)\n",
    "# data1 = fit(\"linePlusL\")\n",
    "data1 = fit(parsefun)\n",
    "data2 = fit(\"3line\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === NULL MODELS\n",
    "\n",
    "# 1) MINIMIZE TOTAL TRAVELED DISTANCE\n",
    "datanull = Dataset(strokes, tasks)\n",
    "\n",
    "# 3) Prepare model\n",
    "PRIORVER = \"prox_to_origin\"\n",
    "LIKELIVER = \"segments\"\n",
    "MODELNAME = PRIORVER\n",
    "priorFunction, NORM_VER = makePriorFunction(ver=PRIORVER)\n",
    "likeliFunction = makeLikeliFunction(ver=LIKELIVER)\n",
    "mod = Model(MODELNAME, priorFunction, likeliFunction, parse_ver=\"permutations\")\n",
    "\n",
    "# 4) apply model to score all beahvuior\n",
    "POSTERIOR_VER = \"weighted\"\n",
    "datanull.applyModel(mod, prior_ver= NORM_VER, posterior_ver=POSTERIOR_VER, parses_bothdir=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.plotPosteriorHist()\n",
    "data2.plotPosteriorHist()\n",
    "datanull.plotPosteriorHist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "x = np.array([1, 5, 3, 2])\n",
    "x = x-100\n",
    "beta = 0.01\n",
    "x = beta * x\n",
    "softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 2) for each trial, compare scores.\n",
    "t = random.randrange(0, len(tasks))\n",
    "\n",
    "data1.plotExampleTrial(t, sort_by_likeli=True)\n",
    "data2.plotExampleTrial(t, sort_by_likeli=True)\n",
    "datanull.plotExampleTrial(t, sort_by_likeli=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit model to training data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    \"getPriors\":{\"normscore\":[0.001]},\n",
    "    \"getLikelis\":None,\n",
    "    \"getPosteriors\":None,\n",
    "    \"parse\":{\"parses_split\":False, \"parses_bothdir\":False},\n",
    "    \"posterior_ver\":\"weighted\",\n",
    "    \"prior_norm_ver\":\"softmax\",\n",
    "    \"standardize\":{\"standardize_strokes\":False},\n",
    "    \"run_only_once\":['parse', 'prior_score', 'getLikelis']\n",
    "}\n",
    "\n",
    "# 3) Prepare model\n",
    "PARAMS_MODEL = {\n",
    "    \"priorver\":\"uniform\",\n",
    "    \"likeliver\":\"segments\",\n",
    "    \"parse_ver\":\"permutations\"\n",
    "}\n",
    "PARAMS_MODEL[\"modelname\"]=\"test\"\n",
    "def getmodel(PARAMS_MODEL):\n",
    "    priorFunction, NORM_VER = makePriorFunction(ver=PARAMS_MODEL[\"priorver\"])\n",
    "    likeliFunction = makeLikeliFunction(ver=PARAMS_MODEL[\"likeliver\"])\n",
    "    mod = Model(PARAMS_MODEL[\"modelname\"], priorFunction, likeliFunction,\n",
    "                parse_ver=PARAMS_MODEL[\"parse_ver\"])\n",
    "    return mod\n",
    "\n",
    "# 4) apply model to score all beahvuior\n",
    "datanull = Dataset(strokes, tasks, PARAMS=PARAMS)\n",
    "datanull.applyModel(getmodel(PARAMS_MODEL))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out =[]\n",
    "for x in np.linspace(-5, 0, 10):\n",
    "    beta = 1*10**x\n",
    "    datanull.PARAMS[\"getPriors\"][\"normscore\"] = [beta]\n",
    "    datanull.run()\n",
    "    posteriors = [t[\"posterior\"] for t in datanull.trials]\n",
    "    \n",
    "    for post in posteriors:\n",
    "        out.append({\n",
    "            \"beta\":beta,\n",
    "            \"posts\":post})\n",
    "                        \n",
    "                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "DF = pd.DataFrame(out)\n",
    "sns.catplot(data=DF, x=\"beta\", y=\"posts\", aspect=4)\n",
    "sns.catplot(data=DF, x=\"beta\", y=\"posts\", aspect=4, kind=\"point\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train and test - cross validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimization code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datanull.trials[0][\"posterior\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = random.randrange(0, len(tasks))\n",
    "datanull.plotExampleTrial(t, sort_by_likeli=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.plotExampleTrial(t, sort_by_likeli=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ==\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.plotPosteriorHist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strokes_beh = data2.trials[t][\"behavior\"][\"strokes\"]\n",
    "strokes_task = data2.trials[t][\"model_parses\"][0][\"strokes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2)\n",
    "plotDatStrokes(strokes_beh, ax=axes[0])\n",
    "plotDatStrokes(strokes_task, ax=axes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distanceDTW(strokes_beh, strokes_task, ver=\"segments\", asymmetric=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distanceDTW(strokes_beh, strokes_task, ver=\"segments\", asymmetric=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTES:\n",
    "Softmax is hacky right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run(prior_ver, likeli_ver=\"split_segments\", posterior_ver=\"weighted\", title=\"\"):\n",
    "    dset = makeDataset(filedata, trials_list)\n",
    "    priorFunction, NORM_VER = makePriorFunction(ver=prior_ver)\n",
    "    likeliFunction = makeLikeliFunction(ver=likeli_ver)\n",
    "    mod = Model(modelname, priorFunction, likeliFunction)\n",
    "    dset.applyModel(mod, prior_ver= NORM_VER, posterior_ver=posterior_ver)\n",
    "    dset.plotPosteriorHist()\n",
    "    plt.title(title)\n",
    "    return dset\n",
    "\n",
    "# 1) random model\n",
    "_run(\"uniform\", \"split_segments\", \"weighted\", \"uniform\")\n",
    "\n",
    "# 2) positive control model [max likelis]\n",
    "_run(\"uniform\", \"split_segments\", \"maxlikeli\", \"maxlikeli\")\n",
    "\n",
    "\n",
    "# 3) positive control model [weigh likelis by softmax(likelis)\n",
    "_run(\"uniform\", \"split_segments\", \"likeli_weighted\", \"likeli_weighted\")\n",
    "\n",
    "\n",
    "# 4) Hypothesis: distance finger travel\n",
    "dset = _run(\"distance_travel\", title=\"distance_travel\")\n",
    "\n",
    "# 5) Hypothesis, angle bias\n",
    "dset = _run(\"angle_test\", title=\"angle_test\")\n",
    "\n",
    "dset.plotExampleTrial(3)\n",
    "\n",
    "# 5) Hypothesis, angle bias\n",
    "dset = _run(\"angle_test\", title=\"angle_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonlib.drawmodel.analysis import *\n",
    "\n",
    "trials = range(1,100)\n",
    "\n",
    "strokes = [getTrialsStrokesByPeanuts(fd, t) for t in trials if getTrialsFixationSuccess(fd,t)]\n",
    "\n",
    "tasks = [getTrialsTask(fd, t) for t in trials if getTrialsFixationSuccess(fd,t)]\n",
    "fix = [getTrialsFix(fd, t)[\"fixpos_pixels\"] for t in trials if getTrialsFixationSuccess(fd,t)]\n",
    "strokes_task = [getTrialsTaskAsStrokes(fd, t) for t in trials if getTrialsFixationSuccess(fd,t)]\n",
    "for s, t, f in zip(strokes_task, tasks, fix):\n",
    "    t[\"strokes\"] = s\n",
    "    t[\"fixpos\"] = f\n",
    "    \n",
    "dset = Dataset(strokes, tasks)\n",
    "\n",
    "# === APPLY MODEL\n",
    "PRIORVER = \"uniform\" # CAHNGE THIS\n",
    "LIKELIVER = \"segments\"\n",
    "MODELNAME = \"test\"\n",
    "priorFunction, NORM_VER = makePriorFunction(ver=PRIORVER)\n",
    "likeliFunction = makeLikeliFunction(ver=LIKELIVER)\n",
    "mod = Model(MODELNAME, priorFunction, likeliFunction)\n",
    "\n",
    "# === apply model to score all beahvuior\n",
    "POSTERIOR_VER = \"maxlikeli\"\n",
    "dset.applyModel(mod, prior_ver= NORM_VER, posterior_ver=POSTERIOR_VER)\n",
    "dset.plotPosteriorHist()\n",
    "plt.title(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# == make prior function that allows only things chunkable.\n",
    "# so is uniform, based on chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset.model.parses_both_direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEVELOPMENT - MAIN ANALYSES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BELOW - OLD SCRATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LOAD ALL SESSIONS FOR A DAY\n",
    "# A = [(\"lines1\", \"Red\", 200831),\n",
    "#  (\"lines1\", \"Pancho\", 200831),\n",
    "#  (\"ESC1\", \"Red\", 200830),\n",
    "#  (\"ESC1\", \"Pancho\", 200830)]\n",
    "A = [(\"lines2\", \"Red\", 200907),\n",
    " (\"lines2\", \"Pancho\", 200907)]\n",
    "\n",
    "for (expt, animal, date) in A:\n",
    "\n",
    "    # expt = \"lines1\"\n",
    "    # animal = \"Red\"\n",
    "    # date = 200831\n",
    "\n",
    "    # expt = \"ESC1\"\n",
    "    # animal = \"Red\"\n",
    "    # date = 200830\n",
    "\n",
    "    # simple, just try many sessions...\n",
    "    fdsessions = []\n",
    "    FD = {}\n",
    "    N = 12\n",
    "    for session in range(10):\n",
    "        fd = loadSingleData(animal, date, expt, session, resave_as_dict=False, load_resaved_data=True, \n",
    "                          resave_overwrite=False)\n",
    "        if fd is not None:\n",
    "            print(f\"appending fd for sess {session}\")\n",
    "            fdsessions.append({\n",
    "                \"session\": session,\n",
    "                \"fd\":fd})\n",
    "            FD[session] = fd\n",
    "            if session==N-1:\n",
    "                assert False, \"got the end and still found.. you need to search for even more sessions...\"\n",
    "\n",
    "            SAVEDIR = f\"{fd['params']['figuredir_notebook']}/probes/{animal}_{date}\"\n",
    "            import os\n",
    "            os.makedirs(SAVEDIR, exist_ok=True)\n",
    "\n",
    "\n",
    "    print(\"--- SUMMARY\")\n",
    "    for f in fdsessions:\n",
    "        print(f\"session: {f['session']}, ntrials: {f['fd']['params']['n_trials']}\")\n",
    "\n",
    "\n",
    "    # === categorize all trials based on task/probe, etc\n",
    "    probedat = []\n",
    "    ct = 0\n",
    "    for sess, fd in FD.items():\n",
    "        for t in getIndsTrials(fd):\n",
    "            if getTrialsFixationSuccess(fd, t):\n",
    "                probe = getTrialsTaskProbeInfo(fd, t)\n",
    "                task = getTrialsTask(fd, t)\n",
    "                \n",
    "#                 print(probe)\n",
    "                \n",
    "                if probe[\"prototype\"]==0 and len(probe[\"saved_setnum\"])==0 and probe[\"resynthesized\"]==0:\n",
    "                    # then this is new random task sampled each block.\n",
    "                    randomtask = True\n",
    "                else:\n",
    "                    randomtask = False\n",
    "\n",
    "                if probe[\"probe\"]==0:\n",
    "                    kind = \"train\"\n",
    "                else:\n",
    "                    if probe[\"feedback_ver\"]==\"same_as_task\" and randomtask==False:\n",
    "                        if probe[\"constraints_to_skip\"]=={}:\n",
    "                            kind = \"probe1_liketrain\"\n",
    "                        elif \"strokes\" in probe[\"constraints_to_skip\"].values():\n",
    "                            kind = \"probe1_nostrokeconstraint\"\n",
    "                    elif probe[\"feedback_ver\"]==\"same_as_task\" and randomtask==True and probe[\"constraints_to_skip\"]=={}:\n",
    "                        kind = \"probe2_liketrain\"\n",
    "                    elif probe[\"feedback_ver\"]==\"same_as_task\" and randomtask==True and \"strokes\" in probe[\"constraints_to_skip\"].values():\n",
    "                        kind = \"probe2_nostrokeconstraint\"\n",
    "                    elif probe[\"feedback_ver\"] in [\"thresh_active\", \"mid_reward\"] and randomtask==False and \"strokes\" in probe[\"constraints_to_skip\"].values():\n",
    "                        kind = \"probe3\"\n",
    "                    elif probe[\"feedback_ver\"] in [\"same_except_model\"] and randomtask==False and \"strokes\" in probe[\"constraints_to_skip\"].values():\n",
    "                        kind = \"probe3_hdpos\"\n",
    "                    elif probe[\"feedback_ver\"] in [\"thresh_active\", \"mid_reward\"] and randomtask==True and \"strokes\" in probe[\"constraints_to_skip\"].values():\n",
    "                        kind = \"probe4\"\n",
    "                    else:\n",
    "                        print(probe)\n",
    "                        assert False, \"what kind is this?\"\n",
    "\n",
    "                probedat.append({\n",
    "                    \"session\":sess,\n",
    "                    \"trial\":t,\n",
    "                    \"trial_day\":ct,\n",
    "                    \"kind\":kind,\n",
    "                    \"taskname\": probe[\"unique_name\"],\n",
    "                    \"stage\":task[\"stage\"],\n",
    "                    \"block\":getTrialsBlock(FD[sess], t),\n",
    "                    \"prototype\":probe[\"prototype\"],\n",
    "                    \"constraints_to_skip\":probe[\"constraints_to_skip\"]})\n",
    "                ct+=1\n",
    "\n",
    "\n",
    "    # == for each category, list names of all tasks\n",
    "    kindlist = set([p[\"kind\"] for p in probedat])\n",
    "    print(\"probe kinds -- trials\");\n",
    "    for kind in kindlist:\n",
    "        print(\" \")\n",
    "        print(kind)\n",
    "        trials = [p[\"trial\"] for p in probedat if p[\"kind\"]==kind]\n",
    "        tasknames = [p[\"taskname\"] for p in probedat if p[\"kind\"]==kind]\n",
    "    #     print(trials)\n",
    "        print(set(tasknames))\n",
    "        plt.figure()\n",
    "        plt.plot(trials, tasknames)\n",
    "\n",
    "    import seaborn as sns\n",
    "    import pandas as pd\n",
    "    dframe = pd.DataFrame(probedat)\n",
    "\n",
    "    # sns.scatterplot(x=\"trial\", y=\"taskname\", data=dframe, hue=\"kind\")\n",
    "    fig, axes = plt.subplots(5, 1, sharex=True, squeeze=False, figsize=(28, 10))\n",
    "\n",
    "    sns.scatterplot(x=\"trial_day\", y=\"kind\", data=dframe, hue=\"kind\", ax=axes[0][0])\n",
    "    sns.scatterplot(x=\"trial_day\", y=\"stage\", data=dframe, hue=\"kind\", ax=axes[1][0])\n",
    "    sns.scatterplot(x=\"trial_day\", y=\"stage\", data=dframe, hue=\"prototype\", ax=axes[2][0])\n",
    "    sns.lineplot(x=\"trial_day\", y=\"session\", data=dframe, ax=axes[3][0])\n",
    "    sns.lineplot(x=\"trial_day\", y=\"block\", data=dframe, ax=axes[4][0])\n",
    "\n",
    "    fig.savefig(f\"{SAVEDIR}/overviewAllTrials.pdf\")\n",
    "\n",
    "\n",
    "\n",
    "    stagelist = set([p[\"stage\"] for p in probedat])\n",
    "    sesslist = set([p[\"session\"] for p in probedat])    \n",
    "    kindlist = set([p[\"kind\"] for p in probedat])\n",
    "\n",
    "    ## PLOT PROBE TASKS OVER THE ENTIRE DAY\n",
    "    maxtrialsplot = 80\n",
    "\n",
    "    for kind in kindlist:\n",
    "        for stage in stagelist:\n",
    "            for s in sesslist:\n",
    "                trialsthis = [p[\"trial\"] for p in probedat if p[\"session\"]==s and p[\"kind\"]==kind and p[\"stage\"]==stage]\n",
    "                fd = FD[s]\n",
    "                print(f\"-- for session {s}, kind {kind}, stage {stage}; trials:\")\n",
    "                print(trialsthis)\n",
    "\n",
    "                if len(trialsthis)>0:\n",
    "                    if len(trialsthis)>maxtrialsplot:\n",
    "                        trialsthis = sorted(random.sample(trialsthis, maxtrialsplot))                        \n",
    "                        r = None\n",
    "                    else:\n",
    "                        r = None\n",
    "                    titles = []\n",
    "                    for t in trialsthis:\n",
    "                        mscore = getTrialsBehEvaluation(fd, t)[\"output\"][\"modelscore\"][\"value\"][0][0]\n",
    "                        bscore = getTrialsBehEvaluation(fd, t)[\"beh_multiplier\"][0][0]                                               \n",
    "                        titles.append(f\"{t},bk{getTrialsBlock(fd, t)},{getTrialsTask(fd, t)['str']}\\nmsco{mscore:.2f},bsco{bscore:.2f}\")\n",
    "\n",
    "#                         titles = [f\"{t},bk{getTrialsBlock(fd, t)}{getTrialsTask(fd, t)['str']}\\nmscore{mscore}\" for t in trialsthis]\n",
    "                    # plot\n",
    "                    \n",
    "                    # construct titles\n",
    "                    \n",
    "                    fig1 = plotMultTrialsSimple(fd, trialsthis, zoom=True, strokes_ver=\"peanuts\", plot_fix=True,\n",
    "                                            plotver=\"strokes\", rand_subset = r, titles=titles)\n",
    "\n",
    "                    scores = [getTrialsScoreRecomputed(fd, t, normalize=True) for t in trialsthis]\n",
    "                    scores_compos = [getTrialsScoreRecomputed(fd, t, ver=\"DTW_min\", normalize=True) for t in trialsthis]\n",
    "                    scores_compos2 = [getTrialsScoreRecomputed(fd, t, ver=\"DTW_min_minus_max\", normalize=True) for t in trialsthis]\n",
    "\n",
    "                    # == plot quick scores\n",
    "                    fig2 = plt.figure(figsize=(10, 12))\n",
    "\n",
    "                    plt.subplot(311)\n",
    "                    plt.title(f\"session {s}, kind {kind}, stage {stage}\")\n",
    "                    plt.plot(trialsthis, scores, \"ok\", label=\"pts\")\n",
    "    #                 plt.plot(trialsthis, scores_compos, \"or\", label=\"compositonal\")\n",
    "    #                 plt.plot(trialsthis, scores_compos2, \"og\", label=\"compositonal_min_minus_max\")\n",
    "                    plt.ylabel(\"score (pts) (norm HD, high is good)\")\n",
    "                    plt.ylim([-0.2, 1])\n",
    "                    plt.legend()\n",
    "\n",
    "                    plt.subplot(312)\n",
    "                    plt.title(f\"session {s}, kind {kind}, stage {stage}\")\n",
    "    #                 plt.plot(trialsthis, scores, \"ok\", label=\"pts\")\n",
    "                    plt.plot(trialsthis, scores_compos, \"or\", label=\"compositonal\")\n",
    "    #                 plt.plot(trialsthis, scores_compos2, \"og\", label=\"compositonal_min_minus_max\")\n",
    "                    plt.ylabel(\"score (compositonal) (norm HD, high is good)\")\n",
    "                    plt.ylim([-0.5, 1])\n",
    "                    plt.legend()\n",
    "\n",
    "                    plt.subplot(313)\n",
    "                    plt.title(f\"session {s}, kind {kind}, stage {stage}\")\n",
    "    #                 plt.plot(trialsthis, scores, \"ok\", label=\"pts\")\n",
    "    #                 plt.plot(trialsthis, scores_compos, \"or\", label=\"compositonal\")\n",
    "                    plt.plot(trialsthis, scores_compos2, \"og\", label=\"compositonal_min_minus_max\")\n",
    "                    plt.ylabel(\"score (compositonal_min_minus_max) (norm HD, high is good)\")\n",
    "                    #     plt.ylim([-0.2, 1])\n",
    "                    plt.legend()\n",
    "\n",
    "    #                 plt.subplot(212)\n",
    "    #                 s1 = (scores - np.mean(scores))/np.std(scores)\n",
    "    #                 s2 = (scores_compos - np.mean(scores_compos))/np.std(scores_compos)\n",
    "    #                 s3 = (scores_compos2 - np.mean(scores_compos2))/np.std(scores_compos2)\n",
    "    #                 plt.plot(trialsthis, s1, \"ok\", label=\"pts\")\n",
    "    #                 plt.plot(trialsthis, s2, \"or\", label=\"compositonal\")\n",
    "    #                 plt.plot(trialsthis, s3, \"og\", label=\"compositonal_min_minus_max\")\n",
    "    #                 plt.title(\"z-scored\")\n",
    "    #                 plt.ylabel(\"score (recomputed) (norm HD, high is good)\")\n",
    "    #                 plt.ylim([-3, 3])\n",
    "    #                 plt.legend()\n",
    "\n",
    "                    # save\n",
    "                    fig1.savefig(f\"{SAVEDIR}/trialsByProbeKind_{kind}-{stage}-sess{s}-fig1.pdf\")\n",
    "                    fig2.savefig(f\"{SAVEDIR}/trialsByProbeKind_{kind}-{stage}-sess{s}-fig2.pdf\")\n",
    "    #                 fig2.savefig(f\"{SAVEDIR}/trialsByProbeKind_sess{s}-{kind}-{stage}-fig2.pdf\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getTrialsTask(fd, t)[\"savedTaskSet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt = \"lines2\"\n",
    "animal = \"Pancho\"\n",
    "date = 200904\n",
    "\n",
    "fdsessions = []\n",
    "FD = {}\n",
    "N = 12\n",
    "for session in range(10):\n",
    "    fd = loadSingleData(animal, date, expt, session, resave_as_dict=False, load_resaved_data=True, \n",
    "                      resave_overwrite=False)\n",
    "    if fd is not None:\n",
    "        print(f\"appending fd for sess {session}\")\n",
    "        fdsessions.append({\n",
    "            \"session\": session,\n",
    "            \"fd\":fd})\n",
    "        FD[session] = fd\n",
    "        if session==N-1:\n",
    "            assert False, \"got the end and still found.. you need to search for even more sessions...\"\n",
    "\n",
    "        SAVEDIR = f\"{fd['params']['figuredir_notebook']}/probes/{animal}_{date}\"\n",
    "        import os\n",
    "        os.makedirs(SAVEDIR, exist_ok=True)\n",
    "\n",
    "\n",
    "print(\"--- SUMMARY\")\n",
    "for f in fdsessions:\n",
    "    print(f\"session: {f['session']}, ntrials: {f['fd']['params']['n_trials']}\")\n",
    "\n",
    "\n",
    "# === categorize all trials based on task/probe, etc\n",
    "probedat = []\n",
    "ct = 0\n",
    "for sess, fd in FD.items():\n",
    "    for t in getIndsTrials(fd):\n",
    "        if getTrialsFixationSuccess(fd, t):\n",
    "            probe = getTrialsTaskProbeInfo(fd, t)\n",
    "            task = getTrialsTask(fd, t)\n",
    "\n",
    "            if probe[\"probe\"]==0:\n",
    "                kind = \"train\"\n",
    "            else:\n",
    "                if probe[\"feedback_ver\"]==\"same_as_task\" and probe[\"prototype\"]==1:\n",
    "                    if probe[\"constraints_to_skip\"]=={}:\n",
    "                        kind = \"probe1_liketrain\"\n",
    "                    elif \"strokes\" in probe[\"constraints_to_skip\"].values():\n",
    "                        kind = \"probe1_nostrokeconstraint\"\n",
    "                elif probe[\"feedback_ver\"]==\"same_as_task\" and probe[\"prototype\"]==0 and probe[\"constraints_to_skip\"]=={}:\n",
    "                    kind = \"probe2_liketrain\"\n",
    "                elif probe[\"feedback_ver\"]==\"same_as_task\" and probe[\"prototype\"]==0 and \"strokes\" in probe[\"constraints_to_skip\"].values():\n",
    "                    kind = \"probe2_nostrokeconstraint\"\n",
    "                elif probe[\"feedback_ver\"] in [\"thresh_active\", \"mid_reward\"] and probe[\"prototype\"]==1 and \"strokes\" in probe[\"constraints_to_skip\"].values():\n",
    "                    kind = \"probe3\"\n",
    "                elif probe[\"feedback_ver\"] in [\"same_except_model\"] and probe[\"prototype\"]==1 and \"strokes\" in probe[\"constraints_to_skip\"].values():\n",
    "                    kind = \"probe3_hdpos\"\n",
    "                elif probe[\"feedback_ver\"] in [\"thresh_active\", \"mid_reward\"] and probe[\"prototype\"]==0 and \"strokes\" in probe[\"constraints_to_skip\"].values():\n",
    "                    kind = \"probe4\"\n",
    "                else:\n",
    "                    print(probe)\n",
    "                    assert False, \"what kind is this?\"\n",
    "\n",
    "            probedat.append({\n",
    "                \"session\":sess,\n",
    "                \"trial\":t,\n",
    "                \"trial_day\":ct,\n",
    "                \"kind\":kind,\n",
    "                \"taskname\": probe[\"unique_name\"],\n",
    "                \"stage\":task[\"stage\"],\n",
    "                \"prototype\":probe[\"prototype\"],\n",
    "                \"constraints_to_skip\":probe[\"constraints_to_skip\"]})\n",
    "            ct+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## for each trial also get online and offline score\n",
    "for p in probedat:\n",
    "    s = p[\"session\"]\n",
    "    t = p[\"trial\"]\n",
    "    \n",
    "    p[\"bk\"] = getTrialsBlock(FD[s], t)\n",
    "    p[\"bq\"] = getTrialsBloque(FD[s], t)\n",
    "    \n",
    "    # score\n",
    "    if getTrialsFixationSuccess(FD[s], t):\n",
    "        p[\"score_model\"] = getTrialsOutcomesWrapper(FD[s], t)[\"beh_evaluation\"][\"output\"][\"modelscore\"][\"value\"][0][0]\n",
    "        p[\"score_hd\"] = getTrialsOutcomesWrapper(FD[s], t)[\"beh_evaluation\"][\"output\"][\"hausdorff\"][\"value\"][0][0]\n",
    "        p[\"score_offline\"] = getTrialsScoreRecomputed(FD[s], t)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "dfthis = pd.DataFrame(probedat)\n",
    "dfthis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### import seaborn as sns\n",
    "\n",
    "y = \"score_model\"\n",
    "sns.relplot(x = \"trial\", y=y, hue=\"bk\", data=dfthis, row=\"kind\", kind=\"scatter\", aspect=3)\n",
    "sns.relplot(x = \"trial_day\", y=y, hue=\"bk\", data=dfthis, row=\"kind\", kind=\"scatter\", aspect=3)\n",
    "sns.relplot(x = \"bq\", y=y, hue=\"bk\", data=dfthis, row=\"kind\", kind=\"scatter\", aspect=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### import seaborn as sns\n",
    "\n",
    "y = \"score_model\"\n",
    "sns.catplot(x = \"bk\", y=y, data=dfthis, row=\"kind\", kind=\"point\", aspect=3)\n",
    "# sns.catplot(x = \"bq\", y=y, data=dfthis, col=\"kind\", row=\"bk\", kind=\"point\", aspect=3)\n",
    "sns.catplot(x = \"bq\", y=y, data=dfthis, row=\"kind\", hue=\"bk\", kind=\"point\", aspect=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## COMPUTE COMPOSITIONAL SCORE\n",
    "# [Quick version] DTW score relative to non-DTW score\n",
    "\n",
    "## ==== [testing] stroke based hd\n",
    "from pythonlib.tools.stroketools import distanceDTW\n",
    "t = random.sample(getIndsTrials(fd),1)[0]\n",
    "\n",
    "strokes_beh = getTrialsStrokesByPeanuts(fd, t, replaynum=1)\n",
    "strokes_task = getTrialsTaskAsStrokes(fd, t)\n",
    "\n",
    "for ass in [True, False]:\n",
    "    print(f\"assymetric: {ass}\")\n",
    "    plotTrialSimple(fd, t, zoom=True, plot_fix=False, plotver=\"strokes\", \n",
    "                    use_peanut_params={'replaynum': 1, 'active': True})\n",
    "\n",
    "    print(distanceDTW(strokes_beh, strokes_task[::-1], ver=\"segments\", asymmetric=ass))\n",
    "    print(distanceDTW(strokes_beh, strokes_task, ver=\"segments\", asymmetric=ass))\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    ax = plt.subplot(211)\n",
    "    plotDatStrokes(strokes_beh, ax=ax)\n",
    "    plotDatStrokes(strokes_task[::-1], ax=ax)\n",
    "\n",
    "    ax = plt.subplot(212)\n",
    "    plotDatStrokes(strokes_beh, ax=ax)\n",
    "    plotDatStrokes(strokes_task, ax=ax)\n",
    "\n",
    "# compute minimum score for all permutations of task strokes\n",
    "# make assymetric false, so forced to use all task strokes.\n",
    "print(\"-- all permutations\")\n",
    "from itertools import permutations\n",
    "scores =[]\n",
    "for s in permutations(strokes_task):\n",
    "    print(distanceDTW(strokes_beh, s, ver=\"segments\", asymmetric=False))\n",
    "    scores.append(distanceDTW(strokes_beh, s, ver=\"segments\", asymmetric=False)[0])\n",
    "score = min(scores)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonlib.tools.listtools import permuteRand\n",
    "\n",
    "permuteRand([1,2,3], 6, not_enough_ok=True)\n",
    "from itertools import permutations\n",
    "\n",
    "for i in permutations([1,2,3]):\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCRATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "featurestoplot = []\n",
    "for key, val in getTrialsBlockParams(fd, 1)[\"behEval\"][\"beh_eval\"].items():\n",
    "    if val[\"feature\"] ==\"hausdorff\" and val[\"weight\"][0][0]>0:\n",
    "        featurestoplot.append(\"hausdorff\")\n",
    "    if val[\"feature\"] ==\"frac_touched\" and val[\"weight\"][0][0]>0:\n",
    "        featurestoplot.append(\"frac_touched\")\n",
    "featurestoplot.append(\"score_offline\")\n",
    "\n",
    "fig1, fig2 = plotOverview_(df, featurestoplot=featurestoplot)\n",
    "fig1.savefig(f\"{SAVEDIRDAY}/overview1.pdf\")\n",
    "fig2.savefig(f\"{SAVEDIRDAY}/overview2.pdf\")\n",
    "\n",
    "# 2) relationship between reward and factors that go into reward\n",
    "figs = plotReward(df, featurestoplot=featurestoplot)\n",
    "for i, f in enumerate(figs):\n",
    "    f.savefig(f\"{SAVEDIRDAY}/reward_score_{i}.pdf\")\n",
    "\n",
    "# 3) PLOT BEHAVIOR FOR TRIALS SORTED BY SCORE\n",
    "import copy\n",
    "scoretypes = copy.copy(featurestoplot)\n",
    "scoretypes.extend([\"behscore\", \"reward\"])\n",
    "for score_type in scoretypes:\n",
    "    FIGS = plotBehSortedByScore(df, fd, score_type)\n",
    "    for ver, figs in FIGS.items():\n",
    "        for i, f in enumerate(figs):\n",
    "            f.savefig(f\"{SAVEDIRDAY}/trialsSortedByScore_{score_type}_{ver}_{i}_.pdf\")\n",
    "\n",
    "# 4) Plot behavior subsampling in chronological order\n",
    "trials = [t for t in getIndsTrials(fd) if getTrialsFixationSuccess(fd, t)]\n",
    "Nrand = 80\n",
    "fig = plotMultTrialsSimple(fd, trials, zoom=True, strokes_ver=\"peanuts\", plot_fix=False,\n",
    "                        plotver=\"strokes\", rand_subset=Nrand)\n",
    "fig.savefig(f\"{SAVEDIRDAY}/trialsRandomChronOrder.pdf\")\n",
    "\n",
    "# 5) TASK VISUALIZATIONS, SCHEDULE, REPETITION\n",
    "figs = plotTaskSchedules(df)\n",
    "for i, f in enumerate(figs):\n",
    "    f.savefig(f\"{SAVEDIRDAY}/taskSchedule{i}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    animal = \"Pancho\"\n",
    "    expt = \"pilot\"\n",
    "    date = 200822\n",
    "    session = 1\n",
    "\n",
    "    fd = loadSingleData(animal, date, expt, session, resave_as_dict=False, load_resaved_data=True, \n",
    "                              resave_overwrite=False)\n",
    "\n",
    "    ## ==== [testing] stroke based hd\n",
    "    from pythonlib.tools.stroketools import distanceDTW\n",
    "    t = random.sample(getIndsTrials(fd),1)[0]\n",
    "\n",
    "    for ass in [True, False]:\n",
    "        print(f\"assymetric: {ass}\")\n",
    "        plotTrialSimple(fd, t, zoom=True, plot_fix=False, plotver=\"strokes\", \n",
    "                        use_peanut_params={'replaynum': 1, 'active': True})\n",
    "\n",
    "        strokes_beh = getTrialsStrokesByPeanuts(fd, t, replaynum=1)\n",
    "        strokes_task = getTrialsTaskAsStrokes(fd, t)\n",
    "        print(distanceDTW(strokes_beh, strokes_task[::-1], ver=\"segments\", asymmetric=ass))\n",
    "        print(distanceDTW(strokes_beh, strokes_task, ver=\"segments\", asymmetric=ass))\n",
    "\n",
    "        plt.figure(figsize=(10,10))\n",
    "        ax = plt.subplot(211)\n",
    "        plotDatStrokes(strokes_beh, ax=ax)\n",
    "        plotDatStrokes(strokes_task[::-1], ax=ax)\n",
    "\n",
    "        ax = plt.subplot(212)\n",
    "        plotDatStrokes(strokes_beh, ax=ax)\n",
    "        plotDatStrokes(strokes_task, ax=ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ==== plot scoring separated by task type\n",
    "\n",
    "df = extractSessionDf(fd)\n",
    "\n",
    "# === add note about what probe type this is\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO:\n",
    "(1) get all permutations\n",
    "(2) normalize by num strokes.\n",
    "(3) systematic - compare to old version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
