# Hand Track Pipeline
On this 26th day of September in the 2024th year of our Lord we thus erect the readme of the Hand Track Pipeline. In this document will be found brief descriptions of each module, as well as the output types, formats and locations.
## A Few Things to Begin With
- There is a file called ```config``` in the main directory, this is where most, if not all, of the global vairables should be set-- the directories of where different code bases the pipeline uses are.
- There are a few folders in this mian directory of note (in alphabetical order):
  - **checkerboard_calib** : This is where checkerboard calibrations for the cameras should be stored. Automatically selects DLT coeffs, but ff new ones need added the conditional statement that selects them should be updated in ```final_analysis.py```. At the time of this writing there are two, one for pre 220914 dates and two for post 220914 dates, based on different camera combos.
  - **dlt_coeffs** : This is where the dlt coeffs are stored, same as above there should be code to select the correct ones, but as more are added it must be updated. The code is written to look for a 'prefix' whihc will be the name of the folder the coeffs are in. In each folder there should be a file ```dltCoefs.csv``` with the coeffs and another called ```columns.csv``` which indicates which column goes to each camera. It is important that this is correct as the code will automatically reorganize the dlt coeffs to match the given day based on these columns.
  - **metadata** : This is where the metadata files for each day are stored, split by animal name. These are mostly autogenerated, but if there is a specific thing you want to do (i.e. only do a subset of trial in a given day, or if it is a wand day) they can be edited manually.
      - Update to metadata: metadata folde rnow lives in videoclass/pyvm. It does not live in the pipeline directory anymore
  - **models** : This stores the DLC models that have been trained, you can alter the config file to user whichever model suits the need. There is currently one model in here that has been working for behavior days (trained on data from both monkeys across a wide range of days) and another for wands (traine don wands from 220914-221015), named respectively.
  - **pipeline-modules** : Contains all the code for the scripts that make up the modules of the pipeline. Each script comes equipped with a description of usage and options.
  - **pipeline-scripts** : some scripts that the modules use, as well as some other scripts for organizing files and such. ```align_wand.py``` is particularly useful for moving labelled wand data to another day (i.e. the checkerboard day) and trackign camera alignment. There are also some other scripts, most should be explained in script or can be inferred based on code. Will eventually spell out how these all work in another file.
  - **setup** : The bulk of packages and libraries used by the pipeline are here. They are relatively static. The environments needed for the pipeline can be built automatically during the env-setup module from these source files. May need to manually add some python libraries on the road, but most deps are handled here.
    - Update to setup : All custom python libraries (drawmonkey, videomonkey, pythonlib) have been moved out of the pipeline setup folder and now live standalone. Pipeline environment will need these libraries to be manually added using a ```pip install -e .``` in the relevant git repo.
    - Any libraries that are small and only used in for ht pipeline activites will still live here
  - **temp_matlab_files** : This can be mostly ignore, it is just a folder for storing intermediate data processed in Python for triangulation in the easyWand package in Matlab and then passed back to Python and automatically deleted. If this process is interupted in the middle, you may need to manually delete these files and begin again.
  - **visualization_nbs** : Just a few nbs for visualizing the data quickly, and for debugging various plotting and transformation elements. ```apply_dlt_qr.ipynb``` has some good 3D plots for trouble shooting dlt coefficient shenanigans. You can traingulate in matalb and upload the data to this notebook to take a look.
  - **wand_calibration** : I don't think this actualkly gets used but I forget.
  - **wand_data** : I also don't think this gets used.
  - **Other files** :
    - ```notrain_run_script.sh``` : Useful script for running a day automatically, both individual sessions/expts and also a loop for all sessions/expts on a given day.
    - ```wands_all.sh``` : Runs labelling and extraction of wand pts for a list of days.
    - ```wrapperwrapper.sh``` : Calls ```notrain_run_script``` for a list of dates
    - ```pancho/diego_run.sh``` : Script to run list of dates for either Pancho or Diego. Can do ```ctrl+z ; bg ; disown``` to run in background on server. Will automatically generate output logs for each run
## Brief description of modules
If more info needed, run the module with a --help flag, for options and usage.
- **env-setup** : Sets up the environments needed for the pipeline, is not exhaustive so some manual additions will need made.
  - Outputs: Shiny new environments.
- **init** : Creates metadata files and directories for a given session
  - Outputs : Links data from server (optional) and makes dirs for each camera, also metadat file in expt directory and metadata file in pipeline metadata.
- **checkerb** : Runs checkerboard for given day, some manual setup is required to make the dirs.
  - Ouputs : Checkerboard calibration matrix and also undistorted frames for review to given cameras folder.
- **dlc-setup** : Should be run before any DLC modules, will intialize and organize data for DLC analysis. Read options carefully based on your use needs. Includes labelling GUI if new model will be trained.
  - Outputs : DLC project, config file, and directories in expt/behavior/DLC.
- **train** : Module to train a new DLC modle. Has a few steps for evaluation and analysis and requires some user labelling in DLC GUI if you want to refine labels.
  - Outputs : Makes folders for model in expt/behavior/DLC, and various stages of analysis
- **analyze** : Main module for model evaluation and data analysis. Can be used with bespoke model or general model. --skiplink flag will determine this. At the end a labelling GUI will appear for user input on quality of labels.
  - Outputs : Makes folders for model and various steps of analysis in expt/behavior/dlc/combined.../
- **wand** : Purported last step in the pipeline, does all the data extractions, transformations, and plotting. Has a few steps, some can be skipped if you already have dlt coeffs for example.
  - Outputs : folder in /behavior/DLC with extracted campy and DLC data and folder in expt/figures. There are a number of figures here, organuized by trial each is pretty self explanatory. fig 3 and 4 are particularly useful for a quick look at how the analysis went. Also plot called all_day_fig that summarizes data across the day.
  - **wand 4** and **wand 5** are the main steps for final preprocessing and saving of the data so I will go into more detial for them
    - **wand 4** loads in all the DLC labels, aligns the pts with the volt times data, traingulates the 2d pts into 3d pts and saves them again for use in wand 5. The final saved file will have xyz coordinates as well as the volt times. [See slides](https://docs.google.com/presentation/d/1byxxQnXv9_OT_P40TYNk78Ght8jlovQEWgyKZpSkTe4/edit?usp=sharing)
    - **wand 5** loads in the triangulated data and processes everything into a final data structure. The main things added here are a regression of DLC data to touchscreen data and an automatic calculation of stroke onsets and offsets based on the z velocity. [There are slides that describe this process](https://docs.google.com/presentation/d/1glAWdqEiynW4p81Q533IEAA9ZpZXy-zrbt2WZ6lcIBg/edit?usp=sharing). Also has some plotting functionality, turning this on increases the run time significantly (~6hrs/day).
- **lag** : Additional module used for finding the lag between touch screen and cam for a given date. The pipeline currently corrects using a value averaged over a few dates (40ms). But if for your analysis you need a more accurate lag calculation, you can run this module to find that number.
  - Outputs : Step1 outputs plots in ```lag_data``` subdirectory to review for good lags to use in calculation. Step2 outputs a final summary fig using all the data and also a plot with the user filtered data.
## Other Important Code
The ht pipeline also accesses code from drawmonkey, videoclass, and pythonlib to function, particularly in the last steps when video data and behavioral data need to be processed and aligned.
#### Videoclass
Videoclass is a custom python library/class that implements all the video-based functions needed to get DLC data in a usable format, as well as handling 3d triangulation and alignment with the volt clock. The main functions/files accessed here are ```dlc_xyz_extraction.py``` and ```videoclass.py```.
#### Drawmonkey
Drawmonkey is another custom python library with the handtrack class. This does many many things, but for relevance here, it contains ```ht_preprocess.py``` which contains the function ```preprocess_clean()``` this function accesses the Handtrack object in handtrack.py and does the preprocessing and creates a data structure with all the relevant data for handtracking. You can also generate plots for each trial, as well as plots summarizing the data over the day. Finally, this step can also do a linear regression on the raw DLC 3d points to align them with the tocuh screen strokes better (usually just a simple translation or small scaling change). There is also a z-coordinate 'regression' that moves the z-coordinate during strokes as close to zero as possible without the trivial solution (i.e. assumes there is some dependence of the z-coordinate on the x and y coordinates). Basically a transformation matrix T is optimized to send each Xn = (xn,yn,zn) (n = 1...num_pts) to 0.
#### pythonlib
pythonlib is an expansive codebase that much of the code in this project requires for dependency. There are not any major steps here than require the use to manually access pythonlib, thus detail is unnecessary but almost every aspect of the pipeline has some functions taken from here.

